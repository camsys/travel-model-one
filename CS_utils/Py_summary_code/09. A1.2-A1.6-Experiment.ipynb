{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec8eb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openmatrix as omx\n",
    "import random\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "from utility import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d84ecc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "    \n",
    "_join = os.path.join\n",
    "_dir = os.path.dirname\n",
    "_norm = os.path.normpath\n",
    "\n",
    "# paths\n",
    "model_outputs_dir = params['model_dir']\n",
    "skims_dir = _join(model_outputs_dir, \"skims\")\n",
    "summary_outputs = params['summary_dir']\n",
    "concept_id = params['concept_id']\n",
    "ctramp_dir = params['ctramp_dir']\n",
    "iteration = params['iteration']\n",
    "\n",
    "concept_id = params['concept_id']\n",
    "time_period_mapping = params['time_periods_mapping']\n",
    "link21_purp_mapping = params['purpose_mapping']\n",
    "mode_cat_mapping = params['mode_mapping']\n",
    "time_periods = params['periods']\n",
    "acc_egg_modes = params['access_egress_modes']\n",
    "\n",
    "preprocess_dir = _join(ctramp_dir, '_pre_process_files')\n",
    "summary_dir = _join(params['summary_dir'], 'more_A1')\n",
    "Path(summary_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "filename_extension = params['filename_extension']\n",
    "perf_measure_columns = params['final_columns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b9e742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs of CT-RAMP model for tour and trip file\n",
    "household_model_dir = _join(model_outputs_dir, \"main\")\n",
    "\n",
    "# input household and person data\n",
    "#person_file = _join(ctramp_dir, 'main\\\\personData_' + str(iteration) + '.csv')\n",
    "#household_file = _join(ctramp_dir, 'main\\\\householdData_' + str(iteration) + '.csv')\n",
    "\n",
    "#person = pd.read_csv(person_file)\n",
    "\n",
    "#hh = pd.read_csv(household_file, usecols = ['hh_id', 'taz'])\n",
    "#hh = hh.rename(columns = {'taz': 'home_zone'})\n",
    "\n",
    "#taz to RDM zones, super districts, county\n",
    "geo_cwks = pd.read_csv(_join(params['common_dir'], \"geographies.csv\")) #columns taz, rdm_zones, super_district, county\n",
    "\n",
    "#taz to priority population\n",
    "pp_perc = pd.read_excel(_join(params['common_dir'], \"TAZ_Tract_cwk_summary.xlsx\")) #columns = taz, pp_share \n",
    "\n",
    "# transbay od pairs\n",
    "#transbay_od = pd.read_csv(_join(params['common_dir'], \"transbay_od.csv\")) #columns = transbay_o, transbay_d\n",
    "\n",
    "demand_matrices_dir = _join(model_outputs_dir, \"demand_matrices\")\n",
    "transit_demand_dir = _join(demand_matrices_dir, \"transit\")\n",
    "transit_skims_dir = _join(skims_dir, \"transit\")\n",
    "best_path_skim_dir = params['best_path_skim_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6e892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_trips = create_trip_roster(ctramp_dir, transbay_od, geo_cwks, link21_purp_mapping)\n",
    "#df_trn = df_trips.loc[df_trips['trip_mode'].isin([6,7,8])]\n",
    "\n",
    "#df_trn['Period'] = df_trn['depart_hour'].map(time_period_mapping)\n",
    "#df_trn['Mode'] = df_trn['trip_mode'].map(mode_cat_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fef60825",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trips = pd.read_parquet(_join(preprocess_dir, 'trip_roster.parquet'))\n",
    "#all_trips = pd.read_parquet(_join(preprocess_dir, 'trip_roster_2050_Baseline_R2_Run4.parquet'))\n",
    "df_trn = all_trips.loc[all_trips['trip_mode'].isin([6,7,8])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7491f3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\MTC_tmpy\\\\TM2_2050Baseline_R2_Run7\\\\tm2py\\\\examples\\\\Link21_3332\\\\skims\\\\transit'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_trn\n",
    "skims_dir_trn = _join(skims_dir, 'transit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5433baa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WLK_TRN_WLK', 'KNR_TRN_WLK', 'PNR_TRN_WLK', 'WLK_TRN_PNR', 'WLK_TRN_KNR']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_egr_modes = params['access_egress_modes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f783632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmam_WLK_TRN_WLK.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmam_KNR_TRN_WLK.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmam_PNR_TRN_WLK.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmam_WLK_TRN_PNR.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmam_WLK_TRN_KNR.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmmd_WLK_TRN_WLK.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmmd_KNR_TRN_WLK.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmmd_PNR_TRN_WLK.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmmd_WLK_TRN_PNR.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmmd_WLK_TRN_KNR.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmpm_WLK_TRN_WLK.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmpm_KNR_TRN_WLK.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmpm_PNR_TRN_WLK.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmpm_WLK_TRN_PNR.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmpm_WLK_TRN_KNR.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmev_WLK_TRN_WLK.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmev_KNR_TRN_WLK.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmev_PNR_TRN_WLK.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmev_WLK_TRN_PNR.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmev_WLK_TRN_KNR.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmea_WLK_TRN_WLK.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmea_KNR_TRN_WLK.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmea_PNR_TRN_WLK.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmea_WLK_TRN_PNR.omx\n",
      "C:\\MTC_tmpy\\TM2_2050Baseline_R2_Run7\\tm2py\\examples\\Link21_3332\\skims\\transit\\trnskmea_WLK_TRN_KNR.omx\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#modify transit skims\n",
    "for period in time_periods:    \n",
    "    for acc in acc_egr_modes:        \n",
    "        file_name = _join(skims_dir_trn, \"trnskm\" + period + '_' + acc + '.omx')    \n",
    "        if os.path.exists(file_name):\n",
    "            print(file_name)\n",
    "            skim = omx.open_file(file_name)\n",
    "            \n",
    "            trip_time = np.array(skim['IVT']) + np.array(skim['DTIME']) + np.array(skim['WACC']) + \\\n",
    "                        np.array(skim['WAIT']) + np.array(skim['WAUX']) + np.array(skim['WEGR'])\n",
    "\n",
    "            ttime = array2df(trip_time, cols = ['orig', 'dest', 'mp_trip_time'])\n",
    "\n",
    "            ivt = skim_core_to_df(skim, 'IVT', cols =['orig', 'dest', 'mp_ivt'])\n",
    "            crowd = skim_core_to_df(skim, 'CROWD', cols =['orig', 'dest', 'mp_crowd'])\n",
    "            \n",
    "            df_trn_skim = pd.merge(ivt, crowd, on = ['orig', 'dest'], how='left').merge(\n",
    "                                   ttime, on=['orig', 'dest'], how = 'left')\n",
    "            \n",
    "            skim.close()\n",
    "\n",
    "            df_trn_skim.to_parquet(_join(preprocess_dir, 'mp_'+period.lower() +'_'+ acc +'_cores.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5265f8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing - am\n",
      "WLK_TRN_WLK am 37 1\n",
      "WLK_TRN_PNR am 1 18\n",
      "PNR_TRN_WLK am 99603 22664\n",
      "WLK_TRN_KNR am 0 275\n",
      "KNR_TRN_WLK am 0 581\n",
      "processing - md\n",
      "WLK_TRN_WLK md 0 0\n",
      "WLK_TRN_PNR md 0 0\n",
      "PNR_TRN_WLK md 0 0\n",
      "WLK_TRN_KNR md 0 31\n",
      "KNR_TRN_WLK md 0 0\n",
      "processing - pm\n",
      "WLK_TRN_WLK pm 0 14\n",
      "WLK_TRN_PNR pm 0 19\n",
      "PNR_TRN_WLK pm 0 85\n",
      "WLK_TRN_KNR pm 0 258\n",
      "KNR_TRN_WLK pm 0 877\n",
      "processing - ev\n",
      "WLK_TRN_WLK ev 0 0\n",
      "WLK_TRN_PNR ev 0 130\n",
      "PNR_TRN_WLK ev 0 0\n",
      "WLK_TRN_KNR ev 0 0\n",
      "KNR_TRN_WLK ev 0 0\n",
      "processing - ea\n",
      "WLK_TRN_WLK ea 0 0\n",
      "WLK_TRN_PNR ea 0 0\n",
      "PNR_TRN_WLK ea 0 0\n",
      "WLK_TRN_KNR ea 0 0\n",
      "KNR_TRN_WLK ea 0 0\n",
      "Wall time: 4min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#time_periods = ['AM']\n",
    "df_temp = []\n",
    "\n",
    "for period in time_periods:\n",
    "    print(f'processing - {period}')\n",
    "    \n",
    "    df_trn_pd = df_trn[df_trn['Period'] == period.lower()]\n",
    "    df_trn_wlk = df_trn_pd[df_trn_pd['Mode'] == 'WALK_TRANSIT']\n",
    "    df_trn_pnr = df_trn_pd[df_trn_pd['Mode'] == 'PNR_TRANSIT']\n",
    "    df_trn_knr = df_trn_pd[df_trn_pd['Mode'] == 'KNR_TRANSIT']\n",
    "    \n",
    "    #for acc_egg in acc_egg_modes:\n",
    "    #for Walk transit Walk\n",
    "    #df_skm_wk = pd.read_parquet(_join(preprocess_dir, period.lower() +'_WLK_TRN_WLK' + '_cores.parquet'))\n",
    "    df_skm_wk1 = pd.read_parquet(_join(preprocess_dir, period.lower() +'_WLK_TRN_WLK' + '_cores.parquet'))\n",
    "    df_skm_wk2 = pd.read_parquet(_join(preprocess_dir, 'mp_' + period.lower() +'_WLK_TRN_WLK' + '_cores.parquet'))\n",
    "    df_skm_wk = pd.merge(df_skm_wk1, df_skm_wk2, on = ['orig', 'dest'], how='left')\n",
    "    df_skm_wk['ivt_sp'] = df_skm_wk['ivt'] - df_skm_wk['mp_crowd']\n",
    "    df_skm_wk['ivt_mp'] = df_skm_wk['mp_ivt'] - df_skm_wk['mp_crowd']\n",
    "    df_skm_wk = df_skm_wk.loc[df_skm_wk['ivt']>0]\n",
    "    print('WLK_TRN_WLK', period, len(df_skm_wk.loc[df_skm_wk['ivt_mp']<0]), len(df_skm_wk.loc[df_skm_wk['ivt_sp']<0]))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    df_wlk = pd.merge(df_trn_wlk, df_skm_wk, \n",
    "                          left_on =['orig_taz', 'dest_taz'], \n",
    "                          right_on=['orig', 'dest'], \n",
    "                          how ='inner')\n",
    "\n",
    "    # PNR Transit\n",
    "    #df_skm_pnr_ib = pd.read_parquet(_join(preprocess_dir, period.lower() +'_WLK_TRN_PNR' + '_cores.parquet'))\n",
    "    df_skm_pnr_ib1 = pd.read_parquet(_join(preprocess_dir, period.lower() +'_WLK_TRN_PNR' + '_cores.parquet'))\n",
    "    df_skm_pnr_ib2 = pd.read_parquet(_join(preprocess_dir, 'mp_' + period.lower() +'_WLK_TRN_PNR' + '_cores.parquet'))\n",
    "    df_skm_pnr_ib = pd.merge(df_skm_pnr_ib1, df_skm_pnr_ib2, on = ['orig', 'dest'], how='left')\n",
    "    df_skm_pnr_ib['ivt_sp'] = df_skm_pnr_ib['ivt'] - df_skm_pnr_ib['mp_crowd']\n",
    "    df_skm_pnr_ib['ivt_mp'] = df_skm_pnr_ib['mp_ivt'] - df_skm_pnr_ib['mp_crowd']\n",
    "    df_skm_pnr_ib = df_skm_pnr_ib[df_skm_pnr_ib['ivt']>0]\n",
    "    print('WLK_TRN_PNR', period, len(df_skm_pnr_ib.loc[df_skm_pnr_ib['ivt_mp']<0]), len(df_skm_pnr_ib.loc[df_skm_pnr_ib['ivt_sp']<0]))\n",
    "    \n",
    "\n",
    "    #df_skm_pnr_ob = pd.read_parquet(_join(preprocess_dir, period.lower() +'_PNR_TRN_WLK' + '_cores.parquet'))\n",
    "    df_skm_pnr_ob1 = pd.read_parquet(_join(preprocess_dir, period.lower() +'_PNR_TRN_WLK' + '_cores.parquet'))\n",
    "    df_skm_pnr_ob2 = pd.read_parquet(_join(preprocess_dir, 'mp_' + period.lower() +'_PNR_TRN_WLK' + '_cores.parquet'))\n",
    "    df_skm_pnr_ob = pd.merge(df_skm_pnr_ob1, df_skm_pnr_ob2, on = ['orig', 'dest'], how='left')\n",
    "    df_skm_pnr_ob['ivt_sp'] = df_skm_pnr_ob['ivt'] - df_skm_pnr_ob['mp_crowd']\n",
    "    df_skm_pnr_ob['ivt_mp'] = df_skm_pnr_ob['mp_ivt'] - df_skm_pnr_ob['mp_crowd']\n",
    "    df_skm_pnr_ob = df_skm_pnr_ob[df_skm_pnr_ob['ivt']>0]\n",
    "    print('PNR_TRN_WLK', period, len(df_skm_pnr_ob.loc[df_skm_pnr_ob['ivt_mp']<0]), len(df_skm_pnr_ob.loc[df_skm_pnr_ob['ivt_sp']<0]))\n",
    "    \n",
    "\n",
    "    df_trn_pnr_ib = df_trn_pnr[df_trn_pnr['inbound'] == 1] # returning home\n",
    "    df_trn_pnr_ib = pd.merge(df_trn_pnr_ib, df_skm_pnr_ib, \n",
    "                              left_on =['orig_taz', 'dest_taz'], \n",
    "                              right_on=['orig', 'dest'], how ='inner')\n",
    "\n",
    "    df_trn_pnr_ob = df_trn_pnr[df_trn_pnr['inbound'] != 1] # returning home\n",
    "    df_trn_pnr_ob = pd.merge(df_trn_pnr_ob, df_skm_pnr_ob, \n",
    "                              left_on =['orig_taz', 'dest_taz'], \n",
    "                              right_on=['orig', 'dest'], how ='inner')\n",
    "\n",
    "    df_pnr = pd.concat([df_trn_pnr_ib, df_trn_pnr_ob], ignore_index=True)\n",
    "\n",
    "        # KNR Transit\n",
    "    #df_skm_knr_ib = pd.read_parquet(_join(preprocess_dir, period.lower() +'_WLK_TRN_KNR' + '_cores.parquet'))\n",
    "    df_skm_knr_ib1 = pd.read_parquet(_join(preprocess_dir, period.lower() +'_WLK_TRN_KNR' + '_cores.parquet'))\n",
    "    df_skm_knr_ib2 = pd.read_parquet(_join(preprocess_dir, 'mp_' + period.lower() +'_WLK_TRN_KNR' + '_cores.parquet'))\n",
    "    df_skm_knr_ib = pd.merge(df_skm_knr_ib1, df_skm_knr_ib2, on = ['orig', 'dest'], how='left')\n",
    "    df_skm_knr_ib['ivt_sp'] = df_skm_knr_ib['ivt'] - df_skm_knr_ib['mp_crowd']\n",
    "    df_skm_knr_ib['ivt_mp'] = df_skm_knr_ib['mp_ivt'] - df_skm_knr_ib['mp_crowd']\n",
    "    df_skm_knr_ib = df_skm_knr_ib[df_skm_knr_ib['ivt']>0]\n",
    "    print('WLK_TRN_KNR', period, len(df_skm_knr_ib.loc[df_skm_knr_ib['ivt_mp']<0]), len(df_skm_knr_ib.loc[df_skm_knr_ib['ivt_sp']<0]))\n",
    "    \n",
    "\n",
    "    df_skm_knr_ob = pd.read_parquet(_join(preprocess_dir, period.lower() +'_KNR_TRN_WLK' + '_cores.parquet'))\n",
    "    df_skm_knr_ob1 = pd.read_parquet(_join(preprocess_dir, period.lower() +'_KNR_TRN_WLK' + '_cores.parquet'))\n",
    "    df_skm_knr_ob2 = pd.read_parquet(_join(preprocess_dir, 'mp_' + period.lower() +'_KNR_TRN_WLK' + '_cores.parquet'))\n",
    "    df_skm_knr_ob = pd.merge(df_skm_knr_ob1, df_skm_knr_ob2, on = ['orig', 'dest'], how='left')\n",
    "    df_skm_knr_ob['ivt_sp'] = df_skm_knr_ob['ivt'] - df_skm_knr_ob['mp_crowd']\n",
    "    df_skm_knr_ob['ivt_mp'] = df_skm_knr_ob['mp_ivt'] - df_skm_knr_ob['mp_crowd']\n",
    "    df_skm_knr_ob = df_skm_knr_ob[df_skm_knr_ob['ivt']>0]\n",
    "    print('KNR_TRN_WLK', period, len(df_skm_knr_ob.loc[df_skm_knr_ob['ivt_mp']<0]), len(df_skm_knr_ob.loc[df_skm_knr_ob['ivt_sp']<0]))\n",
    "    \n",
    "\n",
    "    df_trn_knr_ib = df_trn_knr[df_trn_knr['inbound'] == 1] # returning home\n",
    "    df_trn_knr_ib = pd.merge(df_trn_knr_ib, df_skm_knr_ib, \n",
    "                              left_on =['orig_taz', 'dest_taz'], \n",
    "                              right_on=['orig', 'dest'], how ='inner')\n",
    "\n",
    "    df_trn_knr_ob = df_trn_knr[df_trn_knr['inbound'] != 1] # returning home\n",
    "    df_trn_knr_ob = pd.merge(df_trn_knr_ob, df_skm_knr_ob, \n",
    "                              left_on =['orig_taz', 'dest_taz'], \n",
    "                              right_on=['orig', 'dest'], how ='inner')\n",
    "\n",
    "    df_knr = pd.concat([df_trn_knr_ib, df_trn_knr_ob], ignore_index=True)\n",
    "\n",
    "    df_trn_rail = pd.concat([df_wlk, df_pnr, df_knr], ignore_index=True)\n",
    "    df_temp.append(df_trn_rail)\n",
    "    \n",
    "\n",
    "df_trn_rail = pd.concat(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33ba87e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2f86f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn_rail[df_trn_rail['ivt_sp']<0]['trips'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2431ec79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "502.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn_rail[df_trn_rail['ivt_mp']<0]['trips'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e98d84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn_rail['ivt_sp_adj'] = np.where(df_trn_rail['ivt_sp']<0, df_trn_rail['ivt'], df_trn_rail['ivt_sp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67201509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn_rail[df_trn_rail['ivt_sp_adj']<0]['trips'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75595f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_trn_rail['trip_time_adj'] = df_trn_rail['trip_time'] - df_trn_rail['mp_crowd'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e928486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn_rail['trip_time_adj'] = np.where(df_trn_rail['ivt_sp']<0, df_trn_rail['trip_time'], \n",
    "                                       df_trn_rail['trip_time'] - df_trn_rail['mp_crowd'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31930f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn_rail[df_trn_rail['trip_time_adj']<0]['trips'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a070c0",
   "metadata": {},
   "source": [
    "# summarise for prioirty population\n",
    "summary_cols = params['description_a1.2']\n",
    "\n",
    "time_periods = ['AM']\n",
    "\n",
    "\n",
    "pp_df = []\n",
    "reg_df = []\n",
    "county_df = []\n",
    "sd_df = []\n",
    "tb_df = []\n",
    "rdm_df = []\n",
    "\n",
    "\n",
    "for columns in summary_cols:\n",
    "    \n",
    "    for period in time_periods:\n",
    "    \n",
    "        df_temp = df_trn_rail[(df_trn_rail[columns] > 0) & (df_trn_rail['Period'] == period.lower())]\n",
    "        \n",
    "        #regional value\n",
    "        region_value = df_temp[columns].mean()\n",
    "        reg_df_temp = pd.DataFrame({'Population': 'Whole Population',\n",
    "                              'Period': period,\n",
    "                              'Value': region_value,\n",
    "                              'Orig_zone' : '',\n",
    "                              'Dest_zone' : '',\n",
    "                              'Zone_ID' : 'Megaregion',\n",
    "                              'Geography' : 'Regional',\n",
    "                              'Metric':summary_cols[columns][1],\n",
    "                              'Submetric':summary_cols[columns][1]+'.1',\n",
    "                              'Description' : summary_cols[columns][0],\n",
    "                              'Units' : summary_cols[columns][2],\n",
    "                              'Metric_name' : summary_cols[columns][3]}, index=[0])\n",
    "        reg_df.append(reg_df_temp)\n",
    "        \n",
    "        # transbay region\n",
    "        tb_value = df_temp[df_temp['transbay_od']==1]\n",
    "        tb_value = tb_value[columns].mean()\n",
    "        \n",
    "        tb_df_temp = pd.DataFrame({'Population': 'Whole Population',\n",
    "                                   'Period': period,\n",
    "                                   'Value': tb_value,\n",
    "                                   'Orig_zone' : '',\n",
    "                                   'Dest_zone' : '',\n",
    "                                   'Zone_ID' : 'Megaregion',\n",
    "                                   'Geography' : 'Transbay',\n",
    "                                   'Metric':summary_cols[columns][1],\n",
    "                                   'Submetric':summary_cols[columns][1]+'.6',\n",
    "                                   'Description' : summary_cols[columns][0],\n",
    "                                   'Units' : summary_cols[columns][2],\n",
    "                                   'Metric_name' : summary_cols[columns][3]}, index=[0])\n",
    "                                    \n",
    "        tb_df.append(tb_df_temp)  \n",
    "                \n",
    "        #county \n",
    "        county_df_temp = df_temp.groupby(['orig_county', 'dest_county'])[columns].mean().reset_index()\n",
    "        county_df_temp = rename_columns(county_df_temp, ['orig_county', 'dest_county', columns])\n",
    "        \n",
    "        county_df_temp['Period'] = period\n",
    "        county_df_temp['Population'] = 'Whole Population'\n",
    "        county_df_temp['Zone_Id'] = ''\n",
    "        county_df_temp['Geography'] = 'County'\n",
    "        county_df_temp['Description'] = summary_cols[columns][0]\n",
    "        county_df_temp['Metric'] = summary_cols[columns][1]\n",
    "        county_df_temp['Submetric'] = summary_cols[columns][1]+'.5'\n",
    "        county_df_temp['Units'] = summary_cols[columns][2]\n",
    "        county_df_temp['Metric_name'] = summary_cols[columns][3]\n",
    "        county_df.append(county_df_temp)\n",
    "        \n",
    "        #RDM Zones\n",
    "        rdm_df_temp = df_temp.groupby(['orig_rdm_zones', 'dest_rdm_zones'])[columns].mean().reset_index()\n",
    "        rdm_df_temp =  rename_columns(rdm_df_temp, ['orig_rdm_zones', 'dest_rdm_zones', columns])\n",
    "        \n",
    "        rdm_df_temp['Period'] = period\n",
    "        rdm_df_temp['Population'] = 'Whole Population'\n",
    "        rdm_df_temp['Zone_ID'] = ''\n",
    "        rdm_df_temp['Geography'] = 'RDM'\n",
    "        rdm_df_temp['Description'] = summary_cols[columns][0]     \n",
    "        rdm_df_temp['Metric'] = summary_cols[columns][1]\n",
    "        rdm_df_temp['Submetric'] = summary_cols[columns][1]+'.3'\n",
    "        rdm_df_temp['Units'] = summary_cols[columns][2]\n",
    "        rdm_df_temp['Metric_name'] = summary_cols[columns][3]\n",
    "        rdm_df.append(rdm_df_temp)\n",
    "        \n",
    "        #super district\n",
    "        sd_df_temp = df_temp.groupby(['orig_super_dist', 'dest_super_dist'])[columns].mean().reset_index()\n",
    "        sd_df_temp = rename_columns(sd_df_temp, ['orig_super_dist', 'dest_super_dist', columns])\n",
    "        \n",
    "        sd_df_temp['Period'] = period\n",
    "        sd_df_temp['Population'] = 'Whole Population'\n",
    "        sd_df_temp['Zone_ID'] = ''\n",
    "        sd_df_temp['Geography'] = 'Superdistrict'\n",
    "        sd_df_temp['Description'] = summary_cols[columns][0]\n",
    "        sd_df_temp['Metric'] = summary_cols[columns][1]\n",
    "        sd_df_temp['Submetric'] = summary_cols[columns][1]+'.4'\n",
    "        sd_df_temp['Units'] = summary_cols[columns][2]\n",
    "        sd_df_temp['Metric_name'] = summary_cols[columns][3]\n",
    "        sd_df.append(sd_df_temp)\n",
    "                \n",
    "        #prioirty population\n",
    "        df_temp['pp_share'] = df_temp['pp_share']/100\n",
    "        pp_value = weighted_average(df_temp, 'ivt', 'pp_share')\n",
    "        pp_df_temp = pd.DataFrame({'Population': 'Prioirty Population',\n",
    "                              'Period': period,\n",
    "                              'Value': pp_value,\n",
    "                              'Orig_zone' : '',\n",
    "                              'Dest_zone' : '',\n",
    "                              'Zone_ID' : 'Megaregion',\n",
    "                              'Geography' : 'Regional',\n",
    "                              'Metric':summary_cols[columns][1],\n",
    "                              'Submetric':summary_cols[columns][1]+'.2',\n",
    "                              'Description' : summary_cols[columns][0],\n",
    "                              'Units' : summary_cols[columns][2],\n",
    "                              'Metric_name' : summary_cols[columns][3]}, index=[0])\n",
    "        pp_df.append(pp_df_temp)\n",
    "    \n",
    "    \n",
    "pp_df = pd.concat(pp_df)\n",
    "reg_df = pd.concat(reg_df)\n",
    "county_df = pd.concat(county_df)\n",
    "sd_df = pd.concat(sd_df)\n",
    "rdm_df = pd.concat(rdm_df)\n",
    "tb_df = pd.concat(tb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae2674bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ivt': ['in-vehicle travel time',\n",
       "  'A1.2',\n",
       "  'minutes',\n",
       "  'actual in vehicle travel time'],\n",
       " 'wait': ['wait time', 'A1.3', 'minutes', 'actual wait time'],\n",
       " 'wacc': ['walk access time', 'A1.4', 'minutes', 'actual walk access time'],\n",
       " 'wegr': ['walk egress time', 'A1.5', 'minutes', 'actual walk egress time'],\n",
       " 'dtime': ['drive time', 'A1.6', 'minutes', 'actual drive time'],\n",
       " 'xwait': ['transfer time', 'A1.7', 'minutes', 'actual transfer time'],\n",
       " 'trip_time': ['total trip time', 'A1.8', 'minutes', 'actual total trip time']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#summary_cols = params['description_a1.2']\n",
    "#summary_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a6733965",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cols = {'ivt_sp_adj' : ['in-vehicle travel time', 'A1.2', 'minutes', 'actual in vehicle travel time'], \n",
    "                'trip_time_adj': ['total trip time', 'A1.8', 'minutes', 'actual total trip time']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5e61beaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\MTC_tmpy\\\\TM2_2050Baseline_R2_Run7\\\\performance_measures\\\\more_A1'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1efdb2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ivt_sp_adj\n",
      "30 A1.2.1 actual in vehicle travel time\n",
      "30 A1.2.2 actual in vehicle travel time\n",
      "2391 A1.2.3 actual in vehicle travel time\n",
      "625346 A1.2.4 actual in vehicle travel time\n",
      "23747 A1.2.5 actual in vehicle travel time\n",
      "30 A1.2.6 actual in vehicle travel time\n",
      "625346 A1.2.7 actual in vehicle travel time\n",
      "trip_time_adj\n",
      "30 A1.8.1 actual total trip time\n",
      "30 A1.8.2 actual total trip time\n",
      "2391 A1.8.3 actual total trip time\n",
      "625346 A1.8.4 actual total trip time\n",
      "23747 A1.8.5 actual total trip time\n",
      "30 A1.8.6 actual total trip time\n",
      "625346 A1.8.7 actual total trip time\n"
     ]
    }
   ],
   "source": [
    "#time_periods = ['AM']\n",
    "\n",
    "\n",
    "pp_df = []\n",
    "reg_df = []\n",
    "county_df = []\n",
    "sd_df = []\n",
    "tb_df = []\n",
    "rdm_df = []\n",
    "\n",
    "\n",
    "for columns in summary_cols:\n",
    "    \n",
    "    #for period in time_periods:\n",
    "    print(columns)\n",
    "    df_temp = df_trn_rail[(df_trn_rail[columns] > 0)]\n",
    "    df_temp[columns+'_trips'] = (df_temp[columns] * df_temp['trips'])/100\n",
    "\n",
    "    #regional value\n",
    "    reg_df_temp1 = summarize_all_combinations(df_temp, groupby_columns=['Period', 'Income'], \n",
    "                                                  summary_column=columns+'_trips')\n",
    "    reg_df_temp1 = reg_df_temp1.rename(columns={'Value': columns+'_trips'})\n",
    "    \n",
    "    reg_df_temp2 = summarize_all_combinations(df_temp, groupby_columns=['Period', 'Income'], \n",
    "                                                  summary_column='trips')\n",
    "    reg_df_temp2 = reg_df_temp2.rename(columns={'Value': 'trips'})\n",
    "    \n",
    "    #reg_df_temp1 = df_temp.groupby(['Period'])[columns+'_trips'].sum().reset_index()\n",
    "    #reg_df_temp2 = df_temp.groupby(['Period'])['trips'].sum().reset_index()\n",
    "    reg_df_temp = pd.merge(reg_df_temp1, reg_df_temp2, on=['Period', 'Income'], how='left')\n",
    "    reg_df_temp[columns] = reg_df_temp[columns+'_trips']/reg_df_temp['trips']\n",
    "    reg_df_temp = reg_df_temp[['Period', 'Income', columns]]\n",
    "    reg_df_temp = reg_df_temp.rename(columns={columns:'Value'})\n",
    "    reg_df_temp['Population'] = 'Whole Population'\n",
    "    reg_df_temp['Origin_zone'] = ''\n",
    "    reg_df_temp['Dest_zone'] = ''\n",
    "    reg_df_temp['Zone_ID'] = ''\n",
    "    reg_df_temp['Concept_ID'] = concept_id\n",
    "    reg_df_temp['Geography'] = 'Regional'\n",
    "    reg_df_temp['Purpose'] = ''\n",
    "    reg_df_temp['Mode'] = ''\n",
    "    reg_df_temp['Total_Increment'] = ''\n",
    "    reg_df_temp['Metric_ID'] = summary_cols[columns][1]\n",
    "    reg_df_temp['Submetric'] = summary_cols[columns][1]+'.1'\n",
    "    reg_df_temp['Description'] = 'Average ' + summary_cols[columns][0]+ ' in the region'\n",
    "    reg_df_temp['Units'] = summary_cols[columns][2]\n",
    "    reg_df_temp['Metric_name'] = summary_cols[columns][3]\n",
    "    #reg_df.append(reg_df_temp)\n",
    "\n",
    "    # transbay region\n",
    "    tb_df_t = df_temp[df_temp['transbay_od']==1]\n",
    "    tb_df_temp = tb_df_t.copy()\n",
    "    tb_df_temp1 = summarize_all_combinations(tb_df_temp, groupby_columns=['Period', 'Income'], \n",
    "                                                  summary_column=columns+'_trips')\n",
    "    tb_df_temp1 = tb_df_temp1.rename(columns={'Value': columns+'_trips'})\n",
    "    \n",
    "    tb_df_temp2 = summarize_all_combinations(tb_df_temp, groupby_columns=['Period', 'Income'], \n",
    "                                                  summary_column='trips')\n",
    "    tb_df_temp2 = tb_df_temp2.rename(columns={'Value': 'trips'})\n",
    "    \n",
    "    #tb_df_temp1 = tb_df_temp.groupby(['Period'])[columns+'_trips'].sum().reset_index()\n",
    "    #tb_df_temp2 = tb_df_temp.groupby(['Period'])['trips'].sum().reset_index()\n",
    "    tb_df_temp = pd.merge(tb_df_temp1, tb_df_temp2, on=['Period', 'Income'], how='left')\n",
    "    tb_df_temp[columns] = tb_df_temp[columns+'_trips']/tb_df_temp['trips']\n",
    "    tb_df_temp = tb_df_temp[['Period', 'Income', columns]]\n",
    "    tb_df_temp = tb_df_temp.rename(columns={columns:'Value'})\n",
    "\n",
    "    tb_df_temp['Concept_ID'] = concept_id\n",
    "    tb_df_temp['Population'] = 'Whole Population'\n",
    "    tb_df_temp['Origin_zone'] = ''\n",
    "    tb_df_temp['Dest_zone'] = ''\n",
    "    tb_df_temp['Zone_ID'] = ''\n",
    "    tb_df_temp['Purpose'] = ''\n",
    "    tb_df_temp['Mode'] = ''\n",
    "    tb_df_temp['Geography'] = 'Transbay'\n",
    "    tb_df_temp['Total_Increment'] = ''\n",
    "    tb_df_temp['Metric_ID'] = summary_cols[columns][1]\n",
    "    tb_df_temp['Submetric'] = summary_cols[columns][1]+'.2'\n",
    "    tb_df_temp['Description'] = 'Average' + summary_cols[columns][0] + ' in the transbay region'\n",
    "    tb_df_temp['Units'] = summary_cols[columns][2]\n",
    "    tb_df_temp['Metric_name'] = summary_cols[columns][3]\n",
    "\n",
    "    #tb_df.append(tb_df_temp)         \n",
    "\n",
    "    #county \n",
    "    #county_df_temp1 = df_temp.groupby(['orig_county', 'dest_county', 'Period'])[columns+'_trips'].sum().reset_index()\n",
    "    #county_df_temp2 = df_temp.groupby(['orig_county', 'dest_county', 'Period'])['trips'].sum().reset_index()\n",
    "    county_df_temp1 = summarize_all_combinations(df_temp, groupby_columns=['orig_county', 'dest_county', 'Period', 'Income'], \n",
    "                                                  summary_column=columns+'_trips')\n",
    "    county_df_temp1 = county_df_temp1.rename(columns={'Value': columns+'_trips'})\n",
    "    \n",
    "    county_df_temp2 = summarize_all_combinations(df_temp, groupby_columns=['orig_county', 'dest_county', 'Period', 'Income'], \n",
    "                                                  summary_column='trips')\n",
    "    county_df_temp2 = county_df_temp2.rename(columns={'Value': 'trips'})\n",
    "    \n",
    "    county_df_temp = pd.merge(county_df_temp1, county_df_temp2, on=['orig_county', 'dest_county', 'Period', 'Income'], how='left')\n",
    "    county_df_temp[columns] = county_df_temp[columns+'_trips']/county_df_temp['trips']\n",
    "    county_df_temp = county_df_temp[['orig_county', 'dest_county', 'Period', 'Income' ,columns]]\n",
    "    county_df_temp = county_df_temp.rename(columns={columns:'Value', \n",
    "                                                    'orig_county': 'Origin_zone',\n",
    "                                                    'dest_county': 'Dest_zone'})\n",
    "    county_df_temp['Population'] = 'Whole Population'\n",
    "    county_df_temp['Zone_ID'] = ''\n",
    "    county_df_temp['Concept_ID'] = concept_id\n",
    "    county_df_temp['Geography'] = 'County'\n",
    "    county_df_temp['Purpose'] = ''\n",
    "    county_df_temp['Mode'] = ''\n",
    "    county_df_temp['Total_Increment'] = ''\n",
    "    county_df_temp['Description'] = 'Average ' + summary_cols[columns][0] + ' in the origin and destination county'\n",
    "    county_df_temp['Metric_ID'] = summary_cols[columns][1]\n",
    "    county_df_temp['Submetric'] = summary_cols[columns][1]+'.3'\n",
    "    county_df_temp['Units'] = summary_cols[columns][2]\n",
    "    county_df_temp['Metric_name'] = summary_cols[columns][3]\n",
    "    #county_df.append(county_df_temp)\n",
    "\n",
    "\n",
    "    #RDM Zones\n",
    "    rdm_df_temp1 = summarize_all_combinations(df_temp, groupby_columns=['orig_rdm_zones', 'dest_rdm_zones', 'Period', 'Income'], \n",
    "                                                  summary_column=columns+'_trips')\n",
    "    rdm_df_temp1 = rdm_df_temp1.rename(columns={'Value': columns+'_trips'})\n",
    "    \n",
    "    rdm_df_temp2 = summarize_all_combinations(df_temp, groupby_columns=['orig_rdm_zones', 'dest_rdm_zones', 'Period', 'Income'], \n",
    "                                                  summary_column='trips')\n",
    "    rdm_df_temp2 = rdm_df_temp2.rename(columns={'Value': 'trips'})\n",
    "    #rdm_df_temp1 = df_temp.groupby(['orig_rdm_zones', 'dest_rdm_zones', 'Period'])[columns+'_trips'].sum().reset_index()\n",
    "    #rdm_df_temp2 = df_temp.groupby(['orig_rdm_zones', 'dest_rdm_zones', 'Period'])['trips'].sum().reset_index()\n",
    "    rdm_df_temp = pd.merge(rdm_df_temp1, rdm_df_temp2, on=['orig_rdm_zones', 'dest_rdm_zones', 'Period', 'Income'], how='left')\n",
    "    rdm_df_temp[columns] = rdm_df_temp[columns+'_trips']/rdm_df_temp['trips']\n",
    "    rdm_df_temp = rdm_df_temp[['orig_rdm_zones', 'dest_rdm_zones', 'Period', 'Income', columns]]\n",
    "    rdm_df_temp = rdm_df_temp.rename(columns={'orig_rdm_zones': 'Origin_zone', \n",
    "                                              'dest_rdm_zones': 'Dest_zone', \n",
    "                                              columns: 'Value'})\n",
    "\n",
    "    rdm_df_temp['Population'] = 'Whole Population'\n",
    "    rdm_df_temp['Zone_ID'] = ''\n",
    "    rdm_df_temp['Geography'] = 'RDM'\n",
    "    rdm_df_temp['Concept_ID'] = concept_id\n",
    "    rdm_df_temp['Purpose'] = ''\n",
    "    rdm_df_temp['Mode'] = ''\n",
    "    rdm_df_temp['Total_Increment'] = ''\n",
    "    rdm_df_temp['Description'] = summary_cols[columns][0] + ' in the origin and destination RDM zones'  \n",
    "    rdm_df_temp['Metric_ID'] = summary_cols[columns][1]\n",
    "    rdm_df_temp['Submetric'] = summary_cols[columns][1]+'.4'\n",
    "    rdm_df_temp['Units'] = summary_cols[columns][2]\n",
    "    rdm_df_temp['Metric_name'] = summary_cols[columns][3]\n",
    "    #rdm_df.append(rdm_df_temp)\n",
    "\n",
    "        #super district   \n",
    "    sd_df_temp1 = summarize_all_combinations(df_temp, groupby_columns=['orig_super_dist', 'dest_super_dist', 'Period', 'Income'], \n",
    "                                                  summary_column=columns+'_trips')\n",
    "    sd_df_temp1 = sd_df_temp1.rename(columns={'Value': columns+'_trips'})\n",
    "    \n",
    "    sd_df_temp2 = summarize_all_combinations(df_temp, groupby_columns=['orig_super_dist', 'dest_super_dist', 'Period', 'Income'], \n",
    "                                                  summary_column='trips')\n",
    "    sd_df_temp2 = sd_df_temp2.rename(columns={'Value': 'trips'})\n",
    "    #sd_df_temp1 = df_temp.groupby(['orig_super_dist', 'dest_super_dist', 'Period'])[columns+'_trips'].sum().reset_index()\n",
    "    #sd_df_temp2 = df_temp.groupby(['orig_super_dist', 'dest_super_dist', 'Period'])['trips'].sum().reset_index()\n",
    "    \n",
    "    sd_df_temp = pd.merge(sd_df_temp1, sd_df_temp2, on=['orig_super_dist', 'dest_super_dist', 'Period', 'Income'], how='left')\n",
    "    sd_df_temp[columns] = sd_df_temp[columns+'_trips']/sd_df_temp['trips']\n",
    "    sd_df_temp = sd_df_temp[['orig_super_dist', 'dest_super_dist', 'Period', 'Income', columns]]\n",
    "    sd_df_temp = sd_df_temp.rename(columns={'orig_super_dist': 'Origin_zone', \n",
    "                                            'dest_super_dist': 'Dest_zone',\n",
    "                                             columns: 'Value'})\n",
    "    sd_df_temp['Population'] = 'Whole Population'\n",
    "    sd_df_temp['Zone_ID'] = ''\n",
    "    sd_df_temp['Concept_ID'] = concept_id\n",
    "    sd_df_temp['Purpose'] = ''\n",
    "    sd_df_temp['Mode'] = ''\n",
    "    sd_df_temp['Total_Increment'] = ''\n",
    "    sd_df_temp['Geography'] = 'Superdistrict'\n",
    "    sd_df_temp['Description'] = summary_cols[columns][0] + ' in the origin and destination Super district'  \n",
    "    sd_df_temp['Metric_ID'] = summary_cols[columns][1]\n",
    "    sd_df_temp['Submetric'] = summary_cols[columns][1]+'.5'\n",
    "    sd_df_temp['Units'] = summary_cols[columns][2]\n",
    "    sd_df_temp['Metric_name'] = summary_cols[columns][3]\n",
    "    #sd_df.append(sd_df_temp)\n",
    "\n",
    "        #prioirty population\n",
    "    df_temp['pp_share'] = df_temp['pp_share']/100\n",
    "    df_temp['pp_trips'] = df_temp['pp_share'] * df_temp['trips']\n",
    "    df_temp['pp_'+columns] = df_temp['pp_trips'] * df_temp[columns] / 100\n",
    "    \n",
    "    pp_df_temp1 = summarize_all_combinations(df_temp, groupby_columns=['Period', 'Income'], \n",
    "                                                  summary_column='pp_'+columns)\n",
    "    pp_df_temp1 = pp_df_temp1.rename(columns={'Value': 'pp_'+columns})\n",
    "    \n",
    "    pp_df_temp2 = summarize_all_combinations(df_temp, groupby_columns=['Period', 'Income'], \n",
    "                                                  summary_column='pp_trips')\n",
    "    pp_df_temp2 = pp_df_temp2.rename(columns={'Value': 'pp_trips'})\n",
    "\n",
    "    #pp_df_temp1 = df_temp.groupby(['Period'])['pp_'+columns].sum().reset_index()\n",
    "    #pp_df_temp2 = df_temp.groupby(['Period'])['pp_trips'].sum().reset_index()\n",
    "    pp_df_temp = pd.merge(pp_df_temp1, pp_df_temp2, on=['Period', 'Income'], how='left')\n",
    "    pp_df_temp[columns] = pp_df_temp['pp_'+columns]/pp_df_temp['pp_trips']\n",
    "    pp_df_temp = pp_df_temp[['Period', 'Income' ,columns]]\n",
    "    pp_df_temp = pp_df_temp.rename(columns={columns:'Value'})\n",
    "\n",
    "    pp_df_temp['Population'] = 'Prioirty Population'\n",
    "    pp_df_temp['Origin_zone'] = ''\n",
    "    pp_df_temp['Dest_zone'] = ''\n",
    "    pp_df_temp['Zone_ID'] = ''\n",
    "    pp_df_temp['Purpose'] = ''\n",
    "    pp_df_temp['Mode'] = ''\n",
    "    pp_df_temp['Concept_ID'] = concept_id\n",
    "    pp_df_temp['Geography'] = 'Region'\n",
    "    pp_df_temp['Total_Increment'] = ''\n",
    "    pp_df_temp['Metric_ID'] = summary_cols[columns][1]\n",
    "    pp_df_temp['Submetric'] = summary_cols[columns][1]+'.6'\n",
    "    pp_df_temp['Description'] = summary_cols[columns][0] + ' in the region'\n",
    "    pp_df_temp['Units'] = summary_cols[columns][2]\n",
    "    pp_df_temp['Metric_name'] = summary_cols[columns][3]\n",
    "    #pp_df.append(pp_df_temp)\n",
    "    \n",
    "    pp_rdm_df_temp1 = summarize_all_combinations(df_temp, groupby_columns=['orig_rdm_zones', 'dest_rdm_zones', 'Period', 'Income'], \n",
    "                                                  summary_column='pp_'+columns)\n",
    "    pp_rdm_df_temp1 = pp_rdm_df_temp1.rename(columns={'Value': 'pp_'+columns})\n",
    "    \n",
    "    pp_rdm_df_temp2 = summarize_all_combinations(df_temp, groupby_columns=['orig_rdm_zones', 'dest_rdm_zones', 'Period', 'Income'], \n",
    "                                                  summary_column='pp_trips')\n",
    "    pp_rdm_df_temp2 = pp_rdm_df_temp2.rename(columns={'Value': 'pp_trips'})\n",
    "    \n",
    "    pp_rdm_df_temp = pd.merge(pp_rdm_df_temp1, pp_rdm_df_temp2, on=['orig_rdm_zones', 'dest_rdm_zones', 'Period', 'Income'], how='left')\n",
    "    pp_rdm_df_temp[columns] = pp_rdm_df_temp['pp_'+columns]/pp_rdm_df_temp['pp_trips']\n",
    "    pp_rdm_df_temp = pp_rdm_df_temp[['orig_rdm_zones', 'dest_rdm_zones', 'Period', 'Income', columns]]\n",
    "    pp_rdm_df_temp = pp_rdm_df_temp.rename(columns={columns:'Value',\n",
    "                                                    'orig_rdm_zones': 'Origin_zone', \n",
    "                                                    'dest_rdm_zones': 'Dest_zone'})\n",
    "    \n",
    "    pp_rdm_df_temp['Population'] = 'Prioirty Population'\n",
    "    #pp_rdm_df_temp['Origin_zone'] = ''\n",
    "    #pp_rdm_df_temp['Dest_zone'] = ''\n",
    "    pp_rdm_df_temp['Zone_ID'] = ''\n",
    "    pp_rdm_df_temp['Purpose'] = ''\n",
    "    pp_rdm_df_temp['Mode'] = ''\n",
    "    pp_rdm_df_temp['Concept_ID'] = concept_id\n",
    "    pp_rdm_df_temp['Geography'] = 'RDM'\n",
    "    pp_rdm_df_temp['Total_Increment'] = ''\n",
    "    pp_rdm_df_temp['Metric_ID'] = summary_cols[columns][1]\n",
    "    pp_rdm_df_temp['Submetric'] = summary_cols[columns][1]+'.7'\n",
    "    pp_rdm_df_temp['Description'] = 'Average ' + summary_cols[columns][0] + ' in the region'\n",
    "    pp_rdm_df_temp['Units'] = summary_cols[columns][2]\n",
    "    pp_rdm_df_temp['Metric_name'] = summary_cols[columns][3]\n",
    "    #pp_df_rdm.append(pp_rdm_df_temp)\n",
    "    \n",
    "    all_dfs = [reg_df_temp, tb_df_temp, county_df_temp, rdm_df_temp, sd_df_temp, pp_df_temp, pp_rdm_df_temp]\n",
    "\n",
    "    for dfs in all_dfs:\n",
    "        metric_name = '_' + summary_cols[columns][3].replace(' ', '_') + '_'\n",
    "        dfs = dfs.reset_index(drop=True)\n",
    "        dfs = dfs[perf_measure_columns]\n",
    "        file_name = dfs['Submetric'][0]\n",
    "        geography = '_' + dfs['Geography'][0].replace(' ', '_')\n",
    "        dfs.to_csv(_join(summary_dir, file_name + metric_name + concept_id + geography + filename_extension + '.csv'), index=None)\n",
    "        print(len(dfs), file_name, dfs['Metric_name'][0])\n",
    "\n",
    "    combined_df = pd.concat([reg_df_temp, tb_df_temp, county_df_temp, rdm_df_temp, df_temp, pp_df_temp]).reset_index(drop=True)\n",
    "    combined_df.to_csv(_join(summary_dir, summary_cols[columns][1] + metric_name + concept_id + '_region' +filename_extension+'.csv'), index=None)\n",
    "       \n",
    "#pp_df = pd.concat(pp_df)\n",
    "#reg_df = pd.concat(reg_df)\n",
    "#county_df = pd.concat(county_df)\n",
    "#sd_df = pd.concat(sd_df)\n",
    "#rdm_df = pd.concat(rdm_df)\n",
    "#tb_df = pd.concat(tb_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a508b939",
   "metadata": {},
   "source": [
    "all_dfs = pd.concat([reg_df, tb_df, county_df, rdm_df, sd_df, pp_df], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "unique_metric_ids = all_dfs['Metric_ID'].unique()\n",
    "\n",
    "for mids in unique_metric_ids:\n",
    "    print(mids)\n",
    "    dfs = all_dfs.loc[all_dfs['Metric_ID']==mids]\n",
    "    usbmi = dfs['Submetric'].unique()\n",
    "    \n",
    "    for mi in usbmi:\n",
    "        print(mi)\n",
    "        dfs_temp = dfs.loc[dfs['Submetric'] == mi]\n",
    "        dfs_temp = dfs_temp[perf_measure_columns]\n",
    "        dfs_temp = dfs_temp.reset_index(drop=True)\n",
    "        file_name = mi \n",
    "        metric_name = dfs_temp['Metric_name'][0].replace(' ', '_')\n",
    "        geography = '_' + dfs['Geography'][0].replace(' ', '_')\n",
    "        dfs_temp.to_csv(_join(summary_dir, file_name + '_' + metric_name + concept_id + geography + filename_extension + '.csv'), index=None)\n",
    "        print(len(dfs_temp), file_name, dfs_temp['Metric_name'][0], dfs_temp['Value'].sum())\n",
    "    \n",
    "    dfs = dfs.reset_index(drop=True)\n",
    "    file_name = mids \n",
    "    metric_name = dfs['Metric_name'][0]\n",
    "    dfs.to_csv(_join(summary_dir, file_name + metric_name + '_' + concept_id + '_region' + filename_extension +  '.csv'), index=None)\n",
    "    print(len(dfs), file_name, dfs['Metric_name'][0], dfs['Value'].sum())\n",
    "    \n",
    "    #'A3.1' + '_weekday_linked_trips_' + concept_id + '_region' +filename_extension+'.csv'\n",
    "    \n",
    "    #metric_name = dfs['Metric_name'][0]\n",
    "    #\n",
    "    #\n",
    "    #dfs.to_csv(_join(summary_dir, metric_name + file_name + filename_extension +  '.csv'), index=None)\n",
    "    #print(len(dfs), file_name, dfs['Metric_name'][0], dfs['Value'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b31594c",
   "metadata": {},
   "source": [
    "## B2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa80bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c6d8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73555e97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
