{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065cef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openmatrix as omx\n",
    "import random\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from utility import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7043479",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "    \n",
    "_join = os.path.join\n",
    "_dir = os.path.dirname\n",
    "_norm = os.path.normpath\n",
    "\n",
    "# paths\n",
    "model_outputs_dir = params['model_dir']\n",
    "skims_dir = _join(model_outputs_dir, \"skims\")\n",
    "summary_outputs = params['summary_dir']\n",
    "concept_id = params['concept_id']\n",
    "ctramp_dir = params['ctramp_dir']\n",
    "iteration = params['iteration']\n",
    "\n",
    "concept_id = params['concept_id']\n",
    "time_period_mapping = params['time_periods_mapping']\n",
    "link21_purp_mapping = params['purpose_mapping']\n",
    "mode_cat_mapping = params['mode_mapping']\n",
    "time_periods = params['periods']\n",
    "acc_egr = params['access_egress_modes']\n",
    "acc_egg_modes = params['access_egress_modes']\n",
    "\n",
    "income_categories_bins = params['income_categories_bins']\n",
    "preprocess_dir = _join(ctramp_dir, '_pre_process_files')\n",
    "perf_measure_columns = params['final_columns']\n",
    "\n",
    "perceived_tt_cores = params['perceived_travel_time']\n",
    "\n",
    "best_path_skim_extension = params['best_path_skim_extension']\n",
    "\n",
    "actual_tt_cores = params['total_travel_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1530d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folders to save files\n",
    "summary_dir = params['summary_dir']\n",
    "preprocess_dir = _join(ctramp_dir, '_pre_process_files')\n",
    "\n",
    "Path(summary_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(preprocess_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf3428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs of CT-RAMP model for tour and trip file\n",
    "household_model_dir = _join(model_outputs_dir, \"main\")\n",
    "\n",
    "# input household and person data\n",
    "person_file = _join(ctramp_dir, 'main\\\\personData_' + str(iteration) + '.csv')\n",
    "household_file = _join(ctramp_dir, 'main\\\\householdData_' + str(iteration) + '.csv')\n",
    "\n",
    "person = pd.read_csv(person_file)\n",
    "\n",
    "hh = pd.read_csv(household_file, usecols = ['hh_id', 'taz', 'income'])\n",
    "hh = hh.rename(columns = {'taz': 'home_zone'})\n",
    "\n",
    "#taz to RDM zones, super districts, county\n",
    "geo_cwks = pd.read_csv(_join(params['common_dir'], \"geographies.csv\")) #columns taz, rdm_zones, super_district, county\n",
    "\n",
    "#taz to priority population\n",
    "pp_perc = pd.read_excel(_join(params['common_dir'], \"TAZ_Tract_cwk_summary.xlsx\")) #columns = taz, pp_share \n",
    "\n",
    "# transbay od pairs\n",
    "transbay_od = pd.read_csv(_join(params['common_dir'], \"transbay_od.csv\")) #columns = transbay_o, transbay_d\n",
    "\n",
    "demand_matrices_dir = _join(model_outputs_dir, \"demand_matrices\")\n",
    "transit_demand_dir = _join(demand_matrices_dir, \"transit\")\n",
    "transit_skims_dir = _join(skims_dir, \"transit\")\n",
    "highway_skims_dir = _join(skims_dir, \"highway\")\n",
    "\n",
    "income_categories = params['income_categories_bins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca208af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#create trips\n",
    "df_trips = create_trip_roster(ctramp_dir, hh, pp_perc ,transbay_od, geo_cwks, link21_purp_mapping, iteration)\n",
    "\n",
    "df_trips = df_trips.drop(columns=['person_num', 'tour_id', 'stop_id', 'orig_walk_segment',\n",
    "                                  'tour_purpose', # 'orig_purpose', 'dest_purpose',\n",
    "                                 'dest_walk_segment', 'parking_taz', 'tour_mode', 'tour_category',\n",
    "                                 'avAvailable', 'num_participants', 'new_dest_purp',\n",
    "                                 'new_orig_purp', 'link21_tour_purp', 'link21_orig_purp',\n",
    "                                 'link21_dest_purp', 'taxiWait', 'singleTNCWait', 'sharedTNCWait'])\n",
    "\n",
    "df_trips['Period'] = df_trips['depart_hour'].map(time_period_mapping)\n",
    "df_trips['Mode'] = df_trips['trip_mode'].map(mode_cat_mapping)\n",
    "df_trips = df_trips.rename(columns={'income_bin' : 'Income'})\n",
    "\n",
    "df_trips.to_parquet(_join(preprocess_dir, 'trip_roster.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2e6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tours = create_tour_roster(ctramp_dir, hh, pp_perc ,transbay_od, geo_cwks, iteration)\n",
    "\n",
    "df_tours = df_tours.drop(columns=['person_num', 'person_type', 'tour_id',\n",
    "                                   'tour_category', 'orig_walk_segment', 'dest_walk_segment',\n",
    "                                  'atWork_freq', 'num_ob_stops', 'num_ib_stops', 'avAvailable',\n",
    "                                  'dcLogsum', 'origTaxiWait', 'destTaxiWait', 'origSingleTNCWait',\n",
    "                                  'destSingleTNCWait', 'origSharedTNCWait','destSharedTNCWait', \n",
    "                                  'tour_composition', 'tour_participants'])\n",
    "\n",
    "df_tours['Period'] = df_tours['start_hour'].map(time_period_mapping)\n",
    "df_tours['Mode'] = df_tours['tour_mode'].map(mode_cat_mapping)\n",
    "df_tours = df_tours.rename(columns={'income_bin' : 'Income'})\n",
    "\n",
    "df_tours.to_parquet(_join(preprocess_dir, 'tour_roster.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc3680",
   "metadata": {},
   "source": [
    "## Process Single Path Skims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dfb7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# find the best path with lowest travel time for each time period\n",
    "\n",
    "perc_tod_tt = []\n",
    "all_tod_tt = []\n",
    "for per in time_periods:\n",
    "    \n",
    "    perc_df = []\n",
    "    tt_df = []\n",
    "    for acc in acc_egg:\n",
    "        file_name = _join(params['best_path_skim_dir'], per+'_'+acc+ best_path_skim_extension+'.omx')\n",
    "        if os.path.exists(file_name):\n",
    "            print(file_name)\n",
    "            skim = omx.open_file(file_name)\n",
    "            \n",
    "            # empty matrix \n",
    "            actl_mat_core = np.zeros(skim.shape())\n",
    "            percvd_mat_core = np.zeros(skim.shape())\n",
    "            \n",
    "            # iterate over all cores to get total travel time\n",
    "            for core in actual_tt_cores:\n",
    "                actl_mat_core = actl_mat_core + np.array(skim[core])\n",
    "                #print(mat_core.sum())\n",
    "                \n",
    "            for core in perceived_tt_cores:\n",
    "                percvd_mat_core = percvd_mat_core + np.array(skim[core])\n",
    "\n",
    "            df = pd.DataFrame(actl_mat_core)\n",
    "            df = pd.melt(df.reset_index(), id_vars='index', value_vars=df.columns)\n",
    "            df['index'] = df['index'] + 1\n",
    "            df['variable'] = df['variable'] + 1\n",
    "            df.columns = ['orig', 'dest', 'tt']\n",
    "            df['acc_egr'] = acc\n",
    "            tt_df.append(df)\n",
    "            \n",
    "            df = pd.DataFrame(percvd_mat_core)\n",
    "            df = pd.melt(df.reset_index(), id_vars='index', value_vars=df.columns)\n",
    "            df['index'] = df['index'] + 1\n",
    "            df['variable'] = df['variable'] + 1\n",
    "            df.columns = ['orig', 'dest', 'tt']\n",
    "            df['acc_egr'] = acc\n",
    "            perc_df.append(df)\n",
    "            \n",
    "        else:\n",
    "            print(f'{file_name} doesn\\'t exist')\n",
    "    \n",
    "    if len(tt_df)>0:\n",
    "        df_temp = pd.concat(tt_df)\n",
    "        print(acc_egr)\n",
    "        df_temp = pd.pivot(df_temp, index=['orig', 'dest'], columns = ['acc_egr'], values = 'tt').reset_index()\n",
    "        df_temp['min_tt'] = df_temp[acc_egr][df_temp[acc_egr] > 0].min(axis=1)\n",
    "        df_temp = df_temp[['orig', 'dest', 'min_tt']]\n",
    "        df_temp['tp'] = per\n",
    "        all_tod_tt.append(df_temp)\n",
    "        \n",
    "    if len(perc_df)>0:\n",
    "        df_temp = pd.concat(perc_df)\n",
    "        df_temp = pd.pivot(df_temp, index=['orig', 'dest'], columns = ['acc_egr'], values = 'tt').reset_index()\n",
    "        df_temp['min_tt'] = df_temp[acc_egr][df_temp[acc_egr] > 0].min(axis=1)\n",
    "        df_temp = df_temp[['orig', 'dest', 'min_tt']]\n",
    "        df_temp['tp'] = per\n",
    "        perc_tod_tt.append(df_temp)\n",
    "    \n",
    "    else:\n",
    "        continue\n",
    "\n",
    "all_tod_tt = pd.concat(all_tod_tt)\n",
    "all_tod_tt = pd.pivot(all_tod_tt, index=['orig', 'dest'], columns = ['tp'], values = 'min_tt')  \n",
    "\n",
    "\n",
    "perc_tod_tt = pd.concat(perc_tod_tt)\n",
    "perc_tod_tt = pd.pivot(perc_tod_tt, index=['orig', 'dest'], columns = ['tp'], values = 'min_tt')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dca7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tod_tt.to_parquet(_join(preprocess_dir, 'tod_min_actual_travel_time.parquet'))\n",
    "perc_tod_tt.to_parquet(_join(preprocess_dir, 'tod_min_perceived_travel_time.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510b9e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tod_tt.to_csv(_join(preprocess_dir, 'tod_min_actual_travel_time.csv.gz'), compression='gzip')\n",
    "perc_tod_tt.to_csv(_join(preprocess_dir, 'tod_min_perceived_travel_time.csv.gz'), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea49853",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#create attributes for all time periods and access and egress modes\n",
    "for period in time_periods:\n",
    "    \n",
    "    for acc_egr in acc_egg_modes:\n",
    "        \n",
    "        file_name = _join(params['best_path_skim_dir'], period+'_'+acc_egr+ best_path_skim_extension+'.omx')\n",
    "    \n",
    "        if os.path.exists(file_name):\n",
    "            print(file_name)\n",
    "            skim = omx.open_file(file_name)\n",
    "\n",
    "            trip_time = np.array(skim['IVT']) + np.array(skim['DTIME']) + np.array(skim['WACC']) + \\\n",
    "                        np.array(skim['WAIT']) + np.array(skim['WAUX']) + np.array(skim['WEGR'])\n",
    "\n",
    "            ttime = array2df(trip_time, cols = ['orig', 'dest', 'trip_time'])\n",
    "\n",
    "            ivt = skim_core_to_df(skim, 'IVT', cols =['orig', 'dest', 'ivt'])\n",
    "            wacc = skim_core_to_df(skim, 'WACC', cols =['orig', 'dest', 'wacc'])\n",
    "            wait = skim_core_to_df(skim, 'WAIT', cols =['orig', 'dest', 'wait'])\n",
    "            wegr = skim_core_to_df(skim, 'WEGR', cols =['orig', 'dest', 'wegr'])\n",
    "            dtime = skim_core_to_df(skim, 'DTIME', cols =['orig', 'dest', 'dtime'])\n",
    "            xwait = skim_core_to_df(skim, 'XWAIT', cols =['orig', 'dest', 'xwait'])\n",
    "\n",
    "            df_trn_skim = pd.merge(ivt, wacc, on = ['orig', 'dest'], how='left').merge(\n",
    "                                   wait, on=['orig', 'dest'], how = 'left').merge(\n",
    "                                   wegr, on=['orig', 'dest'], how = 'left').merge(\n",
    "                                   dtime, on=['orig', 'dest'], how = 'left').merge(\n",
    "                                   xwait, on=['orig', 'dest'], how = 'left').merge(\n",
    "                                   ttime, on=['orig', 'dest'], how = 'left')\n",
    "\n",
    "            skim.close()\n",
    "\n",
    "            df_trn_skim.to_parquet(_join(preprocess_dir, period.lower() +'_'+ acc_egr +'_cores.parquet'))\n",
    "\n",
    "        else:\n",
    "            print(f\"file doesn't exist for time period: {period} and mode : {acc_egg}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
