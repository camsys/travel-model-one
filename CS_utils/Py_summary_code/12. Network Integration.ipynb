{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openmatrix as omx\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "from utility import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a035708",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "    \n",
    "_join = os.path.join\n",
    "_dir = os.path.dirname\n",
    "_norm = os.path.normpath\n",
    "\n",
    "# paths\n",
    "model_outputs_dir = params['model_dir']\n",
    "skims_dir = _join(model_outputs_dir, \"skims\")\n",
    "summary_outputs = params['summary_dir']\n",
    "concept_id = params['concept_id']\n",
    "ctramp_dir = params['ctramp_dir']\n",
    "iteration = params['iteration']\n",
    "\n",
    "concept_id = params['concept_id']\n",
    "\n",
    "perf_measure_columns = params['final_columns']\n",
    "summary_dir = params['summary_dir']\n",
    "filename_extension = params['filename_extension']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e11bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing_station_paris(stns_df):\n",
    "    \"\"\" add time of day, clean station names, \n",
    "        \n",
    "        selecting the row (of the set of duplicates) with the lowest number of \n",
    "        transfers by time period and station pair. \n",
    "        \n",
    "        Station names – in GTFS, each station is represented by at least 2 platforms (e.g., northbound and southbound). \n",
    "        So we should group platforms together that belong to the same station. Please note that these GTFS files \n",
    "        include the HSR stations, including ones in the Los Angeles area.\n",
    "        \n",
    "        Time period – for GTFS processing, I’ve developed 6 time periods, with overnight being \n",
    "        encompassing 1900-2600 (late night services that run past midnight are coded as 24:00:00 in GTFS) and \n",
    "        0000-0300. These two periods will need to be combined into a single ‘overnight’ time period.\n",
    "        \n",
    "        Duplicate rows – After doing the above two tests, you’ll have a number of duplicate rows for \n",
    "        each station pair and time period. This is because there are paths from many time points: \n",
    "        sometimes a 1-transfer movement is faster than a no-transfer movement, sometimes it’s vice-versa. \n",
    "        Because this is a connectivity metric, I would suggest selecting the row (of the set of duplicates) with the lowest number of transfers by time period and station pair. \n",
    "        High-Speed Rail – The GTFS files include HSR, so you might want to/need to filter out those station pairs. I am not sure if the BC team wants to include those; I believe the metric is for “regional rail/BART” services.\n",
    "        \n",
    "        Summarization – after the above three steps, you’ll have to develop the metric. \n",
    "        I think they want the number of station pairs by 0/1/2/3+ transfers. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    tod_dict = {'0300-0600' : 'EA', \n",
    "                '0600-1000' : 'AM',\n",
    "                '1000-1500' : 'MD',\n",
    "                '1500-1900' : 'PM',\n",
    "                '1900-2600' : 'EV'}\n",
    "    \n",
    "    \n",
    "    columns = ['stop_name.x', 'stop_name.y']\n",
    "    strngs_replace = [' - Northbound', ' - Southbound', ' - Westbound', ' - Eastbound']\n",
    "\n",
    "    for col in columns:\n",
    "        for strng in strngs_replace:\n",
    "            stns[col] = stns[col].str.replace(strng,'')\n",
    "    \n",
    "    stns['Period'] = stns['time_period'].map(tod_dict)\n",
    "    \n",
    "    stn_pairs = stns.groupby(['stop_name.x', 'stop_name.y', 'Period'])['transfers'].min().reset_index()\n",
    "    \n",
    "    \n",
    "    #TO DO Ask BC team about removing HSR stations\n",
    "    stn_pairs.loc[stn_pairs['transfers']>=2, 'transfers'] = '2+'\n",
    "    stn_pairs['Value'] = 1\n",
    "    \n",
    "    trnsfers = stn_pairs.groupby(['Period', 'transfers'])['Value'].sum().reset_index()\n",
    "    \n",
    "    return trnsfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01686c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transfers_performance_measure(stn_pairs, concept_id):\n",
    "    \"\"\"populates all the relevant columns for final performance measure\n",
    "    \"\"\"\n",
    "    \n",
    "    stn_pairs = stn_pairs.rename(columns={'transfers' : 'Zone_ID'})\n",
    "    stn_pairs['Concept_ID'] = concept_id\n",
    "    stn_pairs['Metric_ID'] = 'A1.12'\n",
    "    stn_pairs['Metric_name'] = 'Network integration (stations)'\n",
    "    stn_pairs['Submetric'] = ''\n",
    "    stn_pairs['Description'] =  'Connectivity at different number of transfers. The Zone ID refers to number of transfers'\n",
    "    stn_pairs['Population'] = 'Whole Population'\n",
    "    stn_pairs['Geography'] = 'Regional'\n",
    "    stn_pairs['Origin_zone'] = ''\n",
    "    stn_pairs['Dest_zone'] = ''\n",
    "    stn_pairs['Purpose'] = ''\n",
    "    stn_pairs['Mode'] = ''\n",
    "    stn_pairs['Income'] = ''\n",
    "    stn_pairs['Units'] = 'Number of station pairs'\n",
    "    stn_pairs['Total_Increment'] = ''\n",
    "    \n",
    "    return stn_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3af9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: change the path  and file name here\n",
    "stns = pd.read_csv(_join(ctramp_dir, r\"R2R40.csv\"))  \n",
    "\n",
    "\n",
    "stn_pairs = post_processing_station_paris(stns)\n",
    "\n",
    "\n",
    "final_df = summarize_all_combinations(stn_pairs, \n",
    "                                      groupby_columns=['Period', 'transfers'], \n",
    "                                      summary_column='Value')\n",
    "\n",
    "final_df = create_transfers_performance_measure(final_df, concept_id)\n",
    "final_df = final_df[perf_measure_columns]\n",
    "#final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dd5d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_df.to_csv(_join(summary_dir, 'A3.3' + '_mode_shares_' + concept_id + '_region' + filename_extension+'.csv'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d5ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(_join(summary_dir, 'A1.12' + '_network_integration_' + concept_id + '_region' + filename_extension+'.csv'), index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
