{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ccda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openmatrix as omx\n",
    "import random\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from utility import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c7ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "    \n",
    "_join = os.path.join\n",
    "_dir = os.path.dirname\n",
    "_norm = os.path.normpath\n",
    "\n",
    "# paths\n",
    "ctramp_dir = params['ctramp_dir']\n",
    "model_outputs_dir = params['model_dir']\n",
    "summary_dir = params['summary_dir']\n",
    "concept_id = params['concept_id']\n",
    "preprocess_dir = _join(ctramp_dir, '_pre_process_files')\n",
    "perf_measure_columns = params['final_columns']\n",
    "model_year = params['model_year']\n",
    "filename_extension = params['filename_extension']\n",
    "hwy_skims_dir = _join(model_outputs_dir, r'skims\\highway' )\n",
    "\n",
    "skims_dir = _join(model_outputs_dir, r'skims')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b397a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(summary_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(preprocess_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff847daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose = ['Work', 'University', 'School', 'Escort', 'Shopping', 'EatOut', \n",
    "           'OthMaint', 'Social', 'OthDiscr', 'WorkBased']\n",
    "\n",
    "time_period = {1:'EA',2:'AM',3:'MD',4:'PM',5:'EV'} #1 for EA, 2 for AM, 3 for MD, 4 for PM and 5 for EV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e3f735",
   "metadata": {},
   "source": [
    "### Calculate the taxi wait time for each origin zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "taz = pd.read_csv(_join(ctramp_dir, 'landuse', 'tazData_' + str(model_year) + '.csv'))\n",
    "taz['popEmpSqMile'] = (taz['TOTPOP'] + taz['TOTEMP']) / (taz['TOTACRE'] * 0.0015625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a63bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "taz = taz[['ZONE', 'popEmpSqMile']]\n",
    "\n",
    "# TNC \n",
    "#TNC_single_waitTime_mean =  10.3,8.5,8.4,6.3,3.0\n",
    "#TNC_single_waitTime_sd =     4.1,4.1,4.1,4.1,2.0\n",
    "\n",
    "#TNC_shared_waitTime_mean =  15.0,15.0,11.0,8.0,5.0\n",
    "#TNC_shared_waitTime_sd =     4.1,4.1,4.1,4.1,2.0\n",
    "\n",
    "#Taxi_waitTime_mean = 26.5,17.3,13.3,9.5,5.5\n",
    "#Taxi_waitTime_sd =    6.4,6.4,6.4,6.4,6.4\n",
    "\n",
    "#WaitTimeDistribution_EndPopEmpPerSqMi = 500,2000,5000,15000,9999999999\n",
    "\n",
    "#TO DO: Ask John which wait time to use\n",
    "taz['density_group'] = pd.cut(taz['popEmpSqMile'], bins= [-1, 500,2000,5000,15000,9999999999], \n",
    "                              labels=[10.3,8.5,8.4,6.3,3.0], ordered=False)\n",
    "#taz['density_group'] = taz['density_group'].fillna(0)\n",
    "taz['density_group'] =taz['density_group'].astype(\"int64\")\n",
    "\n",
    "taz = taz.sort_values('ZONE')\n",
    "taxi_wait_time = np.repeat(taz['density_group'].values, len(taz)).reshape(len(taz), len(taz))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe0248",
   "metadata": {},
   "source": [
    "### Load all the data from Skims "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3f363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# The data tab of the UEC file lists all the matrix cores and location an matrix files of skims\n",
    "# 1 for EA, 2 for AM, 3 for MD, 4 for PM and 5 for EV\n",
    "\n",
    "# extract the file names, matrix cores \n",
    "matrix_df = pd.read_excel(_join(params['common_dir'], r\"TripModeChoice.xlsx\"), sheet_name='data')\n",
    "matrix_df = matrix_df.iloc[9:]\n",
    "matrix_df.columns = ['no', 'token', 'format', 'file','matrix', 'group', 'index']\n",
    "#matrix_df[1:5]\n",
    "\n",
    "# pre-processing\n",
    "matrix_df['matrix_files'] = matrix_df['file'].str.replace('skims/', '')\n",
    "matrix_df['path'] = 'skims'\n",
    "matrix_df.loc[matrix_df['matrix_files'].str.contains('nonmot')==True, 'path'] = 'active'\n",
    "matrix_df.loc[matrix_df['matrix_files'].str.contains('trnskm')==True, 'path'] = 'transit'\n",
    "matrix_df.loc[matrix_df['matrix_files'].str.contains('hwyskm')==True, 'path'] = 'highway'\n",
    "#matrix_df[1:5]\n",
    "\n",
    "# Iterate over the DataFrame rows\n",
    "for _, row in matrix_df.iterrows():\n",
    "    variable_name = row['token']\n",
    "    file_path = row['path']\n",
    "    filename = row['matrix_files']\n",
    "    matrix_cr = row['matrix']\n",
    "    \n",
    "    # Extract the variable name and index (if present)\n",
    "    if '[' in variable_name:\n",
    "        name_start = variable_name.index('[')\n",
    "        name_end = variable_name.index(']')\n",
    "        index = int(variable_name[name_start+1:name_end])\n",
    "        variable_name = variable_name[:name_start]\n",
    "    else:\n",
    "        index=None\n",
    "    \n",
    "    # Read the file using numpy.load() and assign it to the variable with the specified index\n",
    "    file = omx.open_file(_join(skims_dir, file_path, filename))\n",
    "    file_contents = np.array(file[matrix_cr])\n",
    "    print(variable_name,index, _join(skims_dir, file_path, filename), file_contents.sum(), file_contents.min(), file_contents.max())\n",
    "    if '[' in row['token']:\n",
    "        if variable_name in locals() and isinstance(locals()[variable_name], np.ndarray):\n",
    "            arr = locals()[variable_name]\n",
    "            if index >= len(arr):\n",
    "                # Resize the array if the index is out of bounds\n",
    "                new_arr = np.resize(arr, index + 1)\n",
    "                new_arr[index] = file_contents\n",
    "                locals()[variable_name] = new_arr\n",
    "            else:\n",
    "                arr[index] = file_contents\n",
    "        else:\n",
    "            arr = np.empty(index + 1, dtype=object)\n",
    "            arr[index] = file_contents\n",
    "            locals()[variable_name] = arr\n",
    "    else:\n",
    "        locals()[variable_name] = file_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1c06b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change th 1000000.0 values in DISTWALK and DISTBIKE to 0\n",
    "DISTWALK = np.where(DISTWALK == 1000000.0, 0, DISTWALK)\n",
    "DISTBIKE = np.where(DISTBIKE == 1000000.0, 0, DISTBIKE)\n",
    "\n",
    "PNR_TRN_WLK_DTIM[1] =  np.where(PNR_TRN_WLK_DTIM[1] < 0, 0, PNR_TRN_WLK_DTIM[1])\n",
    "PNR_TRN_WLK_DTIM[2] =  np.where(PNR_TRN_WLK_DTIM[2] < 0, 0, PNR_TRN_WLK_DTIM[2])\n",
    "PNR_TRN_WLK_DTIM[3] =  np.where(PNR_TRN_WLK_DTIM[3] < 0, 0, PNR_TRN_WLK_DTIM[3])\n",
    "PNR_TRN_WLK_DTIM[4] =  np.where(PNR_TRN_WLK_DTIM[4] < 0, 0, PNR_TRN_WLK_DTIM[4])\n",
    "PNR_TRN_WLK_DTIM[5] =  np.where(PNR_TRN_WLK_DTIM[5] < 0, 0, PNR_TRN_WLK_DTIM[5])\n",
    "\n",
    "KNR_TRN_WLK_DTIM[1] =  np.where(KNR_TRN_WLK_DTIM[1] < 0, 0, KNR_TRN_WLK_DTIM[1])\n",
    "KNR_TRN_WLK_DTIM[2] =  np.where(KNR_TRN_WLK_DTIM[2] < 0, 0, KNR_TRN_WLK_DTIM[2])\n",
    "KNR_TRN_WLK_DTIM[3] =  np.where(KNR_TRN_WLK_DTIM[3] < 0, 0, KNR_TRN_WLK_DTIM[3])\n",
    "KNR_TRN_WLK_DTIM[4] =  np.where(KNR_TRN_WLK_DTIM[4] < 0, 0, KNR_TRN_WLK_DTIM[4])\n",
    "KNR_TRN_WLK_DTIM[5] =  np.where(KNR_TRN_WLK_DTIM[5] < 0, 0, KNR_TRN_WLK_DTIM[5])\n",
    "\n",
    "WLK_TRN_PNR_DTIM[1] =  np.where(WLK_TRN_PNR_DTIM[1] < 0, 0, WLK_TRN_PNR_DTIM[1])\n",
    "WLK_TRN_PNR_DTIM[2] =  np.where(WLK_TRN_PNR_DTIM[2] < 0, 0, WLK_TRN_PNR_DTIM[2])\n",
    "WLK_TRN_PNR_DTIM[3] =  np.where(WLK_TRN_PNR_DTIM[3] < 0, 0, WLK_TRN_PNR_DTIM[3])\n",
    "WLK_TRN_PNR_DTIM[4] =  np.where(WLK_TRN_PNR_DTIM[4] < 0, 0, WLK_TRN_PNR_DTIM[4])\n",
    "WLK_TRN_PNR_DTIM[5] =  np.where(WLK_TRN_PNR_DTIM[5] < 0, 0, WLK_TRN_PNR_DTIM[5])\n",
    "\n",
    "WLK_TRN_KNR_DTIM[1] =  np.where(WLK_TRN_KNR_DTIM[1] < 0, 0, WLK_TRN_KNR_DTIM[1])\n",
    "WLK_TRN_KNR_DTIM[2] =  np.where(WLK_TRN_KNR_DTIM[2] < 0, 0, WLK_TRN_KNR_DTIM[2])\n",
    "WLK_TRN_KNR_DTIM[3] =  np.where(WLK_TRN_KNR_DTIM[3] < 0, 0, WLK_TRN_KNR_DTIM[3])\n",
    "WLK_TRN_KNR_DTIM[4] =  np.where(WLK_TRN_KNR_DTIM[4] < 0, 0, WLK_TRN_KNR_DTIM[4])\n",
    "WLK_TRN_KNR_DTIM[5] =  np.where(WLK_TRN_KNR_DTIM[5] < 0, 0, WLK_TRN_KNR_DTIM[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aa6ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly check few matrix cores\n",
    "#PNR_TRN_WLK_DDIST[4].sum()\n",
    "#PNR_TRN_WLK_DDIST[2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the PM peak crowding and iwait variables, transpose them, and use them for the AM.\n",
    "PNR_TRN_WLK_CROWD[2] = PNR_TRN_WLK_CROWD[4].T\n",
    "PNR_TRN_WLK_IWAIT[2] = PNR_TRN_WLK_IWAIT[4].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8b7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct ramp has params.properties which has certain parameter values used in the utility equations. \n",
    "# Following function extracts these values.\n",
    "\n",
    "\n",
    "def extract_property_values(file_path, variables):\n",
    "    property_values = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#'):\n",
    "                key, value = line.split('=', 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                if key in variables:\n",
    "                    property_values[key] = value\n",
    "    return property_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274f16e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee800fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for purp in purpose:\n",
    "    #print(purp)\n",
    "    # read the purpose tab from the UEC file. \n",
    "    uec_purp_columns = ['No', 'Token', 'Description', 'Filter','Formula for variable', \n",
    "               'Index','Alt1', 'Alt2', 'Alt3', 'Alt4', 'Alt5', 'Alt6', 'Alt7', 'Alt8', 'Alt9']\n",
    "    \n",
    "    uec_purp = pd.read_excel(_join(params['common_dir'], \"TripModeChoice.xlsx\"), sheet_name=purp)\n",
    "    uec_purp = uec_purp.iloc[2:]\n",
    "    uec_purp.columns = uec_purp_columns # assign column names\n",
    "    \n",
    "    # Removing NAs\n",
    "    uec_purp_params_prop = uec_purp.loc[~uec_purp['Token'].isna()]\n",
    "    # extract the parameters that have % in in their names, clean up-remove % and replace . with _\n",
    "    uec_purp_params_prop = uec_purp_params_prop.loc[(uec_purp_params_prop['Formula for variable'].str.contains('%')==True)]\n",
    "    uec_purp_params_prop['Formula for variable'] = uec_purp_params_prop['Formula for variable'].str.replace('%', '') \n",
    "    uec_purp_params_prop['Formula for variable'] = uec_purp_params_prop['Formula for variable'].str.replace(\".\", \"_\")\n",
    "    # read parameters file\n",
    "    file_path = _join(ctramp_dir, 'input', 'params.properties')\n",
    "    # extract list of parameters\n",
    "    prop_variables = list(uec_purp_params_prop['Formula for variable'])\n",
    "    prop_variables_tokens = list(uec_purp_params_prop['Token'])\n",
    "    prop_variables = [x.replace('_', '.') for x in prop_variables]\n",
    "\n",
    "    values = extract_property_values(file_path, prop_variables)\n",
    "    \n",
    "    # Create a dictionary to store the extracted values\n",
    "    extracted_values = {}\n",
    "\n",
    "    # Assign the extracted values to the dictionary\n",
    "    for variable, value in values.items():\n",
    "        extracted_values[variable] = value\n",
    "\n",
    "    # Print the values from the extracted_values dictionary \n",
    "    for variable, value in extracted_values.items():\n",
    "        #print(f'{variable}: {value}')\n",
    "        exec(f'{variable.replace(\".\", \"_\")} = {value}')\n",
    "    \n",
    "    \n",
    "    # Assign the values to tokens\n",
    "    # example costInitialTaxi = %taxi.baseFare%\n",
    "    for _, row in uec_purp_params_prop.iterrows():\n",
    "        variable_name = row['Token']\n",
    "        expression = row['Formula for variable']\n",
    "\n",
    "        # Evaluate the expression and store the result in the local environment\n",
    "        try:\n",
    "            # Evaluate the expression and store the result in the local environment\n",
    "            if expression in locals() and isinstance(locals()[expression], np.ndarray):\n",
    "                value = locals()[expression]\n",
    "            else:\n",
    "                value = eval(expression)\n",
    "\n",
    "            exec(f'{variable_name} = value')\n",
    "            #print(f\"Variable '{variable_name}' is defined.\")\n",
    "        except NameError:\n",
    "            #print(f\"Variable '{variable_name}' is not defined.\")\n",
    "            continue\n",
    "\n",
    "    \n",
    "    uec_purp_params = uec_purp.loc[~uec_purp['Formula for variable'].isna()]\n",
    "    uec_purp_params = uec_purp_params.loc[~uec_purp_params['Token'].isna()]\n",
    "    uec_purp_params = uec_purp_params.loc[~(uec_purp_params['Formula for variable'].str.contains('if')==True)]\n",
    "    uec_purp_params = uec_purp_params.loc[~(uec_purp_params['Formula for variable'].str.contains('%')==True)]\n",
    "\n",
    "    uec_purp_params['Formula for variable'] = uec_purp_params['Formula for variable'].astype(str)\n",
    "    uec_purp_params['Formula for variable'] = uec_purp_params['Formula for variable'].str.replace('@', '')\n",
    "\n",
    "    key_column = 'Token'\n",
    "    value_column = 'Formula for variable'\n",
    "\n",
    "    # Create dictionary from selected columns\n",
    "    data_dict = {}\n",
    "\n",
    "    for _, row in uec_purp_params.iterrows():\n",
    "        key = row[key_column]\n",
    "        value = row[value_column]\n",
    "\n",
    "        # Handle values that are strings\n",
    "        if isinstance(value, str):\n",
    "            try:\n",
    "                value = int(value)\n",
    "                data_dict[key] = value\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    value = float(value)\n",
    "                    data_dict[key] = value\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "    #get all the parameters\n",
    "    variables = data_dict\n",
    "\n",
    "    for _, row in uec_purp_params.iterrows():\n",
    "        variable_name = row['Token']\n",
    "        expression = row['Formula for variable']\n",
    "\n",
    "        # Evaluate the expression and store the result in the local environment\n",
    "        try:\n",
    "            # Evaluate the expression and store the result in the local environment\n",
    "            if expression in locals() and isinstance(locals()[expression], np.ndarray):\n",
    "                value = locals()[expression]\n",
    "            else:\n",
    "                value = eval(expression)\n",
    "\n",
    "            exec(f'{variable_name} = value')\n",
    "        except NameError:\n",
    "            #print(f\"Variable '{variable_name}' is not defined.\")\n",
    "            continue\n",
    "    \n",
    "    #break\n",
    "    \n",
    "    int_zone = 3332\n",
    "    da_util = np.empty((5, int_zone, int_zone))\n",
    "    sr2_util = np.empty((5, int_zone, int_zone))\n",
    "    sr3_util = np.empty((5, int_zone, int_zone))\n",
    "    wlk_util =  np.empty((5, int_zone, int_zone))\n",
    "    bike_util = np.empty((5, int_zone, int_zone))\n",
    "    wlk_trn_wlk_util = np.empty((5, int_zone, int_zone))\n",
    "    wlk_trn_pnr_util = np.empty((5, int_zone, int_zone))\n",
    "    pnr_trn_wlk_util = np.empty((5, int_zone, int_zone))\n",
    "    wlk_trn_knr_util = np.empty((5, int_zone, int_zone))\n",
    "    knr_trn_wlk_util = np.empty((5, int_zone, int_zone))\n",
    "    taxi_util = np.empty((5, int_zone, int_zone))\n",
    "    \n",
    "    for tripPeriod in time_period:\n",
    "        #for trip_mode in oth_modes:\n",
    "        #uec_purp_mode = uec_purp_df.loc[uec_purp_df['Description'].str.contains(trip_mode)==True]\n",
    "        #uec_purp_mode['Formula for variable'] = uec_purp_mode['Formula for variable'].str.replace('tripPeriod', str(period))\n",
    "        #uec_purp_mode['formula_calculation'] = \n",
    "        util = omx.open_file(_join(preprocess_dir, f'util_{tripPeriod}_{purp}.omx'),'w')\n",
    "\n",
    "        #Drive alone\n",
    "        util['DA'] = c_ivt*SOV_TIME[tripPeriod][:int_zone, :int_zone]\n",
    "        print(tripPeriod, purp, 'DA', \" \", np.array(util['DA']).min(), np.array(util['DA']).max())\n",
    "\n",
    "        #Shared ride 2\n",
    "        util['SR2'] = c_ivt*HOV2_TIME[tripPeriod][:int_zone, :int_zone]\n",
    "        print(tripPeriod, purp, 'SR2', \" \", np.array(util['SR2']).min(), np.array(util['SR2']).max())\n",
    "\n",
    "        #Shared ride 3\n",
    "        util['SR3'] = c_ivt*HOV3_TIME[tripPeriod][:int_zone, :int_zone]\n",
    "        print(tripPeriod, purp, 'SR3', \" \", np.array(util['SR3']).min(), np.array(util['SR3']).max())\n",
    "\n",
    "        # Walk \n",
    "        util['WALK'] = (walk_dist<=1)* (c_walkTimeShort * np.minimum(walk_dist * 60 / walkSpeed, walkThresh * 60 / walkSpeed)) + \\\n",
    "                       (walk_dist>1)* (c_walkTimeLong * np.maximum(walk_dist * 60 / walkSpeed, walkThresh * 60 / walkSpeed)) \n",
    "        print(tripPeriod, purp, 'WALK', \" \", np.array(util['WALK']).min(), np.array(util['WALK']).max())\n",
    "        \n",
    "        #Bike\n",
    "        util['BIKE'] = (bike_dist<=6)*(c_bikeTimeShort* np.minimum(bike_dist*60/bikeSpeed, bikeThresh*60/bikeSpeed)) + \\\n",
    "                       (bike_dist>6)*(c_bikeTimeLong* np.maximum(bike_dist*60/bikeSpeed, bikeThresh*60/bikeSpeed))\n",
    "        print(tripPeriod, purp, 'BIKE', \" \", np.array(util['BIKE']).min(), np.array(util['BIKE']).max())\n",
    "        \n",
    "        \n",
    "        #Walk transit Walk\n",
    "        util['WLK_TRN_WLK'] =  c_ivt*WLK_TRN_WLK_IVT_LOC[tripPeriod]/100 + \\\n",
    "                            c_ivt_exp*WLK_TRN_WLK_IVT_EXP[tripPeriod]/100 + \\\n",
    "                            c_ivt_lrt*WLK_TRN_WLK_IVT_LRT[tripPeriod]/100 + \\\n",
    "                            c_ivt_ferry*WLK_TRN_WLK_IVT_FRY[tripPeriod]/100 + \\\n",
    "                            c_ivt_hvy*WLK_TRN_WLK_IVT_HVY[tripPeriod]/100 + \\\n",
    "                            c_ivt_com*WLK_TRN_WLK_IVT_COM[tripPeriod]/100 + \\\n",
    "                            c_ivt_trn_crwd*WLK_TRN_WLK_CROWD[tripPeriod]/100 + \\\n",
    "                            c_shortiWait*np.minimum(WLK_TRN_WLK_IWAIT[tripPeriod]/100,waitThresh) + \\\n",
    "                            c_longiWait*np.maximum(WLK_TRN_WLK_IWAIT[tripPeriod]/100-waitThresh,0) + \\\n",
    "                            c_xwait*WLK_TRN_WLK_XWAIT[tripPeriod]/100 + \\\n",
    "                            c_xfers_wlk * np.maximum(WLK_TRN_WLK_BOARDS[tripPeriod]-1,0) + \\\n",
    "                            c_waux*WLK_TRN_WLK_WAUX[tripPeriod]/100\n",
    "                            #c_wacc*WLK_TRN_WLK_WACC[tripPeriod]/100 + \\\n",
    "                            #c_wegr*WLK_TRN_WLK_WEGR[tripPeriod]/100\n",
    "        \n",
    "        print(tripPeriod, purp, 'WLK_TRN_WLK', \" \", np.array(util['WLK_TRN_WLK']).min(), np.array(util['WLK_TRN_WLK']).max())\n",
    "        \n",
    "        # Walk Transit PNR - Inbound\n",
    "        util['WLK_TRN_PNR'] =  c_ivt*WLK_TRN_PNR_TOTIVT[tripPeriod]/100 + \\\n",
    "                            (c_ivt_com-c_ivt)*WLK_TRN_PNR_IVT_COM[tripPeriod]/100 + \\\n",
    "                            c_ivt_trn_crwd*WLK_TRN_PNR_CROWD[tripPeriod]/100 + \\\n",
    "                            c_shortiWait*np.minimum(WLK_TRN_PNR_IWAIT[tripPeriod]/100,waitThresh) + \\\n",
    "                            c_longiWait*np.maximum(WLK_TRN_PNR_IWAIT[tripPeriod]/100-waitThresh,0) + \\\n",
    "                            c_xwait*WLK_TRN_PNR_XWAIT[tripPeriod]/100 + \\\n",
    "                            c_dtim*WLK_TRN_PNR_DTIM[tripPeriod]/100 + \\\n",
    "                            c_xfers_drv*np.maximum(WLK_TRN_PNR_BOARDS[tripPeriod]-1,0) + \\\n",
    "                            #c_wacc*WLK_TRN_PNR_WACC[tripPeriod]/100 + \\\n",
    "                            c_waux*WLK_TRN_PNR_WAUX[tripPeriod]/100 + \\\n",
    "                            c_dacc_ratio*(WLK_TRN_PNR_DDIST[tripPeriod]/100/SOV_DIST[tripPeriod][:int_zone, :int_zone])\n",
    "        print(tripPeriod, purp, 'WLK_TRN_PNR', \" \", np.array(util['WLK_TRN_PNR']).min(), np.array(util['WLK_TRN_PNR']).max())\n",
    "\n",
    "        # PNR transit Walk - Outbound\n",
    "        util['PNR_TRN_WLK'] =  c_ivt*PNR_TRN_WLK_TOTIVT[tripPeriod]/100 + \\\n",
    "                            (c_ivt_com-c_ivt)*PNR_TRN_WLK_IVT_COM[tripPeriod]/100 + \\\n",
    "                            c_ivt_trn_crwd*PNR_TRN_WLK_CROWD[tripPeriod]/100 + \\\n",
    "                            c_shortiWait*np.minimum(PNR_TRN_WLK_IWAIT[tripPeriod]/100,waitThresh) + \\\n",
    "                            c_longiWait*np.maximum(PNR_TRN_WLK_IWAIT[tripPeriod]/100-waitThresh,0) + \\\n",
    "                            c_xwait*PNR_TRN_WLK_XWAIT[tripPeriod]/100 + \\\n",
    "                            c_dtim*PNR_TRN_WLK_DTIM[tripPeriod]/100 + \\\n",
    "                            c_xfers_drv*np.maximum(PNR_TRN_WLK_BOARDS[tripPeriod]-1,0) + \\\n",
    "                            #c_wegr*PNR_TRN_WLK_WEGR[tripPeriod]/100 + \\\n",
    "                            c_waux*PNR_TRN_WLK_WAUX[tripPeriod]/100 + \\\n",
    "                            c_dacc_ratio*(PNR_TRN_WLK_DDIST[tripPeriod]/100/SOV_DIST[tripPeriod][:int_zone, :int_zone])\n",
    "        print(tripPeriod, purp, 'PNR_TRN_WLK', \" \", np.array(util['PNR_TRN_WLK']).min(), np.array(util['PNR_TRN_WLK']).max())\n",
    "\n",
    "        # Walk Transit KNR - Inbound\n",
    "        util['WLK_TRN_KNR'] = c_ivt*WLK_TRN_KNR_TOTIVT[tripPeriod]/100 + \\\n",
    "                            (c_ivt_com-c_ivt)*WLK_TRN_KNR_IVT_COM[tripPeriod]/100 + \\\n",
    "                            c_ivt_trn_crwd*WLK_TRN_KNR_CROWD[tripPeriod]/100 + \\\n",
    "                            c_shortiWait*np.minimum(WLK_TRN_KNR_IWAIT[tripPeriod]/100,waitThresh) + \\\n",
    "                            c_longiWait*np.maximum(WLK_TRN_KNR_IWAIT[tripPeriod]/100-waitThresh,0) + \\\n",
    "                            c_xwait*WLK_TRN_KNR_XWAIT[tripPeriod]/100 + \\\n",
    "                            c_xfers_drv*np.maximum(WLK_TRN_KNR_BOARDS[tripPeriod]-1,0) + \\\n",
    "                            c_dtim*WLK_TRN_KNR_DTIM[tripPeriod]/100 + \\\n",
    "                            #c_wacc*WLK_TRN_KNR_WACC[tripPeriod]/100 + \\\n",
    "                            c_waux*WLK_TRN_KNR_WAUX[tripPeriod]/100 + \\\n",
    "                            c_dacc_ratio*(WLK_TRN_KNR_DDIST[tripPeriod]/100/SOV_DIST[tripPeriod][:int_zone, :int_zone])\n",
    "        print(tripPeriod, purp, 'WLK_TRN_KNR', \" \", np.array(util['WLK_TRN_KNR']).min(), np.array(util['WLK_TRN_KNR']).max())\n",
    "\n",
    "        # KNR Transit Walk - Outbound\n",
    "        util['KNR_TRN_WLK'] = c_ivt*KNR_TRN_WLK_TOTIVT[tripPeriod]/100 + \\\n",
    "                            (c_ivt_com-c_ivt)*KNR_TRN_WLK_IVT_COM[tripPeriod]/100 + \\\n",
    "                            c_ivt_trn_crwd*KNR_TRN_WLK_CROWD[tripPeriod]/100 + \\\n",
    "                            c_shortiWait*np.minimum(KNR_TRN_WLK_IWAIT[tripPeriod]/100,waitThresh) + \\\n",
    "                            c_longiWait*np.maximum(KNR_TRN_WLK_IWAIT[tripPeriod]/100-waitThresh,0) + \\\n",
    "                            c_xwait*KNR_TRN_WLK_XWAIT[tripPeriod]/100 + \\\n",
    "                            c_xfers_drv*np.maximum(KNR_TRN_WLK_BOARDS[tripPeriod]-1,0) + \\\n",
    "                            c_dtim*KNR_TRN_WLK_DTIM[tripPeriod]/100 + \\\n",
    "                            #c_wegr*WLK_TRN_KNR_WEGR[tripPeriod]/100 + \\\n",
    "                            c_waux*KNR_TRN_WLK_WAUX[tripPeriod]/100 + \\\n",
    "                            c_dacc_ratio*(KNR_TRN_WLK_DDIST[tripPeriod]/100/SOV_DIST[tripPeriod][:int_zone, :int_zone])\n",
    "        print(tripPeriod, purp, 'KNR_TRN_WLK', \" \", np.array(util['KNR_TRN_WLK']).min(), np.array(util['KNR_TRN_WLK']).max())\n",
    "\n",
    "        # taxi\n",
    "        util['RIDEHAIL'] = c_ivt*HOV2_TIME[tripPeriod][:int_zone, :int_zone]  + c_ivt*1.5*taxi_wait_time\n",
    "        print(tripPeriod, purp, 'RIDEHAIL', \" \", np.array(util['RIDEHAIL']).min(), np.array(util['RIDEHAIL']).max())\n",
    "        \n",
    "    \n",
    "        util.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac47c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398559e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add mappings from time period and purpose\n",
    "df_trips = pd.read_parquet(_join(preprocess_dir, 'trip_roster.parquet'))\n",
    "len(df_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inbound trips get orig purpose, outbound trips get dest purpose\n",
    "df_trips['util_purpose'] = np.where(df_trips['inbound']==1, df_trips['orig_purpose'], df_trips['dest_purpose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57296ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_trips['util_purpose'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb2b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "purp_dict = { 'work' : 'Work', \n",
    "              'shopping' : 'Shopping',\n",
    "              'escort' : 'Escort', \n",
    "              'othdiscr': 'OthDiscr',\n",
    "              'othmaint': 'OthMaint',\n",
    "              'school' : 'School', \n",
    "              'eatout' : 'EatOut', \n",
    "              'atwork' : 'WorkBased', \n",
    "              'social' : 'Social',\n",
    "              'university' : 'University'}\n",
    "\n",
    "time_period = {1:'EA',2:'AM',3:'MD',4:'PM',5:'EV'} #1 for EA, 2 for AM, 3 for MD, 4 for PM and 5 for EV\n",
    "\n",
    "purpose = ['Work', 'University', 'School', 'Escort', 'Shopping', 'EatOut', \n",
    "           'OthMaint', 'Social', 'OthDiscr', 'WorkBased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11506ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips['util_purpose'] = df_trips['util_purpose'].map(purp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafe0b00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "num_zones = 3332\n",
    "\n",
    "for tripPeriod, value in time_period.items():\n",
    "\n",
    "    for purp in purpose:\n",
    "        print(f'Analyzing purpose: {purp} and time period: {value}')\n",
    "        #df_temp = df_trips.query(f\"util_purpose == {purp} and Period == {value.lower()}\")\n",
    "        #df_temp = df_trips.query(f\"util_purpose == '{purp}' and Period == '{value.lower}'\")\n",
    "        df_temp = df_trips.loc[(df_trips['util_purpose'] == purp) & (df_trips['Period'] == value.lower())]\n",
    "        \n",
    "        # Generate all combinations of orig and dest\n",
    "        combinations = list(itertools.product(range(1, num_zones + 1), repeat=2))\n",
    "\n",
    "        # Create the DataFrame with orig and dest columns\n",
    "        purp_df = pd.DataFrame(combinations, columns=['orig', 'dest'])\n",
    "\n",
    "        # read utility files\n",
    "        util_file = omx.open_file(_join(preprocess_dir, f'util_{tripPeriod}_{purp}.omx'))\n",
    "\n",
    "        for core in util_file.list_matrices():\n",
    "            print(f'extracting {core} core form utility file')\n",
    "            mode_core = np.array(util_file[core])\n",
    "            mode_core = np.where(mode_core == 0, -999, mode_core)\n",
    "            skm_df = pd.DataFrame(mode_core)\n",
    "            skm_df = pd.melt(skm_df.reset_index(), id_vars='index', value_vars=skm_df.columns)\n",
    "            skm_df['index'] = skm_df['index'] + 1\n",
    "            skm_df['variable'] = skm_df['variable'] + 1\n",
    "            skm_df.columns = ['orig', 'dest', core]\n",
    "            purp_df = pd.merge(purp_df, skm_df, on=['orig', 'dest'], how='left')\n",
    "        \n",
    "        df_temp = pd.merge(df_temp, purp_df, left_on=['orig_taz', 'dest_taz'], right_on=['orig', 'dest'], how='left')\n",
    "        \n",
    "        print(f'writing the trip file for purpose : {purp} and time period: {value}', df_temp.shape)\n",
    "        df_temp.to_parquet(_join(preprocess_dir, f'trip_{tripPeriod}_{purp}.parquet'))\n",
    "        \n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df1b187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all trips into one file\n",
    "\n",
    "# read trip files\n",
    "final_trips = []\n",
    "\n",
    "for tripPeriod, value in time_period.items():\n",
    "    for purp in purpose:\n",
    "        temp = pd.read_parquet(_join(preprocess_dir, f'trip_{tripPeriod}_{purp}.parquet'))\n",
    "        final_trips.append(temp)\n",
    "        \n",
    "final_trips = pd.concat(final_trips)\n",
    "len(final_trips) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48019235",
   "metadata": {},
   "source": [
    "### Calculate the logsums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e307aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_nesting_coef = 0.72\n",
    "trn_nesting_coef = 0.72\n",
    "nm_nest_coef = 0.72\n",
    "ridehail_nest_coef = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f9c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logsum\n",
    "\n",
    "final_trips['auto_ls'] = auto_nesting_coef * (np.log(np.exp(final_trips['DA']/auto_nesting_coef) + \n",
    "                                                     np.exp(final_trips['SR2']/auto_nesting_coef) + \n",
    "                                                     np.exp(final_trips['SR3']/auto_nesting_coef)))\n",
    "\n",
    "\n",
    "final_trips['exp_trn'] = np.exp(final_trips['WLK_TRN_WLK']/trn_nesting_coef) + \\\n",
    "                               np.exp(final_trips['WLK_TRN_PNR']/trn_nesting_coef) + \\\n",
    "                               np.exp(final_trips['PNR_TRN_WLK']/trn_nesting_coef) + \\\n",
    "                               np.exp(final_trips['WLK_TRN_KNR']/trn_nesting_coef) + \\\n",
    "                               np.exp(final_trips['KNR_TRN_WLK']/trn_nesting_coef)\n",
    "\n",
    "final_trips['trn_ls'] = np.where(final_trips['exp_trn'] > 0, trn_nesting_coef *(np.log(final_trips['exp_trn'])), 0)\n",
    "\n",
    "\n",
    "final_trips['exp_nm'] = np.exp(final_trips['WALK']/nm_nest_coef) + \\\n",
    "                                  np.exp(final_trips['BIKE']/nm_nest_coef)\n",
    "    \n",
    "final_trips['non_mot_ls'] = np.where(final_trips['exp_nm'] > 0, nm_nest_coef * (np.log(final_trips['exp_nm'])), 0)\n",
    "\n",
    "final_trips['exp_ridehail'] = np.exp(final_trips['RIDEHAIL']/ridehail_nest_coef)\n",
    "final_trips['ridehail_ls'] = np.where(final_trips['exp_ridehail']>0, ridehail_nest_coef * (np.log(final_trips['exp_ridehail'])),0)\n",
    "\n",
    "final_trips['allmode_ls'] = np.log(np.exp(final_trips['auto_ls']) + \n",
    "                                np.exp(final_trips['trn_ls']) + \n",
    "                                np.exp(final_trips['non_mot_ls']) + \n",
    "                                np.exp(final_trips['ridehail_ls']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ff0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_trips['allmode_ls_adj'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae62cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logsum\n",
    "final_trips['allmode_ls_adj'] = final_trips['allmode_ls'] - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325627a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_trips['sum_ls'] = np.exp(final_trips['auto_ls']) + np.exp(final_trips['trn_ls']) +  np.exp(final_trips['non_mot_ls']) + np.exp(final_trips['ridehail_ls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aee9e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_trips[final_trips['allmode_ls']<0][1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9198edc1",
   "metadata": {},
   "source": [
    "### Get BETA IVT values for each purpose from UEC sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa90b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get beta IVT for each purpose\n",
    "ivt_purp = pd.DataFrame(columns=['util_purpose'])\n",
    "\n",
    "for purp in purpose:\n",
    "    print(purp)\n",
    "    # read the purpose tab from the UEC file. \n",
    "    uec_purp_columns = ['No', 'Token', 'Description', 'Filter','Formula for variable', \n",
    "               'Index','Alt1', 'Alt2', 'Alt3', 'Alt4', 'Alt5', 'Alt6', 'Alt7', 'Alt8', 'Alt9']\n",
    "    \n",
    "    uec_purp = pd.read_excel(_join(params['common_dir'], \"TripModeChoice.xlsx\"), sheet_name=purp)\n",
    "    uec_purp = uec_purp.iloc[2:]\n",
    "    uec_purp.columns = uec_purp_columns # assign column names\n",
    "    \n",
    "    ivt = uec_purp.loc[uec_purp['Token']=='c_ivt', 'Formula for variable'].item()\n",
    "    #ivt_lrt = uec_purp.loc[uec_purp['Token']=='c_ivt_lrt', 'Formula for variable'].item()\n",
    "    #ivt_ferry = uec_purp.loc[uec_purp['Token']=='c_ivt_ferry', 'Formula for variable'].item()\n",
    "    #ivt_exp = uec_purp.loc[uec_purp['Token']=='c_ivt_exp', 'Formula for variable'].item()\n",
    "    #ivt_hvy = uec_purp.loc[uec_purp['Token']=='c_ivt_hvy', 'Formula for variable'].item()\n",
    "    #ivt_com = uec_purp.loc[uec_purp['Token']=='c_ivt_com', 'Formula for variable'].item()\n",
    "    \n",
    "    ivt_purp = ivt_purp.append({'util_purpose': purp, 'b_ivt': ivt #'b_ivt_lrt': ivt_lrt,\n",
    "                                #'b_ivt_ferry' : ivt_ferry, 'b_ivt_exp': ivt_exp, \n",
    "                                #'b_ivt_hvy': ivt_hvy, 'b_ivt_com': ivt_com\n",
    "                               }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482367fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with trip roster\n",
    "final_trips = pd.merge(final_trips, ivt_purp, on = 'util_purpose', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db18c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logsum benefits auto\n",
    "final_trips['ls_benefit_auto'] =  (final_trips['allmode_ls_adj'] * \\\n",
    "                                    (np.exp(final_trips['auto_ls'])/final_trips['sum_ls']) * \\\n",
    "                                        (final_trips['trips']/final_trips['b_ivt'])) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb787ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logsum benefits transit\n",
    "final_trips['ls_benefit_transit'] = (final_trips['allmode_ls_adj'] * \\\n",
    "                                         (np.exp(final_trips['trn_ls'])/final_trips['sum_ls']) * \\\n",
    "                                            (final_trips['trips']/final_trips['b_ivt']))  #* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fca46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logsum benefits ridehail\n",
    "final_trips['ls_benefit_raidehail'] = (final_trips['allmode_ls_adj'] * \\\n",
    "                                         (np.exp(final_trips['ridehail_ls'])/final_trips['sum_ls']) * \\\n",
    "                                            (final_trips['trips']/final_trips['b_ivt']))  #* final_trips['trips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e77e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logsum benefits non-motorized\n",
    "final_trips['ls_benefit_nm'] =  (final_trips['allmode_ls_adj'] * \\\n",
    "                                         (np.exp(final_trips['non_mot_ls'])/final_trips['sum_ls']) * \\\n",
    "                                            (final_trips['trips']/final_trips['b_ivt']))  #* final_trips['trips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54d48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_col = 'ls_benefit_transit'\n",
    "mode_numbers = [6,7,8]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "trn = df_region_period[['Period', 'Value']]\n",
    "trn.columns = ['Period', 'trn']\n",
    "#df_region_period.to_csv('trn_ls.csv')\n",
    "\n",
    "summary_col = 'ls_benefit_auto'\n",
    "mode_numbers = [1,2,3]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "auto = df_region_period[['Period', 'Value']]\n",
    "auto.columns = ['Period', 'auto']\n",
    "#df_region_period.to_csv('auto_ls.csv')\n",
    "\n",
    "summary_col = 'ls_benefit_nm'\n",
    "mode_numbers = [4,5]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "nm = df_region_period[['Period', 'Value']]\n",
    "nm.columns = ['Period', 'nm']\n",
    "#df_region_period.to_csv('nm_ls.csv')\n",
    "\n",
    "summary_col = 'ls_benefit_raidehail'\n",
    "mode_numbers = [9]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "rh = df_region_period[['Period', 'Value']]\n",
    "rh.columns = ['Period', 'rh']\n",
    "#df_region_period.to_csv('rh_ls.csv')\n",
    "\n",
    "final = pd.merge(trn, auto, on = 'Period').merge(\n",
    "                    nm, on='Period').merge(\n",
    "                    rh, on='Period')\n",
    "\n",
    "final.to_csv(\"ls_benefits\"+concept_id+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d27d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_col = 'trips'\n",
    "mode_numbers = [6,7,8]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "trn = df_region_period[['Period', 'Value']]\n",
    "trn.columns = ['Period', 'trn']\n",
    "#df_region_period.to_csv('trn_ls.csv')\n",
    "\n",
    "summary_col = 'trips'\n",
    "mode_numbers = [1,2,3]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "auto = df_region_period[['Period', 'Value']]\n",
    "auto.columns = ['Period', 'auto']\n",
    "#df_region_period.to_csv('auto_ls.csv')\n",
    "\n",
    "summary_col = 'trips'\n",
    "mode_numbers = [4,5]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "nm = df_region_period[['Period', 'Value']]\n",
    "nm.columns = ['Period', 'nm']\n",
    "#df_region_period.to_csv('nm_ls.csv')\n",
    "\n",
    "summary_col = 'trips'\n",
    "mode_numbers = [9]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "rh = df_region_period[['Period', 'Value']]\n",
    "rh.columns = ['Period', 'rh']\n",
    "#df_region_period.to_csv('rh_ls.csv')\n",
    "\n",
    "final = pd.merge(trn, auto, on = 'Period').merge(\n",
    "                    nm, on='Period').merge(\n",
    "                    rh, on='Period')\n",
    "\n",
    "final.to_csv(\"ls_benefits\"+concept_id+\"_trips.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59945a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Add distance\n",
    "md_dist = omx.open_file(_join(ctramp_dir, 'skims\\HWYSKMmd.omx'))\n",
    "md_dist = skim_core_to_df(md_dist, 'DISTDAM', cols =['orig', 'dest', 'dist'])\n",
    "\n",
    "final_trips = pd.merge(final_trips, md_dist, left_on=['orig_taz', 'dest_taz'], right_on=['orig', 'dest'], how='left')\n",
    "\n",
    "def calculate_weighted_average_by_category(df):\n",
    "    weighted_avgs = df.groupby('Period').apply(\n",
    "        lambda group: (group['trips'] * group['dist']).sum() / group['trips'].sum()\n",
    "    )\n",
    "    \n",
    "    weighted_avgs = weighted_avgs.reset_index()\n",
    "    weighted_avgs.columns = ['Period', 'dist']\n",
    "    weighted_avgs_all = pd.DataFrame(columns=['Period', 'dist'])\n",
    "    weighted_avgs_all.loc[0] = ['All', (df['dist'] * df['trips']).sum() / df['trips'].sum()]\n",
    " \n",
    "    weighted_avgs_all = pd.concat([weighted_avgs_all, weighted_avgs], ignore_index=True)\n",
    "    \n",
    "    return weighted_avgs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf50f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get average trip length\n",
    "summary_col = 'trips'\n",
    "mode_numbers = [6,7,8]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = calculate_weighted_average_by_category(temp)\n",
    "trn = df_region_period[['Period', 'dist']]\n",
    "trn.columns = ['Period', 'trn']\n",
    "#df_region_period.to_csv('trn_ls.csv')\n",
    "\n",
    "summary_col = 'trips'\n",
    "mode_numbers = [1,2,3]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = calculate_weighted_average_by_category(temp)\n",
    "auto = df_region_period[['Period', 'dist']]\n",
    "auto.columns = ['Period', 'auto']\n",
    "#df_region_period.to_csv('auto_ls.csv')\n",
    "\n",
    "summary_col = 'trips'\n",
    "mode_numbers = [4,5]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = calculate_weighted_average_by_category(temp)\n",
    "nm = df_region_period[['Period', 'dist']]\n",
    "nm.columns = ['Period', 'nm']\n",
    "#df_region_period.to_csv('nm_ls.csv')\n",
    "\n",
    "summary_col = 'trips'\n",
    "mode_numbers = [9]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = calculate_weighted_average_by_category(temp)\n",
    "rh = df_region_period[['Period', 'dist']]\n",
    "rh.columns = ['Period', 'rh']\n",
    "#df_region_period.to_csv('rh_ls.csv')\n",
    "\n",
    "final = pd.merge(trn, auto, on = 'Period').merge(\n",
    "                    nm, on='Period').merge(\n",
    "                    rh, on='Period')\n",
    "\n",
    "final.to_csv(\"ls_benefits\"+concept_id+\"_trip_length.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fecd947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get summary of composite utility\n",
    "summary_col = 'allmode_ls_adj'\n",
    "mode_numbers = [6,7,8]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "trn = df_region_period[['Period', 'Value']]\n",
    "trn.columns = ['Period', 'trn']\n",
    "#df_region_period.to_csv('trn_ls.csv')\n",
    "\n",
    "summary_col = 'allmode_ls_adj'\n",
    "mode_numbers = [1,2,3]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "auto = df_region_period[['Period', 'Value']]\n",
    "auto.columns = ['Period', 'auto']\n",
    "#df_region_period.to_csv('auto_ls.csv')\n",
    "\n",
    "summary_col = 'allmode_ls_adj'\n",
    "mode_numbers = [4,5]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "nm = df_region_period[['Period', 'Value']]\n",
    "nm.columns = ['Period', 'nm']\n",
    "#df_region_period.to_csv('nm_ls.csv')\n",
    "\n",
    "summary_col = 'allmode_ls_adj'\n",
    "mode_numbers = [9]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "rh = df_region_period[['Period', 'Value']]\n",
    "rh.columns = ['Period', 'rh']\n",
    "#df_region_period.to_csv('rh_ls.csv')\n",
    "\n",
    "final = pd.merge(trn, auto, on = 'Period').merge(\n",
    "                    nm, on='Period').merge(\n",
    "                    rh, on='Period')\n",
    "\n",
    "final.to_csv(\"ls_benefits_allmode\"+concept_id+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0c4f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c852c197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ded37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a empty dataframe of all OD combinations and time period\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "numbers_1_to_3332 = range(1, 3333)\n",
    "combinations_1_to_5 = range(1, 6)\n",
    "combinations_strings = ['am', 'md', 'pm', 'ev', 'ea']\n",
    "\n",
    "combinations = list(product(numbers_1_to_3332, numbers_1_to_3332, combinations_strings))\n",
    "\n",
    "df = pd.DataFrame(combinations, columns=['orig_taz', 'dest_taz', 'Period'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c3aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter trips by mode\n",
    "auto = final_trips.loc[final_trips['trip_mode'].isin([1,2,3])]\n",
    "trn = final_trips.loc[final_trips['trip_mode'].isin([6,7,8])]\n",
    "nm = final_trips.loc[final_trips['trip_mode'].isin([4,5])]\n",
    "rh = final_trips.loc[final_trips['trip_mode'].isin([9])]                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c83bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#get totals trips for each mode\n",
    "auto_trip = auto.groupby(['orig_taz', 'dest_taz', 'Period'])['trips'].sum().reset_index()\n",
    "auto_trip = auto_trip.rename(columns={'trips': 'auto_trips'})\n",
    "\n",
    "trn_trip = trn.groupby(['orig_taz', 'dest_taz', 'Period'])['trips'].sum().reset_index()\n",
    "trn_trip = trn_trip.rename(columns={'trips': 'trn_trips'})\n",
    "\n",
    "nm_trip = nm.groupby(['orig_taz', 'dest_taz', 'Period'])['trips'].sum().reset_index()\n",
    "nm_trip = nm_trip.rename(columns={'trips': 'nm_trips'})\n",
    "\n",
    "rh_trip = rh.groupby(['orig_taz', 'dest_taz', 'Period'])['trips'].sum().reset_index()\n",
    "rh_trip = rh_trip.rename(columns={'trips': 'rh_trips'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b27ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#merge the trips by each model from previous cell\n",
    "all_modes_trips=pd.merge(df, auto_trip,  on=['orig_taz', 'dest_taz', 'Period'], how='left').merge(\n",
    "    trn_trip, on=['orig_taz', 'dest_taz', 'Period'], how='left').merge(\n",
    "    nm_trip, on=['orig_taz', 'dest_taz', 'Period'], how='left').merge(\n",
    "    rh_trip, on=['orig_taz', 'dest_taz', 'Period'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f446e201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dc67b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# mean of logsum benefits\n",
    "auto_ls = auto.groupby(['orig_taz', 'dest_taz', 'Period'])['ls_benefit_auto'].mean().reset_index()\n",
    "auto_ls = auto_ls.rename(columns={'ls_benefit_auto': 'auto_ls'})\n",
    "\n",
    "trn_ls = trn.groupby(['orig_taz', 'dest_taz', 'Period'])['ls_benefit_transit'].mean().reset_index()\n",
    "trn_ls = trn_ls.rename(columns={'ls_benefit_transit': 'trn_ls'})\n",
    "\n",
    "nm_ls = nm.groupby(['orig_taz', 'dest_taz', 'Period'])['ls_benefit_nm'].mean().reset_index()\n",
    "nm_ls = nm_ls.rename(columns={'ls_benefit_nm': 'nm_ls'})\n",
    "\n",
    "rh_ls = rh.groupby(['orig_taz', 'dest_taz', 'Period'])['ls_benefit_raidehail'].mean().reset_index()\n",
    "rh_ls = rh_ls.rename(columns={'ls_benefit_raidehail': 'rh_ls'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f069dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_modes_ls = pd.merge(df, auto_ls,  on=['orig_taz', 'dest_taz', 'Period'], how='left').merge(\n",
    "    trn_ls, on=['orig_taz', 'dest_taz', 'Period'], how='left').merge(\n",
    "    nm_ls, on=['orig_taz', 'dest_taz', 'Period'], how='left').merge(\n",
    "    rh_ls, on=['orig_taz', 'dest_taz', 'Period'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7743c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in all_modes_ls.columns:\n",
    "    print(cols, all_modes_ls[cols].min(), all_modes_ls[cols].max())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f10b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_modes = pd.merge(all_modes_trips, all_modes_ls, on=['orig_taz', 'dest_taz', 'Period'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc414907",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_modes = all_modes.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d115ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the results\n",
    "all_modes.to_parquet(_join(concept_id+'_perc_ls_trips.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3ff577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba2531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read baseline results\n",
    "baseline = pd.read_parquet(\"2050_Baseline_R2_Run4_perc_ls_trips.parquet\")\n",
    "baseline = baseline.add_suffix('_bl')\n",
    "baseline.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf63bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(baseline, all_modes, left_on=['orig_taz_bl', 'dest_taz_bl', 'Period_bl'],\n",
    "             right_on=['orig_taz', 'dest_taz', 'Period'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['auto', 'trn', 'nm', 'rh']\n",
    "# find differences\n",
    "for cols in columns:\n",
    "    df[cols+'_ls_diff'] = df[cols+'_ls'] - df[cols+'_ls_bl'] \n",
    "    df[cols+'_trp_diff'] = df[cols+'_trips'] - df[cols+'_trips_bl']\n",
    "    df[cols+'_trp_diff'] = df[cols+'_trp_diff'].apply(lambda x: max(0, x))\n",
    "    print(cols+'_ls', '_', df[cols+'_trp_diff'].min(), df[cols+'_ls'].min(), df[cols+'_ls'].max())\n",
    "    df[cols+'_benefits'] = (df[cols+'_trips_bl'] + df[cols+'_trp_diff']) * df[cols+'_ls_diff'] * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the benefits\n",
    "auto = summarize_all_combinations(df, groupby_columns=['Period'], summary_column='auto_benefits')\n",
    "auto = auto.rename(columns={'Value': 'auto'})\n",
    "\n",
    "trn = summarize_all_combinations(df, groupby_columns=['Period'], summary_column='trn_benefits')\n",
    "trn = trn.rename(columns={'Value': 'transit'})\n",
    "\n",
    "nm = summarize_all_combinations(df, groupby_columns=['Period'], summary_column='nm_benefits')\n",
    "nm = nm.rename(columns={'Value': 'nm'})\n",
    "\n",
    "rh = summarize_all_combinations(df, groupby_columns=['Period'], summary_column='rh_benefits')\n",
    "rh = rh.rename(columns={'Value': 'rh'})\n",
    "\n",
    "all_benefits = pd.merge(auto, trn, on='Period').merge(nm, on='Period').merge(rh, on='Period')\n",
    "all_benefits.to_csv(concept_id+\"_ls_benefits_final.csv\")\n",
    "all_benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e516a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae35ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
