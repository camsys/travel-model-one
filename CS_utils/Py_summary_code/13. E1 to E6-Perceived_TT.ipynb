{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e63ccda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openmatrix as omx\n",
    "import random\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from utility import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c7ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "    \n",
    "_join = os.path.join\n",
    "_dir = os.path.dirname\n",
    "_norm = os.path.normpath\n",
    "\n",
    "# paths\n",
    "ctramp_dir = params['ctramp_dir']\n",
    "model_outputs_dir = params['model_dir']\n",
    "summary_dir = params['summary_dir']\n",
    "concept_id = params['concept_id']\n",
    "preprocess_dir = _join(ctramp_dir, '_pre_process_files')\n",
    "perf_measure_columns = params['final_columns']\n",
    "model_year = params['model_year']\n",
    "filename_extension = params['filename_extension']\n",
    "hwy_skims_dir = _join(model_outputs_dir, r'skims\\highway' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b397a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(summary_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(preprocess_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff847daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose = ['Work', 'University', 'School', 'Escort', 'Shopping', 'EatOut', \n",
    "           'OthMaint', 'Social', 'OthDiscr', 'WorkBased']\n",
    "\n",
    "time_period = {1:'EA',2:'AM',3:'MD',4:'PM',5:'EV'} #1 for EA, 2 for AM, 3 for MD, 4 for PM and 5 for EV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e3f735",
   "metadata": {},
   "source": [
    "### Calculate the taxi wait time for each origin zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "taz = pd.read_csv(_join(ctramp_dir, 'landuse', 'tazData_' + str(model_year) + '.csv'))\n",
    "taz['popEmpSqMile'] = (taz['TOTPOP'] + taz['TOTEMP']) / (taz['TOTACRE'] * 0.0015625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a63bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "taz = taz[['ZONE', 'popEmpSqMile']]\n",
    "\n",
    "# TNC \n",
    "#TNC_single_waitTime_mean =  10.3,8.5,8.4,6.3,3.0\n",
    "#TNC_single_waitTime_sd =     4.1,4.1,4.1,4.1,2.0\n",
    "\n",
    "#TNC_shared_waitTime_mean =  15.0,15.0,11.0,8.0,5.0\n",
    "#TNC_shared_waitTime_sd =     4.1,4.1,4.1,4.1,2.0\n",
    "\n",
    "#Taxi_waitTime_mean = 26.5,17.3,13.3,9.5,5.5\n",
    "#Taxi_waitTime_sd =    6.4,6.4,6.4,6.4,6.4\n",
    "\n",
    "#WaitTimeDistribution_EndPopEmpPerSqMi = 500,2000,5000,15000,9999999999\n",
    "\n",
    "#TO DO: Ask John which wait time to use\n",
    "taz['density_group'] = pd.cut(taz['popEmpSqMile'], bins= [-1, 500,2000,5000,15000,9999999999], \n",
    "                              labels=[10.3,8.5,8.4,6.3,3.0], ordered=False)\n",
    "#taz['density_group'] = taz['density_group'].fillna(0)\n",
    "taz['density_group'] =taz['density_group'].astype(\"int64\")\n",
    "\n",
    "taz = taz.sort_values('ZONE')\n",
    "taxi_wait_time = np.repeat(taz['density_group'].values, len(taz)).reshape(len(taz), len(taz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b32fbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6fe0248",
   "metadata": {},
   "source": [
    "### Load all the data from Skims "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3f363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# The data tab of the UEC file lists all the matrix cores and location an matrix files of skims\n",
    "# 1 for EA, 2 for AM, 3 for MD, 4 for PM and 5 for EV\n",
    "\n",
    "# extract the file names, matrix cores \n",
    "matrix_df = pd.read_excel(_join(params['common_dir'], r\"TripModeChoice.xlsx\"), sheet_name='data')\n",
    "matrix_df = matrix_df.iloc[9:]\n",
    "matrix_df.columns = ['no', 'token', 'format', 'file','matrix', 'group', 'index']\n",
    "#matrix_df[1:5]\n",
    "\n",
    "# pre-processing\n",
    "matrix_df['matrix_files'] = matrix_df['file'].str.replace('skims/', '')\n",
    "matrix_df['path'] = 'skims'\n",
    "#matrix_df.loc[matrix_df['matrix_files'].str.contains('nonmot')==True, 'path'] = 'active'\n",
    "#matrix_df.loc[matrix_df['matrix_files'].str.contains('trnskm')==True, 'path'] = 'transit'\n",
    "#matrix_df.loc[matrix_df['matrix_files'].str.contains('hwyskm')==True, 'path'] = 'highway'\n",
    "#matrix_df[1:5]\n",
    "\n",
    "# Iterate over the DataFrame rows\n",
    "for _, row in matrix_df.iterrows():\n",
    "    variable_name = row['token']\n",
    "    file_path = row['path']\n",
    "    filename = row['matrix_files']\n",
    "    matrix_cr = row['matrix']\n",
    "    \n",
    "    # Extract the variable name and index (if present)\n",
    "    if '[' in variable_name:\n",
    "        name_start = variable_name.index('[')\n",
    "        name_end = variable_name.index(']')\n",
    "        index = int(variable_name[name_start+1:name_end])\n",
    "        variable_name = variable_name[:name_start]\n",
    "    else:\n",
    "        index=None\n",
    "    \n",
    "    # Read the file using numpy.load() and assign it to the variable with the specified index\n",
    "    file = omx.open_file(_join(ctramp_dir, file_path, filename))\n",
    "    file_contents = np.array(file[matrix_cr])\n",
    "    print(variable_name,index, _join(ctramp_dir, file_path, filename), file_contents.sum(), file_contents.min(), file_contents.max())\n",
    "    if '[' in row['token']:\n",
    "        if variable_name in locals() and isinstance(locals()[variable_name], np.ndarray):\n",
    "            arr = locals()[variable_name]\n",
    "            if index >= len(arr):\n",
    "                # Resize the array if the index is out of bounds\n",
    "                new_arr = np.resize(arr, index + 1)\n",
    "                new_arr[index] = file_contents\n",
    "                locals()[variable_name] = new_arr\n",
    "            else:\n",
    "                arr[index] = file_contents\n",
    "        else:\n",
    "            arr = np.empty(index + 1, dtype=object)\n",
    "            arr[index] = file_contents\n",
    "            locals()[variable_name] = arr\n",
    "    else:\n",
    "        locals()[variable_name] = file_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1c06b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change th 1000000.0 values in DISTWALK and DISTBIKE to 0\n",
    "DISTWALK = np.where(DISTWALK == 1000000.0, 0, DISTWALK)\n",
    "DISTBIKE = np.where(DISTBIKE == 1000000.0, 0, DISTBIKE)\n",
    "PNR_TRN_WLK_DTIM[1] =  np.where(PNR_TRN_WLK_DTIM[1] < 0, 0, PNR_TRN_WLK_DTIM[1])\n",
    "PNR_TRN_WLK_DTIM[2] =  np.where(PNR_TRN_WLK_DTIM[2] < 0, 0, PNR_TRN_WLK_DTIM[2])\n",
    "PNR_TRN_WLK_DTIM[3] =  np.where(PNR_TRN_WLK_DTIM[3] < 0, 0, PNR_TRN_WLK_DTIM[3])\n",
    "PNR_TRN_WLK_DTIM[4] =  np.where(PNR_TRN_WLK_DTIM[4] < 0, 0, PNR_TRN_WLK_DTIM[4])\n",
    "PNR_TRN_WLK_DTIM[5] =  np.where(PNR_TRN_WLK_DTIM[5] < 0, 0, PNR_TRN_WLK_DTIM[5])\n",
    "\n",
    "KNR_TRN_WLK_DTIM[1] =  np.where(KNR_TRN_WLK_DTIM[1] < 0, 0, KNR_TRN_WLK_DTIM[1])\n",
    "KNR_TRN_WLK_DTIM[2] =  np.where(KNR_TRN_WLK_DTIM[2] < 0, 0, KNR_TRN_WLK_DTIM[2])\n",
    "KNR_TRN_WLK_DTIM[3] =  np.where(KNR_TRN_WLK_DTIM[3] < 0, 0, KNR_TRN_WLK_DTIM[3])\n",
    "KNR_TRN_WLK_DTIM[4] =  np.where(KNR_TRN_WLK_DTIM[4] < 0, 0, KNR_TRN_WLK_DTIM[4])\n",
    "KNR_TRN_WLK_DTIM[5] =  np.where(KNR_TRN_WLK_DTIM[5] < 0, 0, KNR_TRN_WLK_DTIM[5])\n",
    "\n",
    "WLK_TRN_PNR_DTIM[1] =  np.where(WLK_TRN_PNR_DTIM[1] < 0, 0, WLK_TRN_PNR_DTIM[1])\n",
    "WLK_TRN_PNR_DTIM[2] =  np.where(WLK_TRN_PNR_DTIM[2] < 0, 0, WLK_TRN_PNR_DTIM[2])\n",
    "WLK_TRN_PNR_DTIM[3] =  np.where(WLK_TRN_PNR_DTIM[3] < 0, 0, WLK_TRN_PNR_DTIM[3])\n",
    "WLK_TRN_PNR_DTIM[4] =  np.where(WLK_TRN_PNR_DTIM[4] < 0, 0, WLK_TRN_PNR_DTIM[4])\n",
    "WLK_TRN_PNR_DTIM[5] =  np.where(WLK_TRN_PNR_DTIM[5] < 0, 0, WLK_TRN_PNR_DTIM[5])\n",
    "\n",
    "WLK_TRN_KNR_DTIM[1] =  np.where(WLK_TRN_KNR_DTIM[1] < 0, 0, WLK_TRN_KNR_DTIM[1])\n",
    "WLK_TRN_KNR_DTIM[2] =  np.where(WLK_TRN_KNR_DTIM[2] < 0, 0, WLK_TRN_KNR_DTIM[2])\n",
    "WLK_TRN_KNR_DTIM[3] =  np.where(WLK_TRN_KNR_DTIM[3] < 0, 0, WLK_TRN_KNR_DTIM[3])\n",
    "WLK_TRN_KNR_DTIM[4] =  np.where(WLK_TRN_KNR_DTIM[4] < 0, 0, WLK_TRN_KNR_DTIM[4])\n",
    "WLK_TRN_KNR_DTIM[5] =  np.where(WLK_TRN_KNR_DTIM[5] < 0, 0, WLK_TRN_KNR_DTIM[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aa6ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly check few matrix cores\n",
    "#PNR_TRN_WLK_DDIST[4].sum()\n",
    "#PNR_TRN_WLK_DDIST[2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8b7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct ramp has params.properties which has certain parameter values used in the utility equations. \n",
    "# Following function extracts these values.\n",
    "\n",
    "\n",
    "def extract_property_values(file_path, variables):\n",
    "    property_values = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#'):\n",
    "                key, value = line.split('=', 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                if key in variables:\n",
    "                    property_values[key] = value\n",
    "    return property_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274f16e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee800fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for purp in purpose:\n",
    "    #print(purp)\n",
    "    # read the purpose tab from the UEC file. \n",
    "    uec_purp_columns = ['No', 'Token', 'Description', 'Filter','Formula for variable', \n",
    "               'Index','Alt1', 'Alt2', 'Alt3', 'Alt4', 'Alt5', 'Alt6', 'Alt7', 'Alt8', 'Alt9']\n",
    "    \n",
    "    uec_purp = pd.read_excel(_join(params['common_dir'], \"TripModeChoice.xlsx\"), sheet_name=purp)\n",
    "    uec_purp = uec_purp.iloc[2:]\n",
    "    uec_purp.columns = uec_purp_columns # assign column names\n",
    "    \n",
    "    # Removing NAs\n",
    "    uec_purp_params_prop = uec_purp.loc[~uec_purp['Token'].isna()]\n",
    "    # extract the parameters that have % in in their names, clean up-remove % and replace . with _\n",
    "    uec_purp_params_prop = uec_purp_params_prop.loc[(uec_purp_params_prop['Formula for variable'].str.contains('%')==True)]\n",
    "    uec_purp_params_prop['Formula for variable'] = uec_purp_params_prop['Formula for variable'].str.replace('%', '') \n",
    "    uec_purp_params_prop['Formula for variable'] = uec_purp_params_prop['Formula for variable'].str.replace(\".\", \"_\")\n",
    "    # read parameters file\n",
    "    file_path = _join(ctramp_dir, 'input', 'params.properties')\n",
    "    # extract list of parameters\n",
    "    prop_variables = list(uec_purp_params_prop['Formula for variable'])\n",
    "    prop_variables_tokens = list(uec_purp_params_prop['Token'])\n",
    "    prop_variables = [x.replace('_', '.') for x in prop_variables]\n",
    "\n",
    "    values = extract_property_values(file_path, prop_variables)\n",
    "    \n",
    "    # Create a dictionary to store the extracted values\n",
    "    extracted_values = {}\n",
    "\n",
    "    # Assign the extracted values to the dictionary\n",
    "    for variable, value in values.items():\n",
    "        extracted_values[variable] = value\n",
    "\n",
    "    # Print the values from the extracted_values dictionary \n",
    "    for variable, value in extracted_values.items():\n",
    "        #print(f'{variable}: {value}')\n",
    "        exec(f'{variable.replace(\".\", \"_\")} = {value}')\n",
    "    \n",
    "    \n",
    "    # Assign the values to tokens\n",
    "    # example costInitialTaxi = %taxi.baseFare%\n",
    "    for _, row in uec_purp_params_prop.iterrows():\n",
    "        variable_name = row['Token']\n",
    "        expression = row['Formula for variable']\n",
    "\n",
    "        # Evaluate the expression and store the result in the local environment\n",
    "        try:\n",
    "            # Evaluate the expression and store the result in the local environment\n",
    "            if expression in locals() and isinstance(locals()[expression], np.ndarray):\n",
    "                value = locals()[expression]\n",
    "            else:\n",
    "                value = eval(expression)\n",
    "\n",
    "            exec(f'{variable_name} = value')\n",
    "            #print(f\"Variable '{variable_name}' is defined.\")\n",
    "        except NameError:\n",
    "            #print(f\"Variable '{variable_name}' is not defined.\")\n",
    "            continue\n",
    "\n",
    "    \n",
    "    uec_purp_params = uec_purp.loc[~uec_purp['Formula for variable'].isna()]\n",
    "    uec_purp_params = uec_purp_params.loc[~uec_purp_params['Token'].isna()]\n",
    "    uec_purp_params = uec_purp_params.loc[~(uec_purp_params['Formula for variable'].str.contains('if')==True)]\n",
    "    uec_purp_params = uec_purp_params.loc[~(uec_purp_params['Formula for variable'].str.contains('%')==True)]\n",
    "\n",
    "    uec_purp_params['Formula for variable'] = uec_purp_params['Formula for variable'].astype(str)\n",
    "    uec_purp_params['Formula for variable'] = uec_purp_params['Formula for variable'].str.replace('@', '')\n",
    "\n",
    "    key_column = 'Token'\n",
    "    value_column = 'Formula for variable'\n",
    "\n",
    "    # Create dictionary from selected columns\n",
    "    data_dict = {}\n",
    "\n",
    "    for _, row in uec_purp_params.iterrows():\n",
    "        key = row[key_column]\n",
    "        value = row[value_column]\n",
    "\n",
    "        # Handle values that are strings\n",
    "        if isinstance(value, str):\n",
    "            try:\n",
    "                value = int(value)\n",
    "                data_dict[key] = value\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    value = float(value)\n",
    "                    data_dict[key] = value\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "    #get all the parameters\n",
    "    variables = data_dict\n",
    "\n",
    "    for _, row in uec_purp_params.iterrows():\n",
    "        variable_name = row['Token']\n",
    "        expression = row['Formula for variable']\n",
    "\n",
    "        # Evaluate the expression and store the result in the local environment\n",
    "        try:\n",
    "            # Evaluate the expression and store the result in the local environment\n",
    "            if expression in locals() and isinstance(locals()[expression], np.ndarray):\n",
    "                value = locals()[expression]\n",
    "            else:\n",
    "                value = eval(expression)\n",
    "\n",
    "            exec(f'{variable_name} = value')\n",
    "        except NameError:\n",
    "            #print(f\"Variable '{variable_name}' is not defined.\")\n",
    "            continue\n",
    "    \n",
    "    #break\n",
    "    \n",
    "    int_zone = 3332\n",
    "    da_util = np.empty((5, int_zone, int_zone))\n",
    "    sr2_util = np.empty((5, int_zone, int_zone))\n",
    "    sr3_util = np.empty((5, int_zone, int_zone))\n",
    "    wlk_util =  np.empty((5, int_zone, int_zone))\n",
    "    bike_util = np.empty((5, int_zone, int_zone))\n",
    "    wlk_trn_wlk_util = np.empty((5, int_zone, int_zone))\n",
    "    wlk_trn_pnr_util = np.empty((5, int_zone, int_zone))\n",
    "    pnr_trn_wlk_util = np.empty((5, int_zone, int_zone))\n",
    "    wlk_trn_knr_util = np.empty((5, int_zone, int_zone))\n",
    "    knr_trn_wlk_util = np.empty((5, int_zone, int_zone))\n",
    "    taxi_util = np.empty((5, int_zone, int_zone))\n",
    "    \n",
    "    for tripPeriod in time_period:\n",
    "        #for trip_mode in oth_modes:\n",
    "        #uec_purp_mode = uec_purp_df.loc[uec_purp_df['Description'].str.contains(trip_mode)==True]\n",
    "        #uec_purp_mode['Formula for variable'] = uec_purp_mode['Formula for variable'].str.replace('tripPeriod', str(period))\n",
    "        #uec_purp_mode['formula_calculation'] = \n",
    "        util = omx.open_file(_join(preprocess_dir, f'util_{tripPeriod}_{purp}.omx'),'w')\n",
    "\n",
    "        #Drive alone\n",
    "        util['DA'] = c_ivt*SOV_TIME[tripPeriod][:int_zone, :int_zone]\n",
    "        print(tripPeriod, purp, 'DA', \" \", np.array(util['DA']).min(), np.array(util['DA']).max())\n",
    "\n",
    "        #Shared ride 2\n",
    "        util['SR2'] = c_ivt*HOV2_TIME[tripPeriod][:int_zone, :int_zone]\n",
    "        print(tripPeriod, purp, 'SR2', \" \", np.array(util['SR2']).min(), np.array(util['SR2']).max())\n",
    "\n",
    "        #Shared ride 3\n",
    "        util['SR3'] = c_ivt*HOV3_TIME[tripPeriod][:int_zone, :int_zone]\n",
    "        print(tripPeriod, purp, 'SR3', \" \", np.array(util['SR3']).min(), np.array(util['SR3']).max())\n",
    "\n",
    "        # Walk \n",
    "        util['WALK'] = (walk_dist<=1)* (c_walkTimeShort * np.minimum(walk_dist * 60 / walkSpeed, walkThresh * 60 / walkSpeed)) + \\\n",
    "                       (walk_dist>1)* (c_walkTimeLong * np.maximum(walk_dist * 60 / walkSpeed, walkThresh * 60 / walkSpeed)) \n",
    "        print(tripPeriod, purp, 'WALK', \" \", np.array(util['WALK']).min(), np.array(util['WALK']).max())\n",
    "        \n",
    "        #Bike\n",
    "        util['BIKE'] = (bike_dist<=6)*(c_bikeTimeShort* np.minimum(bike_dist*60/bikeSpeed, bikeThresh*60/bikeSpeed)) + \\\n",
    "                       (bike_dist>6)*(c_bikeTimeLong* np.maximum(bike_dist*60/bikeSpeed, bikeThresh*60/bikeSpeed))\n",
    "        print(tripPeriod, purp, 'BIKE', \" \", np.array(util['BIKE']).min(), np.array(util['BIKE']).max())\n",
    "        \n",
    "        \n",
    "        #Walk transit Walk\n",
    "        util['WLK_TRN_WLK'] =  c_ivt*WLK_TRN_WLK_IVT_LOC[tripPeriod]/100 + \\\n",
    "                            c_ivt_exp*WLK_TRN_WLK_IVT_EXP[tripPeriod]/100 + \\\n",
    "                            c_ivt_lrt*WLK_TRN_WLK_IVT_LRT[tripPeriod]/100 + \\\n",
    "                            c_ivt_ferry*WLK_TRN_WLK_IVT_FRY[tripPeriod]/100 + \\\n",
    "                            c_ivt_hvy*WLK_TRN_WLK_IVT_HVY[tripPeriod]/100 + \\\n",
    "                            c_ivt_com*WLK_TRN_WLK_IVT_COM[tripPeriod]/100 + \\\n",
    "                            c_ivt_trn_crwd*WLK_TRN_WLK_CROWD[tripPeriod]/100 + \\\n",
    "                            c_shortiWait*np.minimum(WLK_TRN_WLK_IWAIT[tripPeriod]/100,waitThresh) + \\\n",
    "                            c_longiWait*np.maximum(WLK_TRN_WLK_IWAIT[tripPeriod]/100-waitThresh,0) + \\\n",
    "                            c_xwait*WLK_TRN_WLK_XWAIT[tripPeriod]/100 + \\\n",
    "                            c_xfers_wlk * np.maximum(WLK_TRN_WLK_BOARDS[tripPeriod]-1,0) + \\\n",
    "                            c_waux*WLK_TRN_WLK_WAUX[tripPeriod]/100\n",
    "        \n",
    "        print(tripPeriod, purp, 'WLK_TRN_WLK', \" \", np.array(util['WLK_TRN_WLK']).min(), np.array(util['WLK_TRN_WLK']).max())\n",
    "        \n",
    "        # Walk Transit PNR - Inbound\n",
    "        util['WLK_TRN_PNR'] =  c_ivt*WLK_TRN_PNR_TOTIVT[tripPeriod]/100 + \\\n",
    "                            (c_ivt_com-c_ivt)*WLK_TRN_PNR_IVT_COM[tripPeriod]/100 + \\\n",
    "                            c_ivt_trn_crwd*WLK_TRN_PNR_CROWD[tripPeriod]/100 + \\\n",
    "                            c_shortiWait*np.minimum(WLK_TRN_PNR_IWAIT[tripPeriod]/100,waitThresh) + \\\n",
    "                            c_longiWait*np.maximum(WLK_TRN_PNR_IWAIT[tripPeriod]/100-waitThresh,0) + \\\n",
    "                            c_xwait*WLK_TRN_PNR_XWAIT[tripPeriod]/100 + \\\n",
    "                            c_dtim*WLK_TRN_PNR_DTIM[tripPeriod]/100 + \\\n",
    "                            c_xfers_drv*np.maximum(WLK_TRN_PNR_BOARDS[tripPeriod]-1,0) + \\\n",
    "                            c_waux*WLK_TRN_PNR_WAUX[tripPeriod]/100 + \\\n",
    "                            c_dacc_ratio*(WLK_TRN_PNR_DDIST[tripPeriod]/100/SOV_DIST[tripPeriod][:int_zone, :int_zone])\n",
    "        print(tripPeriod, purp, 'WLK_TRN_PNR', \" \", np.array(util['WLK_TRN_PNR']).min(), np.array(util['WLK_TRN_PNR']).max())\n",
    "\n",
    "        # PNR transit Walk - Outbound\n",
    "        util['PNR_TRN_WLK'] =  c_ivt*PNR_TRN_WLK_TOTIVT[tripPeriod]/100 + \\\n",
    "                            (c_ivt_com-c_ivt)*PNR_TRN_WLK_IVT_COM[tripPeriod]/100 + \\\n",
    "                            c_ivt_trn_crwd*PNR_TRN_WLK_CROWD[tripPeriod]/100 + \\\n",
    "                            c_shortiWait*np.minimum(PNR_TRN_WLK_IWAIT[tripPeriod]/100,waitThresh) + \\\n",
    "                            c_longiWait*np.maximum(PNR_TRN_WLK_IWAIT[tripPeriod]/100-waitThresh,0) + \\\n",
    "                            c_xwait*PNR_TRN_WLK_XWAIT[tripPeriod]/100 + \\\n",
    "                            c_dtim*PNR_TRN_WLK_DTIM[tripPeriod]/100 + \\\n",
    "                            c_waux*PNR_TRN_WLK_WAUX[tripPeriod]/100 + \\\n",
    "                            c_dacc_ratio*(PNR_TRN_WLK_DDIST[tripPeriod]/100/SOV_DIST[tripPeriod][:int_zone, :int_zone])\n",
    "        print(tripPeriod, purp, 'PNR_TRN_WLK', \" \", np.array(util['PNR_TRN_WLK']).min(), np.array(util['PNR_TRN_WLK']).max())\n",
    "\n",
    "        # Walk Transit KNR - Inbound\n",
    "        util['WLK_TRN_KNR'] = c_ivt*WLK_TRN_KNR_TOTIVT[tripPeriod]/100 + \\\n",
    "                            (c_ivt_com-c_ivt)*WLK_TRN_KNR_IVT_COM[tripPeriod]/100 + \\\n",
    "                            c_ivt_trn_crwd*WLK_TRN_KNR_CROWD[tripPeriod]/100 + \\\n",
    "                            c_shortiWait*np.minimum(WLK_TRN_KNR_IWAIT[tripPeriod]/100,waitThresh) + \\\n",
    "                            c_longiWait*np.maximum(WLK_TRN_KNR_IWAIT[tripPeriod]/100-waitThresh,0) + \\\n",
    "                            c_xwait*WLK_TRN_KNR_XWAIT[tripPeriod]/100 + \\\n",
    "                            c_xfers_drv*np.maximum(WLK_TRN_KNR_BOARDS[tripPeriod]-1,0) + \\\n",
    "                            c_dtim*WLK_TRN_KNR_DTIM[tripPeriod]/100 + \\\n",
    "                            c_waux*WLK_TRN_KNR_WAUX[tripPeriod]/100 + \\\n",
    "                            c_dacc_ratio*(WLK_TRN_KNR_DDIST[tripPeriod]/100/SOV_DIST[tripPeriod][:int_zone, :int_zone])\n",
    "        print(tripPeriod, purp, 'WLK_TRN_KNR', \" \", np.array(util['WLK_TRN_KNR']).min(), np.array(util['WLK_TRN_KNR']).max())\n",
    "\n",
    "        # KNR Transit Walk - Outbound\n",
    "        util['KNR_TRN_WLK'] = c_ivt*KNR_TRN_WLK_TOTIVT[tripPeriod]/100 + \\\n",
    "                            (c_ivt_com-c_ivt)*KNR_TRN_WLK_IVT_COM[tripPeriod]/100 + \\\n",
    "                            c_ivt_trn_crwd*KNR_TRN_WLK_CROWD[tripPeriod]/100 + \\\n",
    "                            c_shortiWait*np.minimum(KNR_TRN_WLK_IWAIT[tripPeriod]/100,waitThresh) + \\\n",
    "                            c_longiWait*np.maximum(KNR_TRN_WLK_IWAIT[tripPeriod]/100-waitThresh,0) + \\\n",
    "                            c_xwait*KNR_TRN_WLK_XWAIT[tripPeriod]/100 + \\\n",
    "                            c_xfers_drv*np.maximum(KNR_TRN_WLK_BOARDS[tripPeriod]-1,0) + \\\n",
    "                            c_dtim*KNR_TRN_WLK_DTIM[tripPeriod]/100 + \\\n",
    "                            c_waux*KNR_TRN_WLK_WAUX[tripPeriod]/100 + \\\n",
    "                            c_dacc_ratio*(KNR_TRN_WLK_DDIST[tripPeriod]/100/SOV_DIST[tripPeriod][:int_zone, :int_zone])\n",
    "        print(tripPeriod, purp, 'KNR_TRN_WLK', \" \", np.array(util['KNR_TRN_WLK']).min(), np.array(util['KNR_TRN_WLK']).max())\n",
    "\n",
    "        # taxi\n",
    "        util['RIDEHAIL'] = c_ivt*HOV2_TIME[tripPeriod][:int_zone, :int_zone]  + c_ivt*1.5*taxi_wait_time\n",
    "        print(tripPeriod, purp, 'RIDEHAIL', \" \", np.array(util['RIDEHAIL']).min(), np.array(util['RIDEHAIL']).max())\n",
    "        \n",
    "    \n",
    "        util.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac47c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398559e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add mappings from time period and purpose\n",
    "df_trips = pd.read_parquet(_join(preprocess_dir, 'trip_roster.parquet'))\n",
    "len(df_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inbound trips get orig purpose, outbound trips get dest purpose\n",
    "df_trips['util_purpose'] = np.where(df_trips['inbound']==1, df_trips['orig_purpose'], df_trips['dest_purpose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57296ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_trips['util_purpose'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb2b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "purp_dict = { 'work' : 'Work', \n",
    "              'shopping' : 'Shopping',\n",
    "              'escort' : 'Escort', \n",
    "              'othdiscr': 'OthDiscr',\n",
    "              'othmaint': 'OthMaint',\n",
    "              'school' : 'School', \n",
    "              'eatout' : 'EatOut', \n",
    "              'atwork' : 'WorkBased', \n",
    "              'social' : 'Social',\n",
    "              'university' : 'University'}\n",
    "\n",
    "time_period = {1:'EA',2:'AM',3:'MD',4:'PM',5:'EV'} #1 for EA, 2 for AM, 3 for MD, 4 for PM and 5 for EV\n",
    "\n",
    "purpose = ['Work', 'University', 'School', 'Escort', 'Shopping', 'EatOut', \n",
    "           'OthMaint', 'Social', 'OthDiscr', 'WorkBased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11506ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips['util_purpose'] = df_trips['util_purpose'].map(purp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bafe0b00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing purpose: Work and time period: EA\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_trips' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_trips' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "num_zones = 3332\n",
    "\n",
    "for tripPeriod, value in time_period.items():\n",
    "\n",
    "    for purp in purpose:\n",
    "        print(f'Analyzing purpose: {purp} and time period: {value}')\n",
    "        #df_temp = df_trips.query(f\"util_purpose == {purp} and Period == {value.lower()}\")\n",
    "        #df_temp = df_trips.query(f\"util_purpose == '{purp}' and Period == '{value.lower}'\")\n",
    "        df_temp = df_trips.loc[(df_trips['util_purpose'] == purp) & (df_trips['Period'] == value.lower())]\n",
    "        \n",
    "        # Generate all combinations of orig and dest\n",
    "        combinations = list(itertools.product(range(1, num_zones + 1), repeat=2))\n",
    "\n",
    "        # Create the DataFrame with orig and dest columns\n",
    "        purp_df = pd.DataFrame(combinations, columns=['orig', 'dest'])\n",
    "\n",
    "        # read utility files\n",
    "        util_file = omx.open_file(_join(preprocess_dir, f'util_{tripPeriod}_{purp}.omx'))\n",
    "\n",
    "        for core in util_file.list_matrices():\n",
    "            print(f'extracting {core} core form utility file')\n",
    "            mode_core = np.array(util_file[core])\n",
    "            mode_core = np.where(mode_core == 0, -999, mode_core)\n",
    "            skm_df = pd.DataFrame(mode_core)\n",
    "            skm_df = pd.melt(skm_df.reset_index(), id_vars='index', value_vars=skm_df.columns)\n",
    "            skm_df['index'] = skm_df['index'] + 1\n",
    "            skm_df['variable'] = skm_df['variable'] + 1\n",
    "            skm_df.columns = ['orig', 'dest', core]\n",
    "            purp_df = pd.merge(purp_df, skm_df, on=['orig', 'dest'], how='left')\n",
    "        \n",
    "        df_temp = pd.merge(df_temp, purp_df, left_on=['orig_taz', 'dest_taz'], right_on=['orig', 'dest'], how='left')\n",
    "        \n",
    "        print(f'writing the trip file for purpose : {purp} and time period: {value}', df_temp.shape)\n",
    "        df_temp.to_parquet(_join(preprocess_dir, f'trip_{tripPeriod}_{purp}.parquet'))\n",
    "        \n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9df1b187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35968941"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine all trips into one file\n",
    "\n",
    "# read trip files\n",
    "final_trips = []\n",
    "\n",
    "for tripPeriod, value in time_period.items():\n",
    "    for purp in purpose:\n",
    "        temp = pd.read_parquet(_join(preprocess_dir, f'trip_{tripPeriod}_{purp}.parquet'))\n",
    "        final_trips.append(temp)\n",
    "        \n",
    "final_trips = pd.concat(final_trips)\n",
    "len(final_trips) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48019235",
   "metadata": {},
   "source": [
    "### Calculate the logsums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85e307aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_nesting_coef = 0.72\n",
    "trn_nesting_coef = 0.72\n",
    "nm_nest_coef = 0.72\n",
    "ridehail_nest_coef = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa3f9c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logsum\n",
    "\n",
    "final_trips['auto_ls'] = auto_nesting_coef * (np.log(np.exp(final_trips['DA']/auto_nesting_coef) + \n",
    "                                                     np.exp(final_trips['SR2']/auto_nesting_coef) + \n",
    "                                                     np.exp(final_trips['SR3']/auto_nesting_coef)))\n",
    "\n",
    "\n",
    "final_trips['exp_trn'] = np.exp(final_trips['WLK_TRN_WLK']/trn_nesting_coef) + \\\n",
    "                               np.exp(final_trips['WLK_TRN_PNR']/trn_nesting_coef) + \\\n",
    "                               np.exp(final_trips['PNR_TRN_WLK']/trn_nesting_coef) + \\\n",
    "                               np.exp(final_trips['WLK_TRN_KNR']/trn_nesting_coef) + \\\n",
    "                               np.exp(final_trips['KNR_TRN_WLK']/trn_nesting_coef)\n",
    "\n",
    "final_trips['trn_ls'] = np.where(final_trips['exp_trn'] > 0, trn_nesting_coef *(np.log(final_trips['exp_trn'])), 0)\n",
    "\n",
    "\n",
    "final_trips['exp_nm'] = np.exp(final_trips['WALK']/nm_nest_coef) + \\\n",
    "                                  np.exp(final_trips['BIKE']/nm_nest_coef)\n",
    "    \n",
    "final_trips['non_mot_ls'] = np.where(final_trips['exp_nm'] > 0, nm_nest_coef * (np.log(final_trips['exp_nm'])), 0)\n",
    "\n",
    "final_trips['exp_ridehail'] = np.exp(final_trips['RIDEHAIL']/ridehail_nest_coef)\n",
    "final_trips['ridehail_ls'] = np.where(final_trips['exp_ridehail']>0, ridehail_nest_coef * (np.log(final_trips['exp_ridehail'])),0)\n",
    "\n",
    "final_trips['allmode_ls'] = np.log(np.exp(final_trips['auto_ls']) + \n",
    "                                np.exp(final_trips['trn_ls']) + \n",
    "                                np.exp(final_trips['non_mot_ls']) + \n",
    "                                np.exp(final_trips['ridehail_ls']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae62cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logsum\n",
    "final_trips['allmode_ls_adj'] = final_trips['allmode_ls'] - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "325627a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_trips['sum_ls'] = np.exp(final_trips['auto_ls']) + np.exp(final_trips['trn_ls']) +  np.exp(final_trips['non_mot_ls']) + np.exp(final_trips['ridehail_ls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0aee9e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_trips[final_trips['allmode_ls']<0][1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9198edc1",
   "metadata": {},
   "source": [
    "### Get BETA IVT values for each purpose from UEC sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70fa90b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Work\n",
      "University\n",
      "School\n",
      "Escort\n",
      "Shopping\n",
      "EatOut\n",
      "OthMaint\n",
      "Social\n",
      "OthDiscr\n",
      "WorkBased\n"
     ]
    }
   ],
   "source": [
    "# get beta IVT for each purpose\n",
    "ivt_purp = pd.DataFrame(columns=['util_purpose'])\n",
    "\n",
    "for purp in purpose:\n",
    "    print(purp)\n",
    "    # read the purpose tab from the UEC file. \n",
    "    uec_purp_columns = ['No', 'Token', 'Description', 'Filter','Formula for variable', \n",
    "               'Index','Alt1', 'Alt2', 'Alt3', 'Alt4', 'Alt5', 'Alt6', 'Alt7', 'Alt8', 'Alt9']\n",
    "    \n",
    "    uec_purp = pd.read_excel(_join(params['common_dir'], \"TripModeChoice.xlsx\"), sheet_name=purp)\n",
    "    uec_purp = uec_purp.iloc[2:]\n",
    "    uec_purp.columns = uec_purp_columns # assign column names\n",
    "    \n",
    "    ivt = uec_purp.loc[uec_purp['Token']=='c_ivt', 'Formula for variable'].item()\n",
    "    #ivt_lrt = uec_purp.loc[uec_purp['Token']=='c_ivt_lrt', 'Formula for variable'].item()\n",
    "    #ivt_ferry = uec_purp.loc[uec_purp['Token']=='c_ivt_ferry', 'Formula for variable'].item()\n",
    "    #ivt_exp = uec_purp.loc[uec_purp['Token']=='c_ivt_exp', 'Formula for variable'].item()\n",
    "    #ivt_hvy = uec_purp.loc[uec_purp['Token']=='c_ivt_hvy', 'Formula for variable'].item()\n",
    "    #ivt_com = uec_purp.loc[uec_purp['Token']=='c_ivt_com', 'Formula for variable'].item()\n",
    "    \n",
    "    ivt_purp = ivt_purp.append({'util_purpose': purp, 'b_ivt': ivt #'b_ivt_lrt': ivt_lrt,\n",
    "                                #'b_ivt_ferry' : ivt_ferry, 'b_ivt_exp': ivt_exp, \n",
    "                                #'b_ivt_hvy': ivt_hvy, 'b_ivt_com': ivt_com\n",
    "                               }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "482367fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with trip roster\n",
    "final_trips = pd.merge(final_trips, ivt_purp, on = 'util_purpose', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db18c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logsum benefits auto\n",
    "final_trips['ls_benefit_auto'] =  (final_trips['allmode_ls_adj'] * \\\n",
    "                                    (np.exp(final_trips['auto_ls'])/final_trips['sum_ls']) * \\\n",
    "                                        (final_trips['trips']/final_trips['b_ivt'])) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb787ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logsum benefits transit\n",
    "final_trips['ls_benefit_transit'] = (final_trips['allmode_ls_adj'] * \\\n",
    "                                         (np.exp(final_trips['trn_ls'])/final_trips['sum_ls']) * \\\n",
    "                                            (final_trips['trips']/final_trips['b_ivt']))  #* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32fca46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logsum benefits ridehail\n",
    "final_trips['ls_benefit_raidehail'] = (final_trips['allmode_ls_adj'] * \\\n",
    "                                         (np.exp(final_trips['ridehail_ls'])/final_trips['sum_ls']) * \\\n",
    "                                            (final_trips['trips']/final_trips['b_ivt']))  #* final_trips['trips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30e77e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logsum benefits non-motorized\n",
    "final_trips['ls_benefit_nm'] =  (final_trips['allmode_ls_adj'] * \\\n",
    "                                         (np.exp(final_trips['non_mot_ls'])/final_trips['sum_ls']) * \\\n",
    "                                            (final_trips['trips']/final_trips['b_ivt']))  #* final_trips['trips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e6d19b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# superdistrict average composite utility\n",
    "super_dist = final_trips.groupby(['orig_super_dist', 'dest_super_dist', 'Period'])['allmode_ls'].mean().reset_index()\n",
    "super_dist.to_csv(\"super_dist_composite_utility_\"+concept_id+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97c6f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# superdistrict average composite utility\n",
    "super_dist = final_trips.groupby(['orig_super_dist', 'dest_super_dist', 'Period'])['allmode_ls'].mean().reset_index()\n",
    "super_dist.to_csv(\"super_dist_composite_utility_\"+concept_id+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2efc36eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# superdistrict average composite utility\n",
    "super_dist = final_trips.groupby(['orig_super_dist', 'dest_super_dist', 'Period'])['sum_ls'].mean().reset_index()\n",
    "super_dist.to_csv(\"super_dist_composite_utility_\"+concept_id+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e320308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# superdistrict average composite utility\n",
    "super_dist = final_trips.groupby(['orig_super_dist', 'dest_super_dist', 'Period'])['trips'].sum().reset_index()\n",
    "super_dist.to_csv(\"super_dist_trips_\"+concept_id+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f2c2028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# superdistrict average composite utility\n",
    "super_dist_util = final_trips.groupby(['orig_super_dist', 'dest_super_dist', 'Period'])['allmode_ls', \n",
    "                                                                                   'auto_ls', \n",
    "                                                                                   'trn_ls'].mean().reset_index()\n",
    "\n",
    "super_dist_trips = final_trips.groupby(['orig_super_dist', 'dest_super_dist', 'Period'])['trips'].sum().reset_index()\n",
    "\n",
    "super_dist = pd.merge(super_dist_util,super_dist_trips, on=['orig_super_dist', 'dest_super_dist', 'Period'], how='left')\n",
    "\n",
    "super_dist.to_csv(\"super_dist_util_trips\"+concept_id+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c88032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# superdistrict average composite utility\n",
    "trn_ls = final_trips.groupby(['orig_taz', 'dest_taz', 'Period'])['trn_ls'].mean().reset_index()\n",
    "trn_ls = trn_ls.pivot(index=['orig_taz', 'dest_taz'], columns='Period', values='trn_ls')\n",
    "trn_ls.to_csv(\"taz_trnls\"+concept_id+\".csv.gz\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3f886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f54d48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_col = 'ls_benefit_transit'\n",
    "mode_numbers = [6,7,8]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "trn = df_region_period[['Period', 'Value']]\n",
    "trn.columns = ['Period', 'trn']\n",
    "#df_region_period.to_csv('trn_ls.csv')\n",
    "\n",
    "summary_col = 'ls_benefit_auto'\n",
    "mode_numbers = [1,2,3]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "auto = df_region_period[['Period', 'Value']]\n",
    "auto.columns = ['Period', 'auto']\n",
    "#df_region_period.to_csv('auto_ls.csv')\n",
    "\n",
    "summary_col = 'ls_benefit_nm'\n",
    "mode_numbers = [4,5]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "nm = df_region_period[['Period', 'Value']]\n",
    "nm.columns = ['Period', 'nm']\n",
    "#df_region_period.to_csv('nm_ls.csv')\n",
    "\n",
    "summary_col = 'ls_benefit_raidehail'\n",
    "mode_numbers = [9]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "rh = df_region_period[['Period', 'Value']]\n",
    "rh.columns = ['Period', 'rh']\n",
    "#df_region_period.to_csv('rh_ls.csv')\n",
    "\n",
    "final = pd.merge(trn, auto, on = 'Period').merge(\n",
    "                    nm, on='Period').merge(\n",
    "                    rh, on='Period')\n",
    "\n",
    "final.to_csv(\"ls_benefits\"+concept_id+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "491d27d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_col = 'trips'\n",
    "mode_numbers = [6,7,8]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "trn = df_region_period[['Period', 'Value']]\n",
    "trn.columns = ['Period', 'trn']\n",
    "#df_region_period.to_csv('trn_ls.csv')\n",
    "\n",
    "summary_col = 'trips'\n",
    "mode_numbers = [1,2,3]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "auto = df_region_period[['Period', 'Value']]\n",
    "auto.columns = ['Period', 'auto']\n",
    "#df_region_period.to_csv('auto_ls.csv')\n",
    "\n",
    "summary_col = 'trips'\n",
    "mode_numbers = [4,5]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "nm = df_region_period[['Period', 'Value']]\n",
    "nm.columns = ['Period', 'nm']\n",
    "#df_region_period.to_csv('nm_ls.csv')\n",
    "\n",
    "summary_col = 'trips'\n",
    "mode_numbers = [9]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "rh = df_region_period[['Period', 'Value']]\n",
    "rh.columns = ['Period', 'rh']\n",
    "#df_region_period.to_csv('rh_ls.csv')\n",
    "\n",
    "final = pd.merge(trn, auto, on = 'Period').merge(\n",
    "                    nm, on='Period').merge(\n",
    "                    rh, on='Period')\n",
    "\n",
    "final.to_csv(\"ls_benefits\"+concept_id+\"_trips.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59945a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Add distance\n",
    "md_dist = omx.open_file(_join(ctramp_dir, 'skims\\HWYSKMmd.omx'))\n",
    "md_dist = skim_core_to_df(md_dist, 'DISTDAM', cols =['orig', 'dest', 'dist'])\n",
    "\n",
    "final_trips = pd.merge(final_trips, md_dist, left_on=['orig_taz', 'dest_taz'], right_on=['orig', 'dest'], how='left')\n",
    "\n",
    "def calculate_weighted_average_by_category(df):\n",
    "    weighted_avgs = df.groupby('Period').apply(\n",
    "        lambda group: (group['trips'] * group['dist']).sum() / group['trips'].sum()\n",
    "    )\n",
    "    \n",
    "    weighted_avgs = weighted_avgs.reset_index()\n",
    "    weighted_avgs.columns = ['Period', 'dist']\n",
    "    weighted_avgs_all = pd.DataFrame(columns=['Period', 'dist'])\n",
    "    weighted_avgs_all.loc[0] = ['All', (df['dist'] * df['trips']).sum() / df['trips'].sum()]\n",
    " \n",
    "    weighted_avgs_all = pd.concat([weighted_avgs_all, weighted_avgs], ignore_index=True)\n",
    "    \n",
    "    return weighted_avgs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdf50f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_col = 'trips'\n",
    "mode_numbers = [6,7,8]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = calculate_weighted_average_by_category(temp)\n",
    "trn = df_region_period[['Period', 'dist']]\n",
    "trn.columns = ['Period', 'trn']\n",
    "#df_region_period.to_csv('trn_ls.csv')\n",
    "\n",
    "summary_col = 'trips'\n",
    "mode_numbers = [1,2,3]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = calculate_weighted_average_by_category(temp)\n",
    "auto = df_region_period[['Period', 'dist']]\n",
    "auto.columns = ['Period', 'auto']\n",
    "#df_region_period.to_csv('auto_ls.csv')\n",
    "\n",
    "summary_col = 'trips'\n",
    "mode_numbers = [4,5]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = calculate_weighted_average_by_category(temp)\n",
    "nm = df_region_period[['Period', 'dist']]\n",
    "nm.columns = ['Period', 'nm']\n",
    "#df_region_period.to_csv('nm_ls.csv')\n",
    "\n",
    "summary_col = 'trips'\n",
    "mode_numbers = [9]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = calculate_weighted_average_by_category(temp)\n",
    "rh = df_region_period[['Period', 'dist']]\n",
    "rh.columns = ['Period', 'rh']\n",
    "#df_region_period.to_csv('rh_ls.csv')\n",
    "\n",
    "final = pd.merge(trn, auto, on = 'Period').merge(\n",
    "                    nm, on='Period').merge(\n",
    "                    rh, on='Period')\n",
    "\n",
    "final.to_csv(\"ls_benefits\"+concept_id+\"_trip_length.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f10b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_col = 'allmode_ls'\n",
    "mode_numbers = [6,7,8]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "trn = df_region_period[['Period', 'Value']]\n",
    "trn.columns = ['Period', 'trn']\n",
    "#df_region_period.to_csv('trn_ls.csv')\n",
    "\n",
    "summary_col = 'allmode_ls'\n",
    "mode_numbers = [1,2,3]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "auto = df_region_period[['Period', 'Value']]\n",
    "auto.columns = ['Period', 'auto']\n",
    "#df_region_period.to_csv('auto_ls.csv')\n",
    "\n",
    "summary_col = 'allmode_ls'\n",
    "mode_numbers = [4,5]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "nm = df_region_period[['Period', 'Value']]\n",
    "nm.columns = ['Period', 'nm']\n",
    "#df_region_period.to_csv('nm_ls.csv')\n",
    "\n",
    "summary_col = 'allmode_ls'\n",
    "mode_numbers = [9]\n",
    "temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "rh = df_region_period[['Period', 'Value']]\n",
    "rh.columns = ['Period', 'rh']\n",
    "#df_region_period.to_csv('rh_ls.csv')\n",
    "\n",
    "final = pd.merge(trn, auto, on = 'Period').merge(\n",
    "                    nm, on='Period').merge(\n",
    "                    rh, on='Period')\n",
    "\n",
    "final.to_csv(\"ls_benefits_allmode\"+concept_id+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bafbff",
   "metadata": {},
   "source": [
    "## Creating Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd942ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional Value\n",
    "\n",
    "def create_summaries(final_trips, summary_col, filename_verbose, metric_num, filename_extension, mode_numbers):\n",
    "    \n",
    "    temp = final_trips.loc[final_trips['trip_mode'].isin(mode_numbers)]\n",
    "    # Region\n",
    "    df_region_period = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column=summary_col)\n",
    "    df_region_period = df_region_period[['Period', 'Value']]\n",
    "\n",
    "    df_region_period['Concept_ID'] = concept_id\n",
    "    df_region_period['Metric_ID'] = metric_num\n",
    "    df_region_period['Metric_name'] = 'Travel time savings'\n",
    "    df_region_period['Submetric'] = metric_num + '.1'\n",
    "    df_region_period['Description'] = 'Travel time savings for new and existing users by primary mode'\n",
    "    df_region_period['Population'] = 'Whole Population'\n",
    "    df_region_period['Geography'] = 'Region'\n",
    "    df_region_period['Zone_ID'] = ''\n",
    "    df_region_period['Income'] = ''\n",
    "    df_region_period['Mode'] = ''\n",
    "    df_region_period['Purpose'] = ''\n",
    "    df_region_period['Origin_zone'] = ''\n",
    "    df_region_period['Dest_zone'] = ''\n",
    "    df_region_period['Units'] = 'minutes'\n",
    "    df_region_period['Total_Increment'] = ''\n",
    "    \n",
    "    # County\n",
    "    df_cnty = summarize_all_combinations(temp, groupby_columns=['orig_county', 'dest_county', 'Period'], \n",
    "                                           summary_column=summary_col)\n",
    "\n",
    "    df_cnty = df_cnty.rename(columns={ \n",
    "                                      'orig_county' : 'Origin_zone',\n",
    "                                      'dest_county' : 'Dest_zone'})\n",
    "    df_cnty = df_cnty[['Origin_zone', 'Dest_zone',  'Period', 'Value']]\n",
    "\n",
    "    df_cnty['Concept_ID'] = concept_id\n",
    "    df_cnty['Metric_ID'] = metric_num\n",
    "    df_cnty['Metric_name'] = 'Travel time savings'\n",
    "    df_cnty['Submetric'] =  metric_num + '.2'\n",
    "    df_cnty['Description'] = 'Travel time savings for new and existing users by primary mode in origin and destination county'\n",
    "    df_cnty['Population'] = 'Whole Population'\n",
    "    df_cnty['Geography'] = 'County'\n",
    "    df_cnty['Zone_ID'] = ''\n",
    "    df_cnty['Income'] = ''\n",
    "    df_cnty['Mode'] = ''\n",
    "    df_cnty['Purpose'] = ''\n",
    "    df_cnty['Units'] = 'minutes'\n",
    "    df_cnty['Total_Increment'] = ''\n",
    "    \n",
    "    # RDM\n",
    "    df_rdm = summarize_all_combinations(temp, groupby_columns=['orig_rdm_zones', 'dest_rdm_zones', 'Period'], \n",
    "                                           summary_column=summary_col)\n",
    "\n",
    "    df_rdm = df_rdm.rename(columns={ \n",
    "                                    'orig_rdm_zones' : 'Origin_zone',\n",
    "                                    'dest_rdm_zones' : 'Dest_zone'})\n",
    "\n",
    "    df_rdm = df_rdm[['Origin_zone', 'Dest_zone', 'Period', 'Value']]\n",
    "\n",
    "    df_rdm['Concept_ID'] = concept_id\n",
    "    df_rdm['Metric_ID'] = metric_num\n",
    "    df_rdm['Metric_name'] = 'Travel time savings'\n",
    "    df_rdm['Submetric'] =  metric_num + '.3'\n",
    "    df_rdm['Description'] = 'Travel time savings for new and existing users by primary mode in origin and destination RDM zone'\n",
    "    df_rdm['Population'] = 'Whole Population'\n",
    "    df_rdm['Geography'] = 'RDM'\n",
    "    df_rdm['Zone_ID'] = ''\n",
    "    df_rdm['Income'] = ''\n",
    "    df_rdm['Mode'] = ''\n",
    "    df_rdm['Purpose'] = ''\n",
    "    df_rdm['Units'] = 'minutes'\n",
    "    df_rdm['Total_Increment'] = ''\n",
    "    \n",
    "    # Super Districts\n",
    "    df_sd = summarize_all_combinations(temp, groupby_columns=['orig_super_dist', 'dest_super_dist', 'Period'], \n",
    "                                           summary_column=summary_col)\n",
    "\n",
    "    df_sd = df_sd.rename(columns={ \n",
    "                                  'orig_super_dist' : 'Origin_zone',\n",
    "                                  'dest_super_dist' : 'Dest_zone'})\n",
    "    df_sd = df_sd[['Origin_zone', 'Dest_zone', 'Period', 'Value']]\n",
    "\n",
    "    df_sd['Concept_ID'] = concept_id\n",
    "    df_sd['Metric_ID'] = metric_num\n",
    "    df_sd['Metric_name'] = 'Travel time savings'\n",
    "    df_sd['Submetric'] =  metric_num + '.4'\n",
    "    df_sd['Description'] = 'Travel time savings for new and existing users by primary mode in origin and destination super district'\n",
    "    df_sd['Population'] = 'Whole Population'\n",
    "    df_sd['Geography'] = 'Super district'\n",
    "    df_sd['Zone_ID'] = ''\n",
    "    df_sd['Income'] = ''\n",
    "    df_sd['Mode'] = ''\n",
    "    df_sd['Purpose'] = ''\n",
    "    df_sd['Units'] = 'trips'\n",
    "    df_sd['Total_Increment'] = ''\n",
    "    \n",
    "    # Prioirty Population\n",
    "    temp['pp_wtd_benefit'] = temp[summary_col] * temp['pp_share']/100\n",
    "    df_pp = summarize_all_combinations(temp, groupby_columns=['Period'], summary_column='pp_wtd_benefit')\n",
    "    df_pp = df_pp[['Period', 'Value']]\n",
    "\n",
    "    df_pp['Concept_ID'] = concept_id\n",
    "    df_pp['Metric_ID'] = metric_num\n",
    "    df_pp['Metric_name'] = 'Travel time savings'\n",
    "    df_pp['Submetric'] =  metric_num + '.5'\n",
    "    df_pp['Description'] = 'Travel time savings for new and existing users by primary mode'\n",
    "    df_pp['Population'] = 'Prioirty population'\n",
    "    df_pp['Geography'] = 'Region'\n",
    "    df_pp['Zone_ID'] = ''\n",
    "    df_pp['Origin_zone'] = ''\n",
    "    df_pp['Dest_zone'] = ''\n",
    "    df_pp['Income'] = ''\n",
    "    df_pp['Mode'] = ''\n",
    "    df_pp['Purpose'] = ''\n",
    "    df_pp['Units'] = 'trips'\n",
    "    df_pp['Total_Increment'] = ''\n",
    "    \n",
    "    all_dfs = [df_region_period, df_cnty, df_rdm, df_sd, df_pp]\n",
    "\n",
    "    for dfs in all_dfs:\n",
    "        metric_name = filename_verbose #'_travel_time_auto_savings_'\n",
    "        dfs = dfs.reset_index(drop=True)\n",
    "        dfs = dfs[perf_measure_columns]\n",
    "        file_name = dfs['Submetric'][0]\n",
    "        geography = '_' + dfs['Geography'][0].replace(' ', '_')\n",
    "        dfs.to_csv(_join(summary_dir, file_name + metric_name + concept_id + geography + filename_extension + '.csv'), index=None)\n",
    "        print(len(dfs), file_name, dfs['Metric_name'][0])\n",
    "\n",
    "    combined_df = pd.concat([df_region_period, df_cnty, df_rdm, df_sd, df_pp]).reset_index(drop=True)\n",
    "    combined_df.to_csv(_join(summary_dir,  metric_num + filename_verbose + concept_id + '_region' +filename_extension + '.csv'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d884787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_summaries(final_trips,'ls_benefit_transit', '_travel_time_savings_transit_', 'E1.1', filename_extension, mode_numbers=[6,7,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "849f6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_summaries(final_trips,'ls_benefit_auto', '_travel_time_savings_auto_', 'E1.2', filename_extension,  mode_numbers=[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90b2a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_summaries(final_trips, 'ls_benefit_raidehail', '_travel_time_savings_ridehail_', 'E1.3', filename_extension, mode_numbers=[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3810234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_summaries(final_trips, 'ls_benefit_nm', '_travel_time_savings_non-motorized', 'E1.4', filename_extension, mode_numbers=[4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dde4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf4a492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b6d6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ecc9192",
   "metadata": {},
   "source": [
    "## Effective Density Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a52f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dir = params['common_dir']\n",
    "decay_param_goods = 1.8\n",
    "decay_param_services = 1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "405ffd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data = pd.read_csv(_join(common_dir, f'EmpBreakdown{model_year}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcbd7a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emp_data['sector'] =  emp_data['link21'].map(emp_dict)\n",
    "emp_data = emp_data.groupby(['TAZ'])['jobs'].sum().reset_index()\n",
    "#emp_data = pd.pivot(emp_data, index='TAZ', columns='sector' , values='jobs').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7dadb263",
   "metadata": {},
   "outputs": [],
   "source": [
    "od_logsums = final_trips.groupby(['orig_taz', 'dest_taz'])['allmode_ls'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15cad23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "od_logsums = pd.merge(od_logsums, emp_data, left_on='dest_taz', right_on='TAZ', how='left')\n",
    "od_logsums['jobs'] = od_logsums['jobs'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cfb6a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "od_logsums['alpha_goods'] = decay_param_goods\n",
    "od_logsums['alpha_services'] = decay_param_services\n",
    "\n",
    "od_logsums['effective_density_good'] = od_logsums['jobs'] / (od_logsums['alpha_goods'] * od_logsums['allmode_ls'])\n",
    "od_logsums['effective_density_services'] = od_logsums['jobs'] / (od_logsums['alpha_services'] * od_logsums['allmode_ls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0dacf6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "od_logsums_orig = od_logsums.groupby(['orig_taz'])['effective_density_good', 'effective_density_services'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2598d52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_column = ['effective_density_good', 'effective_density_services']\n",
    "sector_column = 'effective_density_good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7010364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_taz</th>\n",
       "      <th>effective_density_good</th>\n",
       "      <th>effective_density_services</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2,220,711.865</td>\n",
       "      <td>2,103,832.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2,359,808.212</td>\n",
       "      <td>2,235,607.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2,456,595.038</td>\n",
       "      <td>2,327,300.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1,923,223.469</td>\n",
       "      <td>1,822,001.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2,695,514.755</td>\n",
       "      <td>2,553,645.558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3316</th>\n",
       "      <td>3328</td>\n",
       "      <td>2,405,156.953</td>\n",
       "      <td>2,278,569.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3317</th>\n",
       "      <td>3329</td>\n",
       "      <td>1,372,769.304</td>\n",
       "      <td>1,300,518.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3318</th>\n",
       "      <td>3330</td>\n",
       "      <td>2,048,338.317</td>\n",
       "      <td>1,940,531.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3319</th>\n",
       "      <td>3331</td>\n",
       "      <td>1,894,907.923</td>\n",
       "      <td>1,795,175.927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3320</th>\n",
       "      <td>3332</td>\n",
       "      <td>1,418,296.672</td>\n",
       "      <td>1,343,649.478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3321 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      orig_taz  effective_density_good  effective_density_services\n",
       "0            1           2,220,711.865               2,103,832.293\n",
       "1            2           2,359,808.212               2,235,607.780\n",
       "2            3           2,456,595.038               2,327,300.562\n",
       "3            4           1,923,223.469               1,822,001.182\n",
       "4            5           2,695,514.755               2,553,645.558\n",
       "...        ...                     ...                         ...\n",
       "3316      3328           2,405,156.953               2,278,569.745\n",
       "3317      3329           1,372,769.304               1,300,518.288\n",
       "3318      3330           2,048,338.317               1,940,531.037\n",
       "3319      3331           1,894,907.923               1,795,175.927\n",
       "3320      3332           1,418,296.672               1,343,649.478\n",
       "\n",
       "[3321 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "od_logsums_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce2f5e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def effective_density_summary(concept_id, od_logsums_orig, metric_num, sector_column, verbose, filename_extension, filename_verbose):\n",
    "    df_region_ed = od_logsums_orig[['orig_taz', sector_column]]\n",
    "    df_region_ed.columns = ['Origin_zone', 'Value']\n",
    "\n",
    "    df_region_ed['Concept_ID'] = concept_id\n",
    "    df_region_ed['Metric_ID'] = metric_num\n",
    "    df_region_ed['Metric_name'] = 'Effective density'\n",
    "    df_region_ed['Submetric'] = metric_num \n",
    "    df_region_ed['Description'] = 'Effective density for ' + verbose + ' in origin zone'\n",
    "    df_region_ed['Population'] = 'Whole Population'\n",
    "    df_region_ed['Geography'] = 'Region'\n",
    "    df_region_ed['Zone_ID'] = ''\n",
    "    df_region_ed['Income'] = ''\n",
    "    df_region_ed['Mode'] = ''\n",
    "    df_region_ed['Purpose'] = ''\n",
    "    #df_region_ed['Origin_zone'] = '\n",
    "    df_region_ed['Dest_zone'] = ''\n",
    "    df_region_ed['Period'] = ''\n",
    "    df_region_ed['Units'] = ''\n",
    "    df_region_ed['Total_Increment'] = ''\n",
    "    \n",
    "    df_region_ed = df_region_ed[perf_measure_columns]\n",
    "    df_region_ed.to_csv(_join(summary_dir,  metric_num + filename_verbose + \\\n",
    "                              concept_id + '_region' +filename_extension + '.csv'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ebb20eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_density_summary(concept_id , od_logsums_orig, 'E1.5.1', \n",
    "                          'effective_density_good', 'goods producing industries', \n",
    "                          filename_extension, '_effective_density_goods_producing_industries_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2afab13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_density_summary(concept_id , od_logsums_orig, 'E1.5.2', \n",
    "                          'effective_density_services', 'services producing industries', \n",
    "                          filename_extension, '_effective_density_services_producing_industries_')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
