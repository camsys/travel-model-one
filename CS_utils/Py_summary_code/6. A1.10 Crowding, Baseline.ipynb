{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371adeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openmatrix as omx\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "from utility import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f67170c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48e3a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "    \n",
    "_join = os.path.join\n",
    "_dir = os.path.dirname\n",
    "_norm = os.path.normpath\n",
    "\n",
    "# paths\n",
    "model_outputs_dir = params['model_dir']\n",
    "skims_dir = _join(model_outputs_dir, \"skims\")\n",
    "summary_outputs = params['summary_dir']\n",
    "concept_id = params['concept_id']\n",
    "ctramp_dir = params['ctramp_dir']\n",
    "iteration = params['iteration']\n",
    "\n",
    "concept_id = params['concept_id']\n",
    "time_period_mapping = params['time_periods_mapping']\n",
    "link21_purp_mapping = params['purpose_mapping']\n",
    "mode_cat_mapping = params['mode_mapping']\n",
    "time_periods = params['periods']\n",
    "acc_egg_modes = params['access_egress_modes']\n",
    "\n",
    "preprocess_dir = _join(ctramp_dir, '_pre_process_files')\n",
    "perf_measure_columns = params['final_columns']\n",
    "summary_dir = params['summary_dir']\n",
    "\n",
    "demand_matrices_dir = _join(model_outputs_dir, \"demand_matrices\")\n",
    "transit_demand_dir = _join(demand_matrices_dir, \"transit\")\n",
    "transit_skims_dir = _join(skims_dir, \"transit\")\n",
    "best_path_skim_dir = params['best_path_skim_dir']\n",
    "\n",
    "annual_transit_factor = params['annual_transit_factor']\n",
    "annual_auto_factor = params['annual_auto_factor']\n",
    "\n",
    "filename_extension = params['filename_extension']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beca882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips = pd.read_parquet(_join(preprocess_dir, 'trip_roster.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c56401e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\MTC_tmpy\\\\TM2_2050R39_R2_Run4\\\\tm2py\\\\examples\\\\Link21_3332\\\\skims\\\\transit'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transit_skims_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f20d6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = df_trips.loc[df_trips['trip_mode'].isin([6,7,8])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcd2df6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period:  am\n",
      "Access Egress Mode:  WLK_TRN_WLK\n",
      "Access Egress Mode:  KNR_TRN_WLK\n",
      "Access Egress Mode:  PNR_TRN_WLK\n",
      "Access Egress Mode:  WLK_TRN_PNR\n",
      "Access Egress Mode:  WLK_TRN_KNR\n",
      "Period:  md\n",
      "Access Egress Mode:  WLK_TRN_WLK\n",
      "Access Egress Mode:  KNR_TRN_WLK\n",
      "Access Egress Mode:  PNR_TRN_WLK\n",
      "Access Egress Mode:  WLK_TRN_PNR\n",
      "Access Egress Mode:  WLK_TRN_KNR\n",
      "Period:  pm\n",
      "Access Egress Mode:  WLK_TRN_WLK\n",
      "Access Egress Mode:  KNR_TRN_WLK\n",
      "Access Egress Mode:  PNR_TRN_WLK\n",
      "Access Egress Mode:  WLK_TRN_PNR\n",
      "Access Egress Mode:  WLK_TRN_KNR\n",
      "Period:  ev\n",
      "Access Egress Mode:  WLK_TRN_WLK\n",
      "Access Egress Mode:  KNR_TRN_WLK\n",
      "Access Egress Mode:  PNR_TRN_WLK\n",
      "Access Egress Mode:  WLK_TRN_PNR\n",
      "Access Egress Mode:  WLK_TRN_KNR\n",
      "Period:  ea\n",
      "Access Egress Mode:  WLK_TRN_WLK\n",
      "Access Egress Mode:  KNR_TRN_WLK\n",
      "Access Egress Mode:  PNR_TRN_WLK\n",
      "Access Egress Mode:  WLK_TRN_PNR\n",
      "Access Egress Mode:  WLK_TRN_KNR\n"
     ]
    }
   ],
   "source": [
    "create_rail_crowding_od_pairs(preprocess_dir, transit_skims_dir, time_periods, acc_egg_modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2c343b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing - AM\n",
      "Walk, transit, walk.\n",
      "Park-n-ride, transit, walk.\n",
      "Walk, transit, Park-n-ride.\n",
      "Kiss-n-ride, transit, walk.\n",
      "Walk, transit, Kiss-n-ride.\n",
      "Concatinating crowding skims for the connected ODs of the period.\n",
      "Concatinating the three rail trip dataframes of the period.\n",
      "Processing - MD\n",
      "Walk, transit, walk.\n",
      "Park-n-ride, transit, walk.\n",
      "Walk, transit, Park-n-ride.\n",
      "Kiss-n-ride, transit, walk.\n",
      "Walk, transit, Kiss-n-ride.\n",
      "Concatinating crowding skims for the connected ODs of the period.\n",
      "Concatinating the three rail trip dataframes of the period.\n",
      "Processing - PM\n",
      "Walk, transit, walk.\n",
      "Park-n-ride, transit, walk.\n",
      "Walk, transit, Park-n-ride.\n",
      "Kiss-n-ride, transit, walk.\n",
      "Walk, transit, Kiss-n-ride.\n",
      "Concatinating crowding skims for the connected ODs of the period.\n",
      "Concatinating the three rail trip dataframes of the period.\n",
      "Processing - EV\n",
      "Walk, transit, walk.\n",
      "Park-n-ride, transit, walk.\n",
      "Walk, transit, Park-n-ride.\n",
      "Kiss-n-ride, transit, walk.\n",
      "Walk, transit, Kiss-n-ride.\n",
      "Concatinating crowding skims for the connected ODs of the period.\n",
      "Concatinating the three rail trip dataframes of the period.\n",
      "Processing - EA\n",
      "Walk, transit, walk.\n",
      "Park-n-ride, transit, walk.\n",
      "Walk, transit, Park-n-ride.\n",
      "Kiss-n-ride, transit, walk.\n",
      "Walk, transit, Kiss-n-ride.\n",
      "Concatinating crowding skims for the connected ODs of the period.\n",
      "Concatinating the three rail trip dataframes of the period.\n"
     ]
    }
   ],
   "source": [
    "df_connected_cwd_skim = []\n",
    "df_rail_trips = []\n",
    "\n",
    "for period in time_periods:\n",
    "    print(f'Processing - {period.upper()}')\n",
    "    \n",
    "    # Read in which ODs have either nonzero IVTHVY or IVTCOM. The matrix below contains 0s and 1s.\n",
    "    df_od_pr = omx.open_file(_join(preprocess_dir, \"rail_od_v9_trim_\" + period.upper() + \".omx\"))\n",
    "    \n",
    "    # Read in crowding times between ODs (already multiplied by 1.62). Disconnected ODs have a 0 value.\n",
    "    df_od_cwd = omx.open_file(_join(preprocess_dir, \"rail_crowding_od_v9_trim_\" + period.upper() + \".omx\"))\n",
    "    \n",
    "    # Read in transit trips of this period.\n",
    "    df_trn_pd = df_trn[df_trn['Period'] == period]\n",
    "    \n",
    "    # Walk, transit, walk.\n",
    "    print('Walk, transit, walk.')\n",
    "    df_rail_od = skim_core_to_df(df_od_pr, 'WLK_TRN_WLK')\n",
    "    df_rail_od = df_rail_od[df_rail_od['rail_od'] > 0]    \n",
    "    # df_connected_wlk = df_rail_od.copy()\n",
    "    \n",
    "    df_rail_cwd = skim_core_to_df(df_od_cwd, 'WLK_TRN_WLK', cols =['orig', 'dest', 'crowd'])    \n",
    "    df_rail_cwd = df_rail_cwd.merge(df_rail_od, on=['orig', 'dest'], how ='inner')    \n",
    "    df_rail_cwd['period'] = period\n",
    "    df_rail_cwd['mode'] = 'WLK_TRN_WLK'\n",
    "    df_connected_cwd_skim_wlk = df_rail_cwd.copy()\n",
    "    \n",
    "    df_trn_acc = df_trn_pd[df_trn_pd['Mode'] == 'WALK_TRANSIT']\n",
    "    df_trn_wlk = df_trn_acc.merge(df_rail_cwd,\n",
    "                          left_on =['orig_taz', 'dest_taz'], \n",
    "                          right_on=['orig', 'dest'], \n",
    "                          how ='inner')    \n",
    "    # print(df_trn_wlk.columns)\n",
    "    \n",
    "    \n",
    "    # PNR Transit\n",
    "    df_trn_acc = df_trn_pd[df_trn_pd['Mode'] == 'PNR_TRANSIT']\n",
    "    \n",
    "    print('Park-n-ride, transit, walk.')\n",
    "    df_rail_od = skim_core_to_df(df_od_pr, 'PNR_TRN_WLK')\n",
    "    df_rail_od = df_rail_od[df_rail_od['rail_od'] > 0]    \n",
    "    # df_connected_pnr_inbnd = df_rail_od.copy()\n",
    "    \n",
    "    df_rail_cwd = skim_core_to_df(df_od_cwd, 'PNR_TRN_WLK', cols =['orig', 'dest', 'crowd'])\n",
    "    df_rail_cwd = df_rail_cwd.merge(df_rail_od, on=['orig', 'dest'], how ='inner')\n",
    "    df_rail_cwd['period'] = period\n",
    "    df_rail_cwd['mode'] = 'PNR_TRN_WLK'\n",
    "    df_connected_cwd_skim_pnr_inbnd = df_rail_cwd.copy()\n",
    "    \n",
    "    df_trn_acc_inbnd = df_trn_acc[df_trn_acc['inbound'] == 1]  # Going to work\n",
    "    df_trn_pnr_inbnd = df_trn_acc_inbnd.merge(df_rail_cwd, \n",
    "                              left_on =['orig_taz', 'dest_taz'], \n",
    "                              right_on=['orig', 'dest'], how ='inner')    \n",
    "    \n",
    "    print('Walk, transit, Park-n-ride.')\n",
    "    df_rail_od = skim_core_to_df(df_od_pr, 'WLK_TRN_PNR')\n",
    "    df_rail_od = df_rail_od[df_rail_od['rail_od'] > 0]\n",
    "    # df_connected_pnr_outbnd = df_rail_od.copy()\n",
    "    \n",
    "    df_rail_cwd = skim_core_to_df(df_od_cwd, 'WLK_TRN_PNR', cols =['orig', 'dest', 'crowd'])\n",
    "    df_rail_cwd = df_rail_cwd.merge(df_rail_od, on=['orig', 'dest'], how ='inner')\n",
    "    df_rail_cwd['period'] = period\n",
    "    df_rail_cwd['mode'] = 'WLK_TRN_PNR'\n",
    "    df_connected_cwd_skim_pnr_outbnd = df_rail_cwd.copy()\n",
    "    \n",
    "    df_trn_acc_outbnd = df_trn_acc[df_trn_acc['inbound'] != 1]  # Returning home\n",
    "    df_trn_pnr_outbnd = df_trn_acc_outbnd.merge(df_rail_cwd, \n",
    "                              left_on =['orig_taz', 'dest_taz'], \n",
    "                              right_on=['orig', 'dest'], how ='inner')\n",
    "    \n",
    "    df_connected_cwd_skim_pnr = pd.concat([df_connected_cwd_skim_pnr_inbnd, df_connected_cwd_skim_pnr_outbnd],\n",
    "                                          ignore_index=True)\n",
    "    df_trn_pnr = pd.concat([df_trn_pnr_inbnd, df_trn_pnr_outbnd], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # KNR Transit\n",
    "    df_trn_acc = df_trn_pd[df_trn_pd['Mode'] == 'KNR_TRANSIT']\n",
    "    \n",
    "    print('Kiss-n-ride, transit, walk.')\n",
    "    df_rail_od = skim_core_to_df(df_od_pr, 'KNR_TRN_WLK')\n",
    "    df_rail_od = df_rail_od[df_rail_od['rail_od'] > 0]\n",
    "    # df_connected_knr_inbnd = df_rail_od.copy()\n",
    "    \n",
    "    df_rail_cwd = skim_core_to_df(df_od_cwd, 'KNR_TRN_WLK', cols =['orig', 'dest', 'crowd'])\n",
    "    df_rail_cwd = df_rail_cwd.merge(df_rail_od, on=['orig', 'dest'], how ='inner')   \n",
    "    df_rail_cwd['period'] = period\n",
    "    df_rail_cwd['mode'] = 'KNR_TRN_WLK'\n",
    "    df_connected_cwd_skim_knr_inbnd = df_rail_cwd.copy()\n",
    "    \n",
    "    df_trn_acc_inbnd = df_trn_acc[df_trn_acc['inbound'] == 1]  # Going to work\n",
    "    df_trn_knr_inbnd = df_trn_acc_inbnd.merge(df_rail_cwd, \n",
    "                              left_on =['orig_taz', 'dest_taz'], \n",
    "                              right_on=['orig', 'dest'], how ='inner')    \n",
    "    \n",
    "    print('Walk, transit, Kiss-n-ride.')\n",
    "    df_rail_od = skim_core_to_df(df_od_pr, 'WLK_TRN_KNR')\n",
    "    df_rail_od = df_rail_od[df_rail_od['rail_od'] > 0]    \n",
    "    # df_connected_knr_outbnd = df_rail_od.copy()\n",
    "    \n",
    "    df_rail_cwd = skim_core_to_df(df_od_cwd, 'WLK_TRN_KNR', cols =['orig', 'dest', 'crowd'])\n",
    "    df_rail_cwd = df_rail_cwd.merge(df_rail_od, on=['orig', 'dest'], how ='inner')\n",
    "    df_rail_cwd['period'] = period\n",
    "    df_rail_cwd['mode'] = 'WLK_TRN_KNR'\n",
    "    df_connected_cwd_skim_knr_outbnd = df_rail_cwd.copy()\n",
    "    \n",
    "    df_trn_acc_outbnd = df_trn_acc[df_trn_acc['inbound'] != 1]  # Returning home\n",
    "    df_trn_knr_outbnd = df_trn_acc_outbnd.merge(df_rail_cwd, \n",
    "                              left_on =['orig_taz', 'dest_taz'], \n",
    "                              right_on=['orig', 'dest'], how ='inner')\n",
    "    \n",
    "    df_connected_cwd_skim_knr = pd.concat([df_connected_cwd_skim_knr_inbnd, df_connected_cwd_skim_knr_outbnd],\n",
    "                                          ignore_index=True)\n",
    "    df_trn_knr = pd.concat([df_trn_knr_inbnd, df_trn_knr_outbnd], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # Now, concatinate the three connected-by-rail skim dataframes.\n",
    "    print('Concatinating crowding skims for the connected ODs of the period.')\n",
    "    df_period_connected_cwd_skim = pd.concat([df_connected_cwd_skim_wlk,\n",
    "                                              df_connected_cwd_skim_pnr,\n",
    "                                              df_connected_cwd_skim_knr],\n",
    "                                             ignore_index=True)\n",
    "    df_period_connected_cwd_skim.drop(columns=['rail_od'], inplace=True)\n",
    "    df_connected_cwd_skim.append(df_period_connected_cwd_skim)    \n",
    "    \n",
    "    # Now, concatinate the three rail trip dataframes.\n",
    "    print('Concatinating the three rail trip dataframes of the period.')\n",
    "    df_period_trips = pd.concat([df_trn_wlk, df_trn_pnr, df_trn_knr], ignore_index=True)\n",
    "    df_rail_trips.append(df_period_trips)\n",
    "\n",
    "df_connected_cwd_skim = pd.concat(df_connected_cwd_skim)\n",
    "df_trn_rail = pd.concat(df_rail_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19ab8f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig</th>\n",
       "      <th>dest</th>\n",
       "      <th>crowd</th>\n",
       "      <th>period</th>\n",
       "      <th>mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>267</td>\n",
       "      <td>1</td>\n",
       "      <td>125.293007</td>\n",
       "      <td>am</td>\n",
       "      <td>WLK_TRN_WLK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "      <td>125.293007</td>\n",
       "      <td>am</td>\n",
       "      <td>WLK_TRN_WLK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>269</td>\n",
       "      <td>1</td>\n",
       "      <td>125.293007</td>\n",
       "      <td>am</td>\n",
       "      <td>WLK_TRN_WLK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   orig dest       crowd period         mode\n",
       "0   267    1  125.293007     am  WLK_TRN_WLK\n",
       "1   268    1  125.293007     am  WLK_TRN_WLK\n",
       "2   269    1  125.293007     am  WLK_TRN_WLK"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_connected_cwd_skim.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c6e3f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total connected ODs, averaged over all periods and modes: 8,364,900\n"
     ]
    }
   ],
   "source": [
    "# x = int(round(len(df_connected_cwd_skim.loc[df_connected_cwd_skim['mode']==\"WLK_TRN_WLK\"]) / 5 , 0))\n",
    "x = int(round(len(df_connected_cwd_skim) / (5 * 5), 0))\n",
    "# Note: Five TODs and five modes\n",
    "print(f'Total connected ODs, averaged over all periods and modes: {x:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34cad08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average crowding time, including all ODs, periods, and modes, and not weighted by any trip: 3.589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    2.091225e+08\n",
       "mean     3.589442e+02\n",
       "std      6.498959e+02\n",
       "min      0.000000e+00\n",
       "25%      1.290458e+01\n",
       "50%      6.449345e+01\n",
       "75%      4.481069e+02\n",
       "max      9.039180e+03\n",
       "Name: crowd, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: There are ODs connected by rail with zero crowding time because there aren't many passengers between those ODs.\n",
    "x = df_connected_cwd_skim.crowd.describe()\n",
    "print(f\"Average crowding time, including all ODs, periods, and modes, and not weighted by any trip: {x['mean']/100:.3f}\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9785b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save = df_connected_cwd_skim.drop(columns=['crowd'])\n",
    "df_to_save.to_parquet(_join(preprocess_dir, 'A1.10_baseline_connected_cwd_skim.parquet'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dc55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn_rail.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37ea4784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of transit trips: 1,278,062.0\n"
     ]
    }
   ],
   "source": [
    "x = df_trn_rail.trips.sum()\n",
    "print(f'Total number of transit trips: {x:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a2e6de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn_rail['crowd_trips'] = df_trn_rail['trips'] * df_trn_rail['crowd'] / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53822a61",
   "metadata": {},
   "source": [
    "df_trn_rail['pp_trips'] = df_trn_rail['trips'] * df_trn_rail['pp_share'] / 100\n",
    "df_trn_rail['crowd_pp_trips'] = df_trn_rail['pp_trips'] * df_trn_rail['crowd'] / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da704e42",
   "metadata": {},
   "source": [
    "df_temp1 = summarize_all_combinations(df_trn_rail, groupby_columns=['Period', 'Income'], \n",
    "                                      summary_column='crowd_trips')\n",
    "#df_temp1 = df_temp1.rename(columns={'Value':'crowd_trips'})\n",
    "\n",
    "\n",
    "#df_temp2 = summarize_all_combinations(df_trn_rail, groupby_columns=['Period', 'Income'], \n",
    "#                                      summary_column='trips')\n",
    "#df_temp2 = df_temp2.rename(columns={'Value':'trips'})\n",
    "\n",
    "\n",
    "#region_value = pd.merge(df_temp1, df_temp2, on = ['Period', 'Income'], how='left')\n",
    "#region_value['Value'] = region_value['crowd_trips'] / region_value['trips']\n",
    "\n",
    "region_value = df_temp1 \n",
    "region_value = region_value[['Period', 'Income', 'Value']]\n",
    "\n",
    "#regional value\n",
    "#region_value = df_trn_rail.groupby(['Period'])['trips'].mean().reset_index()\n",
    "#region_value = region_value.rename(columns={'trips': 'Value'})\n",
    "region_value['Concept_ID'] = concept_id\n",
    "region_value['Metric_ID'] = 'A1.10'\n",
    "region_value['Metric_name'] = 'Crowding (Region)'\n",
    "region_value['Submetric'] = 'A1.10.1'\n",
    "region_value['Description'] = 'Regional crowding'\n",
    "region_value['Population'] = 'Whole Population'\n",
    "region_value['Geography'] = 'Regional'\n",
    "region_value['Origin_zone'] = ''\n",
    "region_value['Dest_zone'] = ''\n",
    "region_value['Purpose'] = ''\n",
    "region_value['Mode'] = ''\n",
    "#region_value['Income'] = ''\n",
    "region_value['Zone_ID'] = ''\n",
    "region_value['Units'] = 'minutes'\n",
    "region_value['Total_Increment'] = ''\n",
    "region_value = region_value[perf_measure_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa792e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df1.to_csv(_join(summary_dir, \"A1.10.1_regional_crowding_\" + concept_id + '_region' + filename_extension + \".csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99b2ba1",
   "metadata": {},
   "source": [
    "# summarise for prioirty population\n",
    "#regional value\n",
    "\n",
    "df_temp1 = summarize_all_combinations(df_trn_rail, groupby_columns=['Period', 'Income'], \n",
    "                                      summary_column='crowd_pp_trips')\n",
    "#df_temp1 = df_temp1.rename(columns={'Value':'crowd_pp_trips'}) # BC team is interested in total value instead of mean\n",
    "\n",
    "\n",
    "#df_temp2 = summarize_all_combinations(df_trn_rail, groupby_columns=['Period', 'Income'], \n",
    "#                                      summary_column='pp_trips')\n",
    "#df_temp2 = df_temp2.rename(columns={'Value':'pp_trips'})\n",
    "\n",
    "\n",
    "#pp_region_value = pd.merge(df_temp1, df_temp2, on = ['Period', 'Income'], how='left')\n",
    "#pp_region_value['Value'] = pp_region_value['crowd_pp_trips'] / pp_region_value['pp_trips']\n",
    "pp_region_value = df_temp1\n",
    "pp_region_value = pp_region_value[['Period', 'Income', 'Value']]\n",
    "\n",
    "#region_value = df_trn_rail.groupby(['Period'])['pp_trips'].mean().reset_index()\n",
    "#region_value = region_value.rename(columns={'pp_trips': 'Value'})\n",
    "pp_region_value['Concept_ID'] = concept_id\n",
    "pp_region_value['Metric_ID'] = 'A1.10'\n",
    "pp_region_value['Metric_name'] = 'Crowding (Region)'\n",
    "pp_region_value['Submetric'] = 'A1.10.2'\n",
    "pp_region_value['Description'] = 'Rgional crowding'\n",
    "pp_region_value['Population'] = 'Prioirty Population'\n",
    "pp_region_value['Geography'] = 'Regional'\n",
    "pp_region_value['Origin_zone'] = ''\n",
    "pp_region_value['Dest_zone'] = ''\n",
    "pp_region_value['Purpose'] = ''\n",
    "pp_region_value['Mode'] = ''\n",
    "#pp_region_value['Income'] = ''\n",
    "pp_region_value['Zone_ID'] = ''\n",
    "pp_region_value['Units'] = 'minutes'\n",
    "pp_region_value['Total_Increment'] = ''\n",
    "pp_region_value = pp_region_value[perf_measure_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e068a",
   "metadata": {},
   "source": [
    "#df_rdm = df_tours.groupby(['orig_rdm_zones', 'dest_rdm_zones', 'Mode', 'Period', 'tour_purpose'])['tours'].sum().reset_index()\n",
    "\n",
    "df_temp1 = summarize_all_combinations(df_trn_rail, groupby_columns=['orig_rdm_zones', 'dest_rdm_zones', 'Period', 'Income'], \n",
    "                                      summary_column='crowd_trips')\n",
    "#df_temp1 = df_temp1.rename(columns={'Value': 'trips'})\n",
    "\n",
    "#df_temp2 = summarize_all_combinations(df_trn_rail, groupby_columns=['orig_rdm_zones', 'dest_rdm_zones', 'Period', 'Income'], \n",
    "#                                      summary_column='crowd_trips')\n",
    "#df_temp2 = df_temp2.rename(columns={'Value': 'crowd_trips'})\n",
    "\n",
    "\n",
    "#df_rdm = df_temp1.merge(df_temp2, on = ['orig_rdm_zones', 'dest_rdm_zones', 'Period', 'Income'], how = 'left')\n",
    "#df_rdm['Value'] = df_rdm['crowd_trips'] /  df_rdm['trips']\n",
    "\n",
    "df_rdm = df_temp1\n",
    "\n",
    "df_rdm = df_rdm.rename(columns={ \n",
    "                                'orig_rdm_zones' : 'Origin_zone',\n",
    "                                'dest_rdm_zones' : 'Dest_zone'})\n",
    "df_rdm = df_rdm[['Origin_zone', 'Dest_zone', 'Period', 'Income', 'Value']]\n",
    "\n",
    "df_rdm['Concept_ID'] = concept_id\n",
    "df_rdm['Metric_ID'] = 'A1.10'\n",
    "df_rdm['Metric_name'] = 'Crowding (Region)'\n",
    "df_rdm['Submetric'] = 'A1.10.3'\n",
    "df_rdm['Description'] = 'Crowding between RDM zones'\n",
    "df_rdm['Population'] = 'Whole Population'\n",
    "df_rdm['Geography'] = 'RDM'\n",
    "df_rdm['Zone_ID'] = ''\n",
    "df_rdm['Purpose'] = ''\n",
    "df_rdm['Mode'] = ''\n",
    "#df_rdm['Income'] = ''\n",
    "df_rdm['Units'] = 'minutes'\n",
    "df_rdm['Total_Increment'] = ''\n",
    "df_rdm = df_rdm[perf_measure_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9daa4",
   "metadata": {},
   "source": [
    "#df_temp1 = summarize_all_combinations(df_trn_rail, groupby_columns=['orig_rdm_zones', 'dest_rdm_zones', 'Period', 'Income'], \n",
    "#                                      summary_column='pp_trips')\n",
    "#df_temp1 = df_temp1.rename(columns={'Value': 'pp_trips'})\n",
    "\n",
    "df_temp2 = summarize_all_combinations(df_trn_rail, groupby_columns=['orig_rdm_zones', 'dest_rdm_zones', 'Period', 'Income'], \n",
    "                                      summary_column='crowd_pp_trips')\n",
    "#df_temp2 = df_temp2.rename(columns={'Value': 'crowd_pp_trips'})\n",
    "\n",
    "\n",
    "#df_rdm_pp = df_temp1.merge(df_temp2, on = ['orig_rdm_zones', 'dest_rdm_zones', 'Period', 'Income'], how = 'left')\n",
    "\n",
    "df_rdm_pp = df_temp2\n",
    "#df_rdm_pp = df_rdm_pp.loc[df_rdm_pp['pp_trips']>0]\n",
    "\n",
    "#df_rdm_pp['Value'] = df_rdm_pp['crowd_pp_trips'] /  df_rdm_pp['pp_trips']\n",
    "\n",
    "\n",
    "df_rdm_pp = df_rdm_pp.rename(columns={ \n",
    "                                'orig_rdm_zones' : 'Origin_zone',\n",
    "                                'dest_rdm_zones' : 'Dest_zone'})\n",
    "df_rdm_pp = df_rdm_pp[['Origin_zone', 'Dest_zone', 'Period', 'Income' ,'Value']]\n",
    "\n",
    "df_rdm_pp['Concept_ID'] = concept_id\n",
    "df_rdm_pp['Metric_ID'] = 'A1.10'\n",
    "df_rdm_pp['Metric_name'] = 'Crowding (Region)'\n",
    "df_rdm_pp['Submetric'] = 'A1.10.4'\n",
    "df_rdm_pp['Description'] = 'Crowding between RDM zones'\n",
    "df_rdm_pp['Population'] = 'Prioirty Population'\n",
    "df_rdm_pp['Geography'] = 'RDM'\n",
    "df_rdm_pp['Zone_ID'] = ''\n",
    "df_rdm_pp['Purpose'] = ''\n",
    "df_rdm_pp['Mode'] = ''\n",
    "#df_rdm_pp['Income'] = ''\n",
    "df_rdm_pp['Units'] = 'minutes'\n",
    "df_rdm_pp['Total_Increment'] = ''\n",
    "df_rdm_pp = df_rdm_pp[perf_measure_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c3bb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp1 = summarize_all_combinations(df_trn_rail, groupby_columns=['Period'], \n",
    "                                      summary_column='crowd_trips')\n",
    "df_temp1 = df_temp1.rename(columns={'Value':'crowd_trips'})\n",
    "\n",
    "\n",
    "df_temp2 = summarize_all_combinations(df_trn_rail, groupby_columns=['Period'], \n",
    "                                     summary_column='trips')\n",
    "df_temp2 = df_temp2.rename(columns={'Value':'trips'})\n",
    "\n",
    "\n",
    "region_value2 = pd.merge(df_temp1, df_temp2, on = ['Period'], how='left')\n",
    "region_value2['Value'] = region_value2['crowd_trips'] / region_value2['trips']\n",
    "\n",
    "region_value2 = region_value2[['Period', 'Value']]\n",
    "\n",
    "#regional value\n",
    "#region_value = df_trn_rail.groupby(['Period'])['trips'].mean().reset_index()\n",
    "#region_value = region_value.rename(columns={'trips': 'Value'})\n",
    "region_value2['Concept_ID'] = concept_id\n",
    "region_value2['Metric_ID'] = 'A1.10'\n",
    "region_value2['Metric_name'] = 'Crowding (Region)'\n",
    "region_value2['Submetric'] = 'A1.10.5'\n",
    "region_value2['Description'] = 'Regional crowding'\n",
    "region_value2['Population'] = 'Whole Population'\n",
    "region_value2['Geography'] = 'Regional'\n",
    "region_value2['Origin_zone'] = ''\n",
    "region_value2['Dest_zone'] = ''\n",
    "region_value2['Purpose'] = ''\n",
    "region_value2['Mode'] = ''\n",
    "region_value2['Income'] = ''\n",
    "region_value2['Zone_ID'] = ''\n",
    "region_value2['Units'] = 'minutes'\n",
    "region_value2['Total_Increment'] = ''\n",
    "region_value2 = region_value2[perf_measure_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f2b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dfs = [region_value, pp_region_value, df_rdm, df_rdm_pp, region_value2]\n",
    "all_dfs = [region_value2]\n",
    "\n",
    "for dfs in all_dfs:\n",
    "    metric_name = '_regional_crowding_'\n",
    "    dfs = dfs.reset_index(drop=True)\n",
    "    dfs = dfs[perf_measure_columns]\n",
    "    file_name = dfs['Submetric'][0]\n",
    "    geography = '_' + dfs['Geography'][0].replace(' ', '_')\n",
    "    dfs.to_csv(_join(summary_dir, file_name + metric_name + concept_id + geography + filename_extension + '.csv'), index=None)\n",
    "    print(len(dfs), file_name, dfs['Metric_name'][0])\n",
    "    \n",
    "# combined_df = pd.concat([region_value, pp_region_value, df_rdm, df_rdm_pp]).reset_index(drop=True)\n",
    "combined_df = region_value2.reset_index(drop=True)\n",
    "combined_df.to_csv(_join(summary_dir, 'A10.1' + '_regional_crowding_' + concept_id + '_region' +filename_extension+'.csv'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad0aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_value['Value'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f840ba",
   "metadata": {},
   "source": [
    "#county \n",
    "county_df_temp = df_trn_rail.groupby(['orig_county', 'dest_county', 'Period'])['trips'].mean().reset_index()\n",
    "county_df_temp = county_df_temp.rename(columns={'trips': 'Value',\n",
    "                                               'orig_county': 'Origin_zone',\n",
    "                                               'dest_county': 'Dest_zone'})\n",
    "county_df_temp['Concept_ID'] = concept_id\n",
    "county_df_temp['Metric_ID'] = 'A1.9'\n",
    "county_df_temp['Metric_name'] = 'Crowding (Region)'\n",
    "county_df_temp['Submetric'] = 'A1.9.2'\n",
    "county_df_temp['Description'] = 'Crowding level between origin and destination county'\n",
    "county_df_temp['Population'] = 'Whole Population'\n",
    "county_df_temp['Geography'] = 'County'\n",
    "county_df_temp['Zone_ID'] = ''\n",
    "#df_cnty['Origin_zone'] = ''\n",
    "#df_cnty['Dest_zone'] = ''\n",
    "county_df_temp['Units'] = 'minutes'\n",
    "county_df_temp['Total_Increment'] = ''\n",
    "\n",
    "\n",
    "#super district\n",
    "sd_df_temp = df_trn_rail.groupby(['orig_super_dist', 'dest_super_dist', 'Period'])['trips'].mean().reset_index()\n",
    "sd_df_temp = sd_df_temp.rename(columns={'trips': 'Value',\n",
    "                                        'orig_super_dist': 'Origin_zone',\n",
    "                                        'dest_super_dist': 'Dest_zone'})\n",
    "sd_df_temp['Concept_ID'] = concept_id\n",
    "sd_df_temp['Metric_ID'] = 'A1.9'\n",
    "sd_df_temp['Metric_name'] = 'Crowding (Region)'\n",
    "sd_df_temp['Submetric'] = 'A1.9.3'\n",
    "sd_df_temp['Description'] = 'Regional crowding level'\n",
    "sd_df_temp['Population'] = 'Whole Population'\n",
    "sd_df_temp['Geography'] = 'Super district'\n",
    "sd_df_temp['Zone_ID'] = ''\n",
    "#df_cnty['Origin_zone'] = ''\n",
    "#df_cnty['Dest_zone'] = ''\n",
    "sd_df_temp['Units'] = 'minutes'\n",
    "sd_df_temp['Total_Increment'] = ''\n",
    "\n",
    "\n",
    "#RDM Zones\n",
    "rdm_df_temp = df_trn_rail.groupby(['orig_rdm_zones', 'dest_rdm_zones', 'Period'])['trips'].mean().reset_index()\n",
    "rdm_df_temp = rdm_df_temp.rename(columns={'trips': 'Value',\n",
    "                                        'orig_rdm_zones': 'Origin_zone',\n",
    "                                        'dest_rdm_zones': 'Dest_zone'})\n",
    "rdm_df_temp['Concept_ID'] = concept_id\n",
    "rdm_df_temp['Metric_ID'] = 'A1.9'\n",
    "rdm_df_temp['Metric_name'] = 'Crowding (Region)'\n",
    "rdm_df_temp['Submetric'] = 'A1.9.4'\n",
    "rdm_df_temp['Description'] = 'Regional crowding level'\n",
    "rdm_df_temp['Population'] = 'Whole Population'\n",
    "rdm_df_temp['Geography'] = 'RDM'\n",
    "rdm_df_temp['Zone_ID'] = ''\n",
    "#df_cnty['Origin_zone'] = ''\n",
    "#df_cnty['Dest_zone'] = ''\n",
    "rdm_df_temp['Units'] = 'minutes'\n",
    "rdm_df_temp['Total_Increment'] = ''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# transbay region\n",
    "tb_value = df_trn_rail[df_trn_rail['transbay_od']==1]\n",
    "tb_value = tb_value.groupby(['Period'])['trips'].mean().reset_index()\n",
    "\n",
    "tb_value['Concept_ID'] = concept_id\n",
    "tb_value['Metric_ID'] = 'A1.9'\n",
    "tb_value['Metric_name'] = 'Crowding (Region)'\n",
    "tb_value['Submetric'] = 'A1.9.5'\n",
    "tb_value['Description'] = 'regional crowding level'\n",
    "tb_value['Population'] = 'Whole Population'\n",
    "tb_value['Geography'] = 'Transbay'\n",
    "tb_value['Zone_ID'] = ''\n",
    "tb_value['Origin_zone'] = ''\n",
    "tb_value['Dest_zone'] = ''\n",
    "tb_value['Units'] = 'minutes'\n",
    "tb_value['Total_Increment'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf6bef",
   "metadata": {},
   "source": [
    "df_rail_trips = []\n",
    "\n",
    "for period in time_periods:\n",
    "    print(f'processing - {period.upper()}')\n",
    "    \n",
    "    # Read in which ODs have either nonzero IVTHVY or IVTCOM. The matrix below contains 0s and 1s.\n",
    "    df_od_pr = omx.open_file(_join(preprocess_dir, \"rail_od_v9_trim_\" + period.upper() + \".omx\"))\n",
    "    \n",
    "    # Read in crowding times between ODs (already multiplied by 1.62). Disconnected ODs have a 0 value.\n",
    "    df_od_cwd = omx.open_file(_join(preprocess_dir, \"rail_crowding_od_v9_trim_\" + period.upper() + \".omx\"))\n",
    "    \n",
    "    # Read in transit trips of this period.\n",
    "    df_trn_pd = df_trn[df_trn['Period'] == period]\n",
    "    \n",
    "    #walk transit\n",
    "    df_rail_od = skim_core_to_df(df_od_pr, 'WLK_TRN_WLK')\n",
    "    df_rail_od = df_rail_od[df_rail_od['rail_od'] > 0]        \n",
    "    df_rail_cwd = skim_core_to_df(df_od_cwd, 'WLK_TRN_WLK', cols =['orig', 'dest', 'crowd'])\n",
    "    df_rail_cwd = df_rail_cwd.merge(df_rail_od, on=['orig', 'dest'], how ='inner')\n",
    "    \n",
    "    df_trn_acc = df_trn_pd[df_trn_pd['Mode'] == 'WALK_TRANSIT']\n",
    "    df_trn_wlk = pd.merge(df_trn_acc, df_rail_od, \n",
    "                          left_on =['orig_taz', 'dest_taz'], \n",
    "                          right_on=['orig', 'dest'], \n",
    "                          how ='inner')\n",
    "    df_trn_wlk = pd.merge(df_trn_wlk, df_rail_cwd,\n",
    "                          left_on =['orig_taz', 'dest_taz'], \n",
    "                          right_on=['orig', 'dest'], \n",
    "                          how ='inner')\n",
    "    #print(df_trn_wlk.columns)\n",
    "    \n",
    "    # PNR Transit\n",
    "    df_trn_acc = df_trn_pd[df_trn_pd['Mode'] == 'PNR_TRANSIT']\n",
    "    df_trn_acc_inbnd = df_trn_acc[df_trn_acc['inbound'] == 1] # returning home\n",
    "    df_rail_od = skim_core_to_df(df_od_pr, 'WLK_TRN_PNR')\n",
    "    df_rail_cwd = skim_core_to_df(df_od_cwd, 'WLK_TRN_PNR', cols =['orig', 'dest', 'crowd']) # add walk access/egress\n",
    "    df_rail_od = df_rail_od[df_rail_od['rail_od'] > 0]\n",
    "    df_trn_pnr_inb = pd.merge(df_trn_acc_inbnd, df_rail_od, \n",
    "                              left_on =['orig_taz', 'dest_taz'], \n",
    "                              right_on=['orig', 'dest'], how ='inner')\n",
    "    #del df_trn_pnr_inb['orig']\n",
    "    #del df_trn_pnr_inb['dest']\n",
    "    \n",
    "    df_trn_pnr_inb = pd.merge(df_trn_pnr_inb, df_rail_cwd, \n",
    "                              left_on =['orig_taz', 'dest_taz'], \n",
    "                              right_on=['orig', 'dest'], how ='inner')\n",
    "    \n",
    "    #print(df_trn_pnr_inb.columns)\n",
    "    \n",
    "    df_trn_acc_outbnd = df_trn_acc[df_trn_acc['inbound'] != 1] # returning home\n",
    "    df_rail_od = skim_core_to_df(df_od_pr, 'PNR_TRN_WLK')\n",
    "    df_rail_cwd = skim_core_to_df(df_od_cwd, 'PNR_TRN_WLK', cols =['orig', 'dest', 'crowd']) # add walk access/egress\n",
    "    df_rail_od = df_rail_od[df_rail_od['rail_od'] > 0]\n",
    "    df_trn_pnr_outbnd = pd.merge(df_trn_acc_outbnd, df_rail_od, \n",
    "                              left_on =['orig_taz', 'dest_taz'], \n",
    "                              right_on=['orig', 'dest'], how ='inner')\n",
    "    \n",
    "    #del \n",
    "    df_trn_pnr_outbnd = pd.merge(df_trn_pnr_outbnd, df_rail_cwd, \n",
    "                              left_on =['orig_taz', 'dest_taz'], \n",
    "                              right_on=['orig', 'dest'], how ='inner')\n",
    "    \n",
    "    #print(df_trn_pnr_outbnd.columns)\n",
    "\n",
    "    df_trn_pnr = pd.concat([df_trn_pnr_inb, df_trn_pnr_outbnd], ignore_index=True)\n",
    "    \n",
    "    # KNR Transit\n",
    "    df_trn_acc = df_trn_pd[df_trn_pd['Mode'] == 'KNR_TRANSIT']\n",
    "    df_trn_acc_inbnd = df_trn_acc[df_trn_acc['inbound'] == 1] # returning home\n",
    "    df_rail_cwd = skim_core_to_df(df_od_cwd, 'WLK_TRN_KNR', cols =['orig', 'dest', 'crowd']) # add walk access/egress\n",
    "    df_rail_od = skim_core_to_df(df_od_pr, 'WLK_TRN_KNR')\n",
    "    df_rail_od = df_rail_od[df_rail_od['rail_od'] > 0]\n",
    "    df_trn_knr_inb = pd.merge(df_trn_acc_inbnd, df_rail_od, \n",
    "                              left_on =['orig_taz', 'dest_taz'], \n",
    "                              right_on=['orig', 'dest'], how ='inner')\n",
    "    df_trn_knr_inb = pd.merge(df_trn_knr_inb, df_rail_cwd, \n",
    "                              left_on =['orig_taz', 'dest_taz'], \n",
    "                              right_on=['orig', 'dest'], how ='inner')\n",
    "\n",
    "\n",
    "    df_trn_acc_outbnd = df_trn_acc[df_trn_acc['inbound'] != 1] # returning home\n",
    "    df_rail_od = skim_core_to_df(df_od_pr, 'KNR_TRN_WLK')\n",
    "    df_rail_cwd = skim_core_to_df(df_od_cwd, 'KNR_TRN_WLK', cols =['orig', 'dest', 'crowd']) # add walk access/egress\n",
    "    df_rail_od = df_rail_od[df_rail_od['rail_od'] > 0]\n",
    "    df_trn_knr_outbnd = pd.merge(df_trn_acc_outbnd, df_rail_od, \n",
    "                              left_on =['orig_taz', 'dest_taz'], \n",
    "                              right_on=['orig', 'dest'], how ='inner')\n",
    "    df_trn_knr_outbnd = pd.merge(df_trn_knr_outbnd, df_rail_cwd, \n",
    "                              left_on =['orig_taz', 'dest_taz'], \n",
    "                              right_on=['orig', 'dest'], how ='inner')\n",
    "\n",
    "    df_trn_knr = pd.concat([df_trn_knr_inb, df_trn_knr_outbnd], ignore_index=True)\n",
    "    \n",
    "    df_trn_rail = pd.concat([df_trn_wlk, df_trn_pnr, df_trn_knr], ignore_index=True)\n",
    "    df_rail_trips.append(df_trn_rail)\n",
    "\n",
    "df_trn_rail = pd.concat(df_rail_trips)\n",
    "print(df_trn_rail.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b356f54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing - AM\n",
      "Processing - MD\n",
      "Processing - PM\n",
      "Processing - EV\n",
      "Processing - EA\n"
     ]
    }
   ],
   "source": [
    "df_connected_cwd_skim = []\n",
    "df_all_trips = []\n",
    "\n",
    "for period in time_periods:\n",
    "    print(f'Processing - {period.upper()}')\n",
    "    \n",
    "    # Read in which ODs have either nonzero IVTHVY or IVTCOM. The matrix below contains 0s and 1s.\n",
    "    df_od_pr = omx.open_file(_join(preprocess_dir, \"rail_od_v9_trim_\" + period.upper() + \".omx\"))\n",
    "    \n",
    "    # Read in crowding times between ODs (already multiplied by 1.62). Disconnected ODs have a 0 value.\n",
    "    df_od_cwd = omx.open_file(_join(preprocess_dir, \"rail_crowding_od_v9_trim_\" + period.upper() + \".omx\"))\n",
    "    \n",
    "    # Read in transit trips of this period.\n",
    "    df_trips_pd = df_trips[df_trips['Period'] == period]\n",
    "    \n",
    "    # Only for walk, transit, walk.    \n",
    "    df_rail_od = skim_core_to_df(df_od_pr, 'WLK_TRN_WLK')\n",
    "    df_rail_od = df_rail_od[df_rail_od['rail_od'] > 0]        \n",
    "    \n",
    "    df_rail_cwd = skim_core_to_df(df_od_cwd, 'WLK_TRN_WLK', cols =['orig', 'dest', 'crowd'])    \n",
    "    df_rail_cwd = df_rail_cwd.merge(df_rail_od, on=['orig', 'dest'], how ='inner')    \n",
    "    df_rail_cwd['period'] = period\n",
    "    df_connected_cwd_skim_wlk = df_rail_cwd.copy()    \n",
    "\n",
    "    df_trips_wlk = df_trips_pd.merge(df_rail_cwd,\n",
    "                          left_on =['orig_taz', 'dest_taz'], \n",
    "                          right_on=['orig', 'dest'], \n",
    "                          how ='inner')        \n",
    "  \n",
    "    \n",
    "    df_connected_cwd_skim_wlk.drop(columns=['rail_od'], inplace=True)\n",
    "    df_connected_cwd_skim.append(df_connected_cwd_skim_wlk)    \n",
    "    \n",
    "    df_all_trips.append(df_trips_wlk)\n",
    "\n",
    "df_connected_cwd_skim = pd.concat(df_connected_cwd_skim)\n",
    "df_all_trips = pd.concat(df_all_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a811c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_trips.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e388951b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trips: 5,258,505.0\n"
     ]
    }
   ],
   "source": [
    "x = df_all_trips.trips.sum()\n",
    "print(f'Total number of trips: {x:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_trips['crowd_trips'] = df_all_trips['trips'] * df_all_trips['crowd'] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c19853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp1 = summarize_all_combinations(df_all_trips, groupby_columns=['Period'], \n",
    "                                      summary_column='crowd_trips')\n",
    "df_temp1 = df_temp1.rename(columns={'Value':'crowd_trips'})\n",
    "\n",
    "\n",
    "df_temp2 = summarize_all_combinations(df_all_trips, groupby_columns=['Period'], \n",
    "                                     summary_column='trips')\n",
    "df_temp2 = df_temp2.rename(columns={'Value':'trips'})\n",
    "\n",
    "\n",
    "region_value2 = pd.merge(df_temp1, df_temp2, on = ['Period'], how='left')\n",
    "region_value2['Value'] = region_value2['crowd_trips'] / region_value2['trips']\n",
    "\n",
    "region_value2 = region_value2[['Period', 'Value']]\n",
    "\n",
    "region_value2['Concept_ID'] = concept_id\n",
    "region_value2['Metric_ID'] = 'A1.10'\n",
    "region_value2['Metric_name'] = 'Crowding (Region)'\n",
    "region_value2['Submetric'] = 'A1.10.5'\n",
    "region_value2['Description'] = 'Regional crowding'\n",
    "region_value2['Population'] = 'Whole Population'\n",
    "region_value2['Geography'] = 'Regional'\n",
    "region_value2['Origin_zone'] = ''\n",
    "region_value2['Dest_zone'] = ''\n",
    "region_value2['Purpose'] = ''\n",
    "region_value2['Mode'] = ''\n",
    "region_value2['Income'] = ''\n",
    "region_value2['Zone_ID'] = ''\n",
    "region_value2['Units'] = 'minutes'\n",
    "region_value2['Total_Increment'] = ''\n",
    "region_value2 = region_value2[perf_measure_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6143010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dfs = [region_value, pp_region_value, df_rdm, df_rdm_pp, region_value2]\n",
    "all_dfs = [region_value2]\n",
    "\n",
    "for dfs in all_dfs:\n",
    "    metric_name = '_regional_crowding_'\n",
    "    dfs = dfs.reset_index(drop=True)\n",
    "    dfs = dfs[perf_measure_columns]\n",
    "    file_name = dfs['Submetric'][0]\n",
    "    geography = '_' + dfs['Geography'][0].replace(' ', '_')\n",
    "    dfs.to_csv(_join(summary_dir, file_name + metric_name + concept_id + geography + filename_extension + '.csv'), index=None)\n",
    "    print(len(dfs), file_name, dfs['Metric_name'][0])\n",
    "    \n",
    "# combined_df = pd.concat([region_value, pp_region_value, df_rdm, df_rdm_pp]).reset_index(drop=True)\n",
    "combined_df = region_value2.reset_index(drop=True)\n",
    "combined_df.to_csv(_join(summary_dir, 'A10.1' + '_regional_crowding_' + concept_id + '_region' +filename_extension+'_all_trips.csv'), index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
