{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee4554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmatrix as omx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from utility import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7689f-a667-4cfb-a2fc-88d8b851e3fe",
   "metadata": {},
   "source": [
    "#### Details\n",
    "Jobs accessible from people's homes : \n",
    "    count of jobs accessible to each zone within isochrone thresholds (both actual and perceived). Weighted average by workforce in origin zone.\n",
    "    PP metric: yes\n",
    "    Multi or Single path : Single\n",
    "    Modes : rail inclusive (Does this mean only rail OD pairs (IVTHWY + IVTCOM > 0) ?)\n",
    "    Purpose split: No\n",
    "    Period splits: Yes, average weekday and annual\n",
    "    Geography: region, zone origin and destination (RDM, Super district, county) : Just the origin zone ?   \n",
    "    \n",
    "Non-work destinations accessible from people's homes\n",
    "    count of Non-work-destinations accessible to each zone within isochrone thresholds (both actual and perceived). Weighted average by population in origin zone.\n",
    "    PP metric: yes\n",
    "    Multi or Single path : Single\n",
    "    Modes : rail inclusive (Does this mean only rail OD pairs (IVTHWY + IVTCOM > 0) ?)\n",
    "    Purpose split: No\n",
    "    Period splits: Yes, average weekday and annual\n",
    "    Geography: region, zone origin and destination (RDM, Super district, county) : Just the origin zone ?       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "    \n",
    "_join = os.path.join\n",
    "_dir = os.path.dirname\n",
    "_norm = os.path.normpath\n",
    "\n",
    "# paths\n",
    "model_outputs_dir = params['model_dir']\n",
    "\n",
    "skims_dir = _join(model_outputs_dir, \"skims\")\n",
    "#landuse_dir = _join(model_outputs_dir, params['zone_file'])\n",
    "\n",
    "#hwyskmMD = _join(params['best_path_skim_dir'], 'am_KNR_TRN_WLK_v9_1_release11302022_bestpathresults.omx')\n",
    "\n",
    "#transit skims - names\n",
    "#acc_egr = params['access_egress_modes']\n",
    "#transit_skim_files = []\n",
    "#for per in params['periods']:\n",
    "#    for acc in acc_egr:\n",
    "#        file_name = _join(params['best_path_skim_dir'], per+'_'+acc+'_v9_1_release11302022_bestpathresults.omx')\n",
    "#        transit_skim_files.append(file_name)\n",
    "\n",
    "#transit_skim_files = [r'C:\\VY-Projects\\Link21\\BaseYear2015\\Best Single Path\\am_KNR_TRN_WLK_v9_1_release11302022_bestpathresults.omx']\n",
    "#cores - 'BOARDS', 'DDIST', 'DTIME', 'FARE', 'IVT', 'IVTCOM', 'IVTEXP', 'IVTFRY', 'IVTHVY', 'IVTLOC', 'IVTLRT',\n",
    "# 'IWAIT', 'PIVTCOM', 'PIVTEXP', 'PIVTFRY', 'PIVTHVY', 'PIVTLOC', 'PIVTLRT', 'WACC', 'WAIT', 'WAUX', 'WEGR', 'XWAIT'\n",
    "\n",
    "ctramp_dir = params['ctramp_dir']\n",
    "actual_tt_cores = params['total_travel_time']\n",
    "perceived_tt_cores = params['perceived_travel_time']\n",
    "time_periods = params['periods']\n",
    "acc_egr = params['access_egress_modes']\n",
    "\n",
    "concept_id = params['concept_id']\n",
    "summary_columns = params['final_columns']\n",
    "\n",
    "preprocess_dir = _join(ctramp_dir, '_pre_process_files')\n",
    "perf_measure_columns = params['final_columns']\n",
    "period = params['periods']\n",
    "summary_dir = params['summary_dir']\n",
    "\n",
    "best_path_skim_extension = params['best_path_skim_extension']\n",
    "\n",
    "annual_transit_factor = params['annual_transit_factor']\n",
    "annual_auto_factor = params['annual_auto_factor']\n",
    "\n",
    "filename_extension = params['filename_extension']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4584e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data\n",
    "\n",
    "#taz to RDM zones, super districts, county\n",
    "geo_cwks = pd.read_csv(_join(params['common_dir'], \"geographies.csv\")) #columns taz, rdm_zones, super_district, county\n",
    "\n",
    "#taz to priority population\n",
    "pp_perc = pd.read_excel(_join(params['common_dir'], \"TAZ_Tract_cwk_summary.xlsx\")) #columns = taz, pp_share \n",
    "\n",
    "#get geogrpahies and priority population in the same file\n",
    "geo_pp_cwks = pd.merge(geo_cwks, pp_perc, on = 'taz', how = 'left') \n",
    "\n",
    "#transbay od pairs\n",
    "transbay_od = pd.read_csv(_join(params['common_dir'], \"transbay_od.csv\")) #columns = transbay_o, transbay_d\n",
    "\n",
    "#non work destinations tazs\n",
    "non_work_tazs = pd.read_excel(_join(params['common_dir'], 'non_work_destinations.xlsx'))\n",
    "#non_work_tazs = list(non_work_tazs['non_wrk_taz'])\n",
    "\n",
    "#taz data for empoyments and resindent employments\n",
    "tazData = pd.read_csv(_join(params['model_dir'], params['zone_file']))\n",
    "tazDataTotemp = tazData[[\"ZONE\", \"TOTEMP\", \"RETEMPN\", \"FPSEMPN\", \"HEREMPN\", \"OTHEMPN\", \"AGREMPN\", \"MWTEMPN\"]]\n",
    "tazDataEmpres = tazData[[\"ZONE\", \"EMPRES\"]]\n",
    "tazDataPop = tazData[[\"ZONE\", \"TOTPOP\"]]\n",
    "\n",
    "#mat_core = params['connectivity_mat_core']\n",
    "time_thresholds = params['accessibility_thresholds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90f0bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data = pd.read_csv(_join(params['common_dir'], 'employments_link21_15categories.csv'))\n",
    "emp_data = emp_data[['TAZ', 'link21', 'jobs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa37050",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data = pd.pivot_table(emp_data, values='jobs', index=['TAZ'],\n",
    "                       columns=['link21']).fillna(0)\n",
    "emp_data['TOTEMP']= emp_data.sum(axis=1)\n",
    "emp_data=emp_data.reset_index()\n",
    "emp_data=emp_data.rename(columns={'TAZ':'ZONE'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ce88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tazDataTotemp = emp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281fdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = params['iteration']\n",
    "household_model_dir = _join(model_outputs_dir, \"main\")\n",
    "\n",
    "# input household and person data\n",
    "household_file = _join(ctramp_dir, 'main\\\\householdData_' + str(iteration) + '.csv')\n",
    "\n",
    "hh = pd.read_csv(household_file, usecols = ['hh_id', 'taz', 'income'])\n",
    "hh = hh.rename(columns = {'taz': 'home_zone'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cbaa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hh = assign_income_categories(hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9131502",
   "metadata": {},
   "outputs": [],
   "source": [
    "hh = hh.groupby(['home_zone', 'income_bin'])['hh_id'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a31688",
   "metadata": {},
   "outputs": [],
   "source": [
    "hh = hh[['home_zone', 'income_bin']]\n",
    "hh = hh.rename(columns={'income_bin': 'Income'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e8ff43-e803-4a22-acf3-3d1f45dfa6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ck = omx.open_file(transit_skim_files[0])\n",
    "#ck.list_matrices()\n",
    "\n",
    "all_tod_tt = pd.read_parquet(_join(preprocess_dir, 'tod_min_actual_travel_time.parquet'))\n",
    "perc_tod_tt = pd.read_parquet(_join(preprocess_dir, 'tod_min_perceived_travel_time.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05497cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_categories = {'TOTEMP': 'Total employment',\n",
    "                         'CONS': 'Construction employment', \n",
    "                         'FFRE': 'Farming, Forestry, Resource Extraction', \n",
    "                         'FIRE': 'Finance, Insurance, Real Estate, Professional Services employment', \n",
    "                         'GOVT': 'Public Administration/Government employment',\n",
    "                         'HIED': 'Higher Education and Educational Services employment',\n",
    "                         'HMED': 'Health and Medical Services employment',\n",
    "                         'K12E': 'K12 Education employment',\n",
    "                         'MFRG': 'Manufacturing employment',\n",
    "                         'PERS': 'Personal and Repair Services employment',\n",
    "                         'RECS': 'Restaurants, Hotels/Motels, Recreation Services employment',\n",
    "                         'RETL': 'Local Serving Retail (*NAICS identical to TM2.1) employment',\n",
    "                         'RETR': 'Regional Retail (*NAICS identical to TM2.1) employment',\n",
    "                         'SOCS': 'Social Services employment',\n",
    "                         'UTIL': 'Utilities employment',\n",
    "                         'WTWT': 'Wholesale Trade, Warehousing, Transportation employment'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59723ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for emp_cat in employment_categories:\n",
    "    print(emp_cat, employment_categories[emp_cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9130d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accessibility_jobs(all_tod_tt, time_periods, time_thresholds, geo_pp_cwks, tt_verbose):\n",
    "\n",
    "    acc_jobs = []\n",
    "    acc_jobs_nwd = []\n",
    "    df = []\n",
    "    acc_jobs_pp = []\n",
    "    acc_jobs_rdm = []\n",
    "    acc_jobs_sd = []\n",
    "    acc_jobs_cnty = []\n",
    "    \n",
    "    for period in time_periods:\n",
    "        \n",
    "            \n",
    "        for emp_cat in employment_categories:\n",
    "            print(period, emp_cat)\n",
    "            emp = employment_categories[emp_cat]\n",
    "\n",
    "            timedaData = all_tod_tt[[period]].reset_index()\n",
    "            #print(timedaData.columns)\n",
    "            timedaData = timedaData.fillna(0)\n",
    "            timedaData[period] = timedaData[period]/100\n",
    "\n",
    "            #total employment based on destination\n",
    "            od_data = pd.merge(timedaData, tazDataTotemp, left_on='dest', right_on='ZONE', how = 'left')\n",
    "            #print(od_data.columns)\n",
    "            od_data = pd.merge(od_data, hh, left_on='orig', right_on='home_zone', how = 'left')\n",
    "            \n",
    "            #for time thresholds - currently set in config files\n",
    "            for threshold in time_thresholds:\n",
    "                # print(f'processing {transit_file} for time threshold {threshold}')\n",
    "                # create data for all destinations\n",
    "                oddata_min = od_data.loc[od_data[period] <= threshold]\n",
    "                oddata_min = oddata_min.groupby(['orig', 'Income'])[emp_cat].sum().reset_index()\n",
    "\n",
    "                # adding employed residents\n",
    "                oddata_min = pd.merge(oddata_min, tazDataEmpres, left_on= ['orig'], right_on =['ZONE'], how ='left')\n",
    "\n",
    "                # regional \n",
    "                oddata_min['wt_empres'] = oddata_min[emp_cat] * oddata_min['EMPRES']\n",
    "                \n",
    "                df_temp_region = oddata_min.groupby(['Income'])['wt_empres','EMPRES'].sum().reset_index()\n",
    "                df_temp_region['Value'] =  df_temp_region['wt_empres']/df_temp_region['EMPRES']\n",
    "                df_temp_region['Description'] = f\"{emp} accessible within \" + tt_verbose + \" \" + str(threshold) + \" minutes\"\n",
    "                df_temp_region['Period'] = period\n",
    "                df_temp_region['Geography'] = 'Regional'\n",
    "                df_temp_region['Population'] = 'Whole Population'\n",
    "                df_temp_region['Zone_ID'] = 'Region'\n",
    "                df_temp_region = df_temp_region[['Description', 'Population', 'Period', 'Income',\n",
    "                                                 'Geography', 'Zone_ID', 'Value']]\n",
    "                df_temp_region['Submetric'] = 'B1.1.1'\n",
    "                df_temp_region['Total_Increment'] = ''\n",
    "                \n",
    "                df.append(df_temp_region)                \n",
    "\n",
    "                # adding priority population and geographies\n",
    "                oddata_min = pd.merge(oddata_min, geo_pp_cwks, left_on= ['orig'], right_on =['taz'], how ='left')\n",
    "\n",
    "                 # for county\n",
    "                #if 'county' in geo_pp_cwks.columns:\n",
    "                oddata_min_cnty = oddata_min.groupby(['county'])['wt_empres','EMPRES'].sum().reset_index()\n",
    "                oddata_min_cnty['Value'] =  oddata_min_cnty['wt_empres']/oddata_min_cnty['EMPRES']\n",
    "                oddata_min_cnty['Description'] = f\"{emp} accessible within \" + tt_verbose + \" \" + str(threshold) + \" minutes\"\n",
    "                oddata_min_cnty['Period'] = period\n",
    "                oddata_min_cnty['Geography'] = 'County'\n",
    "                oddata_min_cnty['Population'] = 'Whole Population'\n",
    "                oddata_min_cnty = oddata_min_cnty.rename(columns={'county': 'Zone_ID'})\n",
    "                oddata_min_cnty = oddata_min_cnty[['Description', 'Population', 'Period',\n",
    "                                                 'Geography', 'Zone_ID', 'Value']]   \n",
    "                oddata_min_cnty['Submetric'] = 'B1.1.2'\n",
    "                oddata_min_cnty['Income'] = ''\n",
    "                oddata_min_cnty['Total_Increment'] = ''\n",
    "                acc_jobs_cnty.append(oddata_min_cnty)\n",
    "\n",
    "                # for RDM zones\n",
    "                #if 'rdm_zones' in geo_pp_cwks.columns:\n",
    "                oddata_min_rdm = oddata_min.groupby(['rdm_zones', 'Income'])['wt_empres','EMPRES'].sum().reset_index()\n",
    "                oddata_min_rdm['Value'] =  oddata_min_rdm['wt_empres']/oddata_min_rdm['EMPRES']\n",
    "                oddata_min_rdm['Description'] = f\"{emp} accessible within \" + tt_verbose + \" \" + str(threshold) + \" minutes\"\n",
    "                oddata_min_rdm['Period'] = period\n",
    "                oddata_min_rdm['Geography'] = 'RDM'\n",
    "                oddata_min_rdm['Population'] = 'Whole Population'\n",
    "                oddata_min_rdm = oddata_min_rdm.rename(columns={'rdm_zones' : 'Zone_ID'})\n",
    "                oddata_min_rdm = oddata_min_rdm[['Description', 'Population', 'Period','Income',\n",
    "                                                 'Geography', 'Zone_ID', 'Value']]\n",
    "                oddata_min_rdm['Submetric'] = 'B1.1.3'\n",
    "                oddata_min_rdm['Total_Increment'] = ''\n",
    "                acc_jobs_rdm.append(oddata_min_rdm)\n",
    "\n",
    "                # for superdistrict\n",
    "                #if 'super_district' in geo_pp_cwks.columns:\n",
    "                oddata_min_sd = oddata_min.groupby(['super_district'])['wt_empres','EMPRES'].sum().reset_index()\n",
    "                oddata_min_sd['Value'] =  oddata_min_sd['wt_empres']/oddata_min_sd['EMPRES']\n",
    "                oddata_min_sd['Description'] = f\"{emp} accessible within \" + tt_verbose + \" \" + str(threshold) + \" minutes\"\n",
    "                oddata_min_sd['Period'] = period\n",
    "                oddata_min_sd['Population'] = 'Whole Population'\n",
    "                oddata_min_sd['Geography'] = 'Super district'\n",
    "                oddata_min_sd.rename(columns={'super_district': 'Zone_ID'}, inplace=True)\n",
    "                oddata_min_sd = oddata_min_sd[['Description', 'Population', 'Period',\n",
    "                                                 'Geography', 'Zone_ID', 'Value']]\n",
    "                oddata_min_sd['Income'] = ''\n",
    "                oddata_min_sd['Submetric'] = 'B1.1.4'\n",
    "                oddata_min_sd['Total_Increment'] = ''\n",
    "                acc_jobs_sd.append(oddata_min_sd)\n",
    "\n",
    "                # for prioirty population\n",
    "                #if 'pp_share' in geo_pp_cwks.columns:\n",
    "                oddata_min['priority_population'] = oddata_min['pp_share'].apply(lambda x: 1 if x > 0 else 0)\n",
    "                oddata_min['wt_empres_pp'] = oddata_min[emp_cat] * oddata_min['EMPRES'] * oddata_min['pp_share']/100\n",
    "                oddata_min['EMPRES_pp'] = oddata_min['EMPRES'] * oddata_min['pp_share']/100\n",
    "                oddata_min_pp = oddata_min[oddata_min['priority_population']==1]\n",
    "                oddata_min_nonpp = oddata_min[oddata_min['priority_population']==0]\n",
    "                oddata_min_pp = oddata_min_pp.groupby(['priority_population'])['wt_empres_pp','EMPRES_pp'].sum().reset_index()\n",
    "                oddata_min_pp['Value'] = oddata_min_pp['wt_empres_pp']/oddata_min_pp['EMPRES_pp']\n",
    "                oddata_min_pp = oddata_min_pp[['priority_population', 'Value']]\n",
    "\n",
    "                oddata_min_nonpp = oddata_min_nonpp.groupby(['priority_population'])['wt_empres','EMPRES'].sum().reset_index()\n",
    "                oddata_min_nonpp['Value'] = oddata_min_nonpp['wt_empres']/oddata_min_nonpp['EMPRES']\n",
    "                oddata_min_nonpp = oddata_min_nonpp[['priority_population', 'Value']]\n",
    "\n",
    "                oddata_min_comb = pd.concat([oddata_min_pp, oddata_min_nonpp], ignore_index=False)\n",
    "                oddata_min_comb = oddata_min_comb.loc[oddata_min_comb['priority_population'] == 1]\n",
    "                oddata_min_comb = oddata_min_comb.drop(columns=['priority_population'])\n",
    "                oddata_min_comb['Description'] = f\"{emp} accessible within \" + tt_verbose + \" \" + str(threshold) + \" minutes\"\n",
    "                oddata_min_comb['Period'] = period             \n",
    "                oddata_min_comb['Population'] = 'Priority population'\n",
    "                oddata_min_comb['Geography'] = 'Region'\n",
    "                oddata_min_comb['Zone_ID'] = ''\n",
    "                oddata_min_comb = oddata_min_comb[['Description', 'Population', 'Period', \n",
    "                                                 'Geography', 'Zone_ID', 'Value']]\n",
    "                oddata_min_comb['Income'] = ''\n",
    "                oddata_min_comb['Submetric'] = 'B1.1.5'\n",
    "                oddata_min_comb['Total_Increment'] = ''\n",
    "                acc_jobs_pp.append(oddata_min_comb)\n",
    "                \n",
    "                #break\n",
    "            #break\n",
    "        #break\n",
    "            \n",
    "    df_region = pd.concat(df).reset_index(drop=True)\n",
    "    df_pp = pd.concat(acc_jobs_pp).reset_index(drop=True)\n",
    "    df_rdm = pd.concat(acc_jobs_rdm).reset_index(drop=True)\n",
    "    df_sd = pd.concat(acc_jobs_sd).reset_index(drop=True)\n",
    "    df_cnty = pd.concat(acc_jobs_cnty).reset_index(drop=True)\n",
    "    \n",
    "    return df_region, df_pp, df_rdm, df_sd, df_cnty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c73b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_region_act, df_pp_act, df_rdm_act, df_sd_act, df_cnty_act = get_accessibility_jobs(all_tod_tt, time_periods, time_thresholds, \n",
    "                                                                  geo_pp_cwks, \"actual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5adf9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_region_perc, df_pp_perc, df_rdm_perc, df_sd_perc, df_cnty_perc = get_accessibility_jobs(perc_tod_tt, time_periods, time_thresholds, \n",
    "                                                                  geo_pp_cwks, \"perceived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd88b3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decimals = 0  \n",
    "#df['column'] = df['column'].apply(lambda x: round(x, decimals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef40d3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df_act = pd.concat([df_region_act, df_pp_act, df_rdm_act, df_sd_act, df_cnty_act], ignore_index=True)\n",
    "comb_df_act['Concept_ID'] = concept_id\n",
    "comb_df_act['Metric_ID'] = 'B1.1'\n",
    "comb_df_act['Metric_name'] = 'Jobs accessible from peoples homes - actual travel time'\n",
    "comb_df_act['Origin_zone'] = ''\n",
    "comb_df_act['Dest_zone'] = ''\n",
    "comb_df_act['Purpose'] = ''\n",
    "comb_df_act['Mode'] = ''\n",
    "#comb_df_act['Income'] = ''\n",
    "comb_df_act['Units'] = 'Jobs'\n",
    "comb_df_act['Value'] = comb_df_act['Value'].apply(lambda x: round(x, decimals))\n",
    "comb_df_act = comb_df_act[summary_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbca21c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_medtric_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab55de",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_medtric_ids = comb_df_act['Submetric'].unique()\n",
    "for mids in unique_medtric_ids:\n",
    "    metric_name = '_jobs_accessible_from_home_actual_time_'\n",
    "    dfs = comb_df_act.loc[comb_df_act['Submetric']==mids]\n",
    "    dfs = dfs.reset_index(drop=True)\n",
    "    dfs = dfs[perf_measure_columns]\n",
    "    file_name = mids\n",
    "    geography = '_' + dfs['Geography'][0].replace(' ', '_')\n",
    "    dfs.to_csv(_join(summary_dir, file_name + metric_name + concept_id + geography + filename_extension + '.csv'), index=None)\n",
    "    print(len(dfs), file_name, dfs['Value'].sum())\n",
    "\n",
    "comb_df_act.to_csv(_join(summary_dir, 'B1.1'+'_jobs_accessible_from_home_actual_time_'  + concept_id + '_region' +filename_extension + '.csv'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b4b640",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df_perc = pd.concat([df_region_perc, df_pp_perc, df_rdm_perc, df_sd_perc, df_cnty_perc], ignore_index=True)\n",
    "comb_df_perc['Concept_ID'] = concept_id\n",
    "comb_df_perc['Metric_ID'] = 'B1.1'\n",
    "comb_df_perc['Metric_name'] = 'Jobs accessible from peoples homes - perceived travel time'\n",
    "comb_df_perc['Origin_zone'] = ''\n",
    "comb_df_perc['Dest_zone'] = ''\n",
    "comb_df_perc['Purpose'] = ''\n",
    "comb_df_perc['Mode'] = ''\n",
    "#comb_df_perc['Income'] = ''\n",
    "comb_df_perc['Units'] = 'Jobs'\n",
    "comb_df_perc['Value'] = comb_df_perc['Value'].apply(lambda x: round(x, decimals))\n",
    "comb_df_perc = comb_df[summary_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcb1da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_medtric_ids = comb_df_perc['Submetric'].unique()\n",
    "for mids in unique_medtric_ids:\n",
    "    metric_name = '_jobs_accessible_from_home_perceived_time_'\n",
    "    dfs = comb_df_perc.loc[comb_df_perc['Submetric']==mids]\n",
    "    dfs = dfs.reset_index(drop=True)\n",
    "    dfs = dfs[perf_measure_columns]\n",
    "    file_name = mids\n",
    "    geography = '_' + dfs['Geography'][0].replace(' ', '_')\n",
    "    dfs.to_csv(_join(summary_dir, file_name + metric_name + concept_id + geography + filename_extension + '.csv'), index=None)\n",
    "    print(len(dfs), file_name, dfs['Value'].sum())\n",
    "\n",
    "comb_df_perc.to_csv(_join(summary_dir, 'B1.1' + metric_name + concept_id + '_region' + filename_extension + '.csv'), index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c69b8c",
   "metadata": {},
   "source": [
    "def get_non_work_destionations(all_tod_tt, time_periods, time_thresholds, geo_pp_cwks, non_work_dest_tazs, tt_verbose):\n",
    "\n",
    "    acc_jobs = []\n",
    "    df = []\n",
    "    nwd_acc_min = []\n",
    "    nwd_jobs_pp = []\n",
    "    nwd_jobs_rdm = []\n",
    "    nwd_jobs_sd = []\n",
    "    nwd_jobs_cnty = []\n",
    "    \n",
    "    for period in time_periods:\n",
    "        \n",
    "        if period in all_tod_tt.columns:\n",
    "\n",
    "            timedaData = all_tod_tt[[period]].reset_index()\n",
    "            print(timedaData.columns)\n",
    "            timedaData = timedaData.fillna(0)\n",
    "            timedaData = timedaData.loc[timedaData[period]>0]\n",
    "            timedaData[period] = timedaData[period]/100\n",
    "\n",
    "            #total non work destinations based on destination\n",
    "            od_data = pd.merge(timedaData, non_work_dest_tazs, left_on='dest', right_on='taz', how = 'inner')\n",
    "            od_data.isna().sum()\n",
    "\n",
    "            #for time thresholds - currently set in config files\n",
    "            for threshold in time_thresholds:\n",
    "                # print(f'processing {transit_file} for time threshold {threshold}')\n",
    "                # create data for all destinations\n",
    "                oddata_min = od_data.loc[od_data[period] <= threshold]\n",
    "                oddata_min = oddata_min.groupby(['orig'])['non_work_dest'].sum().reset_index()\n",
    "\n",
    "                # adding employed residents\n",
    "                oddata_min = pd.merge(oddata_min, tazDataPop, left_on= ['orig'], right_on =['ZONE'], how ='left')\n",
    "\n",
    "                # regional \n",
    "                oddata_min['wt_pop'] = oddata_min['non_work_dest'] * oddata_min['TOTPOP']\n",
    "                nwd_acc_min = oddata_min['wt_pop'].sum()/oddata_min['non_work_dest'].sum()\n",
    "                #acc_jobs.append(job_acc_min)\n",
    "\n",
    "                df_temp_region = pd.DataFrame({'Description': \"Non-work destinations accessible within \" + tt_verbose + \" \" + str(threshold) + \" minutes\",\n",
    "                                               'Population': 'Whole Population',\n",
    "                                               'Period': period,\n",
    "                                               'Geography': 'Regional total',\n",
    "                                               'Zone_ID': 'Region',\n",
    "                                               'Submetric': 'B1.3.1',\n",
    "                                               'Total_Increment': '',\n",
    "                                               'Value': nwd_acc_min}, index=[0])\n",
    "                \n",
    "                \n",
    "                df.append(df_temp_region)\n",
    "\n",
    "                # adding priority population and geographies\n",
    "                oddata_min = pd.merge(oddata_min, geo_pp_cwks, left_on= ['orig'], right_on =['taz'], how ='left')\n",
    "\n",
    "                # for prioirty population\n",
    "                if 'pp_share' in geo_pp_cwks.columns:\n",
    "                    oddata_min['priority_population'] = oddata_min['pp_share'].apply(lambda x: 1 if x > 0 else 0)\n",
    "                    oddata_min['wt_pop_pp'] = oddata_min['non_work_dest'] * oddata_min['TOTPOP'] * oddata_min['pp_share']/100\n",
    "                    oddata_min['pop_pp'] = oddata_min['TOTPOP'] * oddata_min['pp_share']/100\n",
    "                    oddata_min_pp = oddata_min[oddata_min['priority_population']==1]\n",
    "                    oddata_min_nonpp = oddata_min[oddata_min['priority_population']==0]\n",
    "                    oddata_min_pp = oddata_min_pp.groupby(['priority_population'])['wt_pop_pp','pop_pp'].sum().reset_index()\n",
    "                    oddata_min_pp['Value'] = oddata_min_pp['wt_pop_pp']/oddata_min_pp['pop_pp']\n",
    "                    oddata_min_pp = oddata_min_pp[['priority_population', 'Value']]\n",
    "\n",
    "                    oddata_min_nonpp = oddata_min_nonpp.groupby(['priority_population'])['wt_pop','TOTPOP'].sum().reset_index()\n",
    "                    oddata_min_nonpp['Value'] = oddata_min_nonpp['wt_pop']/oddata_min_nonpp['TOTPOP']\n",
    "                    oddata_min_nonpp = oddata_min_nonpp[['priority_population', 'Value']]\n",
    "\n",
    "                    oddata_min_comb = pd.concat([oddata_min_pp, oddata_min_nonpp], ignore_index=False)\n",
    "                    oddata_min_comb = oddata_min_comb.loc[oddata_min_comb['priority_population'] == 1]\n",
    "                    oddata_min_comb = oddata_min_comb.drop(columns=['priority_population'])\n",
    "                    oddata_min_comb['Description'] = \"Non-work destinations accessible within \" + tt_verbose + \" \" + str(threshold) + \" minutes\"\n",
    "                    oddata_min_comb['Period'] = period\n",
    "                    oddata_min_comb['Population'] = 'Priority population'\n",
    "                    oddata_min_comb['Geography'] = 'Regional total'\n",
    "                    oddata_min_comb['Zone_ID'] = 'Region'\n",
    "                    oddata_min_comb = oddata_min_comb[['Description', 'Population', 'Period', \n",
    "                                                     'Geography', 'Zone_ID', 'Value']]\n",
    "                    oddata_min_comb['Submetric'] = 'B1.3.2'\n",
    "                    oddata_min_comb['Total_Increment'] = ''\n",
    "                    nwd_jobs_pp.append(oddata_min_comb)\n",
    "\n",
    "                # for RDM zones\n",
    "                if 'rdm_zones' in geo_pp_cwks.columns:\n",
    "                    oddata_min_rdm = oddata_min.groupby(['rdm_zones'])['wt_pop','TOTPOP'].sum().reset_index()\n",
    "                    oddata_min_rdm['Value'] =  oddata_min_rdm['wt_pop']/oddata_min_rdm['TOTPOP']\n",
    "                    oddata_min_rdm['Description'] = \"Non-work destinations accessible within \" + tt_verbose + \" \" + str(threshold) + \" minutes\"\n",
    "                    oddata_min_rdm['Period'] = period\n",
    "                    oddata_min_rdm['Geography'] = 'RDM'\n",
    "                    oddata_min_rdm['Population'] = 'Whole Population'\n",
    "                    oddata_min_rdm.rename(columns={'rdm_zones' : 'Zone_ID'}, inplace=True)\n",
    "                    oddata_min_rdm = oddata_min_rdm[['Description', 'Population', 'Period',\n",
    "                                                     'Geography', 'Zone_ID', 'Value']]\n",
    "                    oddata_min_rdm['Submetric'] = 'B1.3.3'\n",
    "                    oddata_min_rdm['Total_Increment'] = ''\n",
    "                    nwd_jobs_rdm.append(oddata_min_rdm)\n",
    "\n",
    "                # for superdistrict\n",
    "                if 'super_district' in geo_pp_cwks.columns:\n",
    "                    oddata_min_sd = oddata_min.groupby(['super_district'])['wt_pop','TOTPOP'].sum().reset_index()\n",
    "                    oddata_min_sd['Value'] =  oddata_min_sd['wt_pop']/oddata_min_sd['TOTPOP']\n",
    "                    oddata_min_sd['Description'] = \"Non-work destinations accessible within \" + tt_verbose + \" \" + str(threshold) + \" minutes\"\n",
    "                    oddata_min_sd['Period'] = period\n",
    "                    oddata_min_sd['Population'] = 'Whole Population'\n",
    "                    oddata_min_sd['Geography'] = 'Superdistrict'\n",
    "                    oddata_min_sd.rename(columns={'super_district': 'Zone_ID'}, inplace=True)\n",
    "                    oddata_min_sd = oddata_min_sd[['Description', 'Population', 'Period',\n",
    "                                                     'Geography', 'Zone_ID', 'Value']]\n",
    "                    oddata_min_sd['Submetric'] = 'B1.3.4'\n",
    "                    oddata_min_sd['Total_Increment'] = ''\n",
    "                    nwd_jobs_sd.append(oddata_min_sd)\n",
    "\n",
    "                # for county\n",
    "                if 'county' in geo_pp_cwks.columns:\n",
    "                    oddata_min_cnty = oddata_min.groupby(['county'])['wt_pop','TOTPOP'].sum().reset_index()\n",
    "                    oddata_min_cnty['Value'] =  oddata_min_cnty['wt_pop']/oddata_min_cnty['TOTPOP']\n",
    "                    oddata_min_cnty['Description'] = \"Non-work destinations accessible within \" + tt_verbose + \" \" + str(threshold) + \" minutes\"\n",
    "                    oddata_min_cnty['Period'] = period\n",
    "                    oddata_min_cnty['Geography'] = 'County'\n",
    "                    oddata_min_cnty['Population'] = 'Whole Population'\n",
    "                    oddata_min_cnty.rename(columns={'county': 'Zone_ID'}, inplace=True)\n",
    "                    oddata_min_cnty = oddata_min_cnty[['Description', 'Population', 'Period',\n",
    "                                                     'Geography', 'Zone_ID', 'Value']]   \n",
    "                    oddata_min_cnty['Submetric'] = 'B1.3.5'\n",
    "                    oddata_min_cnty['Total_Increment'] = ''\n",
    "                    nwd_jobs_cnty.append(oddata_min_cnty)\n",
    "                        \n",
    "        else:\n",
    "            print(f'{period} travel time doesn\\'t exist.')\n",
    "            \n",
    "    df_region = pd.concat(df).reset_index(drop=True)\n",
    "    df_pp = pd.concat(nwd_jobs_pp).reset_index(drop=True)\n",
    "    df_rdm = pd.concat(nwd_jobs_rdm).reset_index(drop=True)\n",
    "    df_sd = pd.concat(nwd_jobs_sd).reset_index(drop=True)\n",
    "    df_cnty = pd.concat(nwd_jobs_cnty).reset_index(drop=True)\n",
    "    \n",
    "    return df_region, df_pp, df_rdm, df_sd, df_cnty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0fdd99",
   "metadata": {},
   "source": [
    "df_region, df_pp, df_rdm, df_sd, df_cnty = get_non_work_destionations(all_tod_tt, time_periods, time_thresholds,\n",
    "                                                                      geo_pp_cwks, non_work_tazs, \"actual\")\n",
    "\n",
    "comb_df = pd.concat([df_region, df_pp, df_rdm, df_sd, df_cnty], ignore_index=True)\n",
    "\n",
    "comb_df['Concept_ID'] = concept_id\n",
    "comb_df['Metric_ID'] = 'B1.3'\n",
    "comb_df['Metric_name'] = 'Non-work destinations accessible from people\\'s homes'\n",
    "comb_df['Origin_zone'] = ''\n",
    "comb_df['Dest_zone'] = ''\n",
    "comb_df['Purpose'] = ''\n",
    "comb_df['Units'] = 'Destinations'\n",
    "comb_df['Value'] = comb_df['Value'].apply(lambda x: round(x, decimals))\n",
    "\n",
    "comb_df_nwd_act = comb_df[summary_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffba3aff",
   "metadata": {},
   "source": [
    "df_region, df_pp, df_rdm, df_sd, df_cnty = get_non_work_destionations(perc_tod_tt, time_periods, time_thresholds,\n",
    "                                                                      geo_pp_cwks, non_work_tazs, \"perceived\")\n",
    "\n",
    "comb_df = pd.concat([df_region, df_pp, df_rdm, df_sd, df_cnty], ignore_index=True)\n",
    "\n",
    "comb_df['Concept_ID'] = concept_id\n",
    "comb_df['Metric_ID'] = 'B1.3'\n",
    "comb_df['Metric_name'] = 'Non-work destinations accessible from people\\'s homes'\n",
    "comb_df['Origin_zone'] = ''\n",
    "comb_df['Dest_zone'] = ''\n",
    "comb_df['Purpose'] = ''\n",
    "comb_df['Units'] = 'Destinations'\n",
    "comb_df['Value'] = comb_df['Value'].apply(lambda x: round(x, decimals))\n",
    "\n",
    "comb_df_nwd_perc = comb_df[summary_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de93e085",
   "metadata": {},
   "source": [
    "comb_df_nwd = pd.concat([comb_df_nwd_act, comb_df_nwd_perc], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b8dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comb_df = pd.concat([comb_df_acc, comb_df_nwd], ignore_index=True)\n",
    "#comb_df = comb_df[summary_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5282af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with pd.ExcelWriter(os.path.join(summary_outputs, 'concept-BY15.xlsx'), engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "#    comb_df_acc.to_excel(writer, sheet_name='B1.1', startcol=0, index=False)\n",
    "#    comb_df_nwd.to_excel(writer, sheet_name='B1.3', startcol=0, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5fe1f",
   "metadata": {},
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d55e183",
   "metadata": {},
   "source": [
    "def get_accessibility_jobs(transit_skim_files, mat_core, geo_pp_cwks, non_work_dest_tazs):\n",
    "\n",
    "    acc_jobs = []\n",
    "    acc_jobs_nwd = []\n",
    "    df = []\n",
    "    acc_jobs_pp = []\n",
    "    acc_jobs_rdm = []\n",
    "    acc_jobs_sd = []\n",
    "    acc_jobs_cnty = []\n",
    "    \n",
    "    #for 25 transit skims files - this includes time periods\n",
    "    for transit_file in transit_skim_files:\n",
    "        \n",
    "        transit_file_name = os.path.split(transit_file)[-1]\n",
    "        \n",
    "        for core in mat_core:\n",
    "            # od data from skims\n",
    "            timedaData = convertMat2Df(transit_file, core)\n",
    "            timedaData = timedaData.loc[timedaData[core]>0]\n",
    "            timedaData[core] = timedaData[core]/100\n",
    "              \n",
    "            #total employment based on destination\n",
    "            od_data = pd.merge(timedaData, tazDataTotemp, left_on='dest', right_on='ZONE', how = 'left')\n",
    "            od_data.columns\n",
    "\n",
    "            #for time thresholds - currently set in config files\n",
    "            for threshold in time_thresholds:\n",
    "                print(f'processing {transit_file} for time threshold {threshold}')\n",
    "                # create data for all destinations\n",
    "                oddata_min = od_data.loc[od_data[core] <= threshold]\n",
    "                oddata_min = oddata_min.groupby(['orig'])['TOTEMP'].sum().reset_index()\n",
    "\n",
    "                # adding employed residents\n",
    "                oddata_min = pd.merge(oddata_min, tazDataEmpres, left_on= ['orig'], right_on =['ZONE'], how ='left')\n",
    "\n",
    "                # regional \n",
    "                oddata_min['wt_empres'] = oddata_min['TOTEMP'] * oddata_min['EMPRES']\n",
    "                job_acc_min = oddata_min['wt_empres'].sum()/oddata_min['EMPRES'].sum()\n",
    "                #acc_jobs.append(job_acc_min)\n",
    "                \n",
    "                df_temp_region = pd.DataFrame({'transit_file': os.path.split(transit_file)[-1], 'time_threshold': threshold, 'jobs_from_home': job_acc_min, 'time': core}, index=[0])\n",
    "                df.append(df_temp_region)\n",
    "\n",
    "                # adding priority population and geographies\n",
    "                oddata_min = pd.merge(oddata_min, geo_pp_cwks, left_on= ['orig'], right_on =['taz'], how ='left')\n",
    "                \n",
    "                # for prioirty population\n",
    "                if 'pp_share' in geo_pp_cwks.columns:\n",
    "                    oddata_min['priority_population'] = oddata_min['pp_share'].apply(lambda x: 1 if x > 0 else 0)\n",
    "                    oddata_min['wt_empres_pp'] = oddata_min['TOTEMP'] * oddata_min['EMPRES'] * oddata_min['pp_share']/100\n",
    "                    oddata_min['EMPRES_pp'] = oddata_min['EMPRES'] * oddata_min['pp_share']/100\n",
    "                    oddata_min_pp = oddata_min[oddata_min['priority_population']==1]\n",
    "                    oddata_min_nonpp = oddata_min[oddata_min['priority_population']==0]\n",
    "                    oddata_min_pp = oddata_min_pp.groupby(['priority_population'])['wt_empres_pp','EMPRES_pp'].sum().reset_index()\n",
    "                    oddata_min_pp['jobs_from_home'] = oddata_min_pp['wt_empres_pp']/oddata_min_pp['EMPRES_pp']\n",
    "                    oddata_min_pp = oddata_min_pp[['priority_population', 'jobs_from_home']]\n",
    "\n",
    "                    oddata_min_nonpp = oddata_min_nonpp.groupby(['priority_population'])['wt_empres','EMPRES'].sum().reset_index()\n",
    "                    oddata_min_nonpp['jobs_from_home'] = oddata_min_nonpp['wt_empres']/oddata_min_nonpp['EMPRES']\n",
    "                    oddata_min_nonpp = oddata_min_nonpp[['priority_population', 'jobs_from_home']]\n",
    "\n",
    "                    oddata_min_comb = pd.concat([oddata_min_pp, oddata_min_nonpp], ignore_index=False)\n",
    "                    oddata_min_comb['time_threshold'] = threshold\n",
    "                    oddata_min_comb['transit_file'] = transit_file_name\n",
    "                    oddata_min_comb['time'] = core\n",
    "                    acc_jobs_pp.append(oddata_min_comb)\n",
    "\n",
    "                # for RDM zones\n",
    "                if 'rdm_zones' in geo_pp_cwks.columns:\n",
    "                    oddata_min_rdm = oddata_min.groupby(['rdm_zones'])['wt_empres','EMPRES'].sum().reset_index()\n",
    "                    oddata_min_rdm['jobs_from_home'] =  oddata_min_rdm['wt_empres']/oddata_min_rdm['EMPRES']\n",
    "                    oddata_min_rdm['time_threshold'] = threshold\n",
    "                    oddata_min_rdm['transit_file'] = transit_file_name\n",
    "                    oddata_min_rdm['time'] = core\n",
    "                    oddata_min_rdm = oddata_min_rdm[['rdm_zones', 'jobs_from_home', 'time_threshold', 'transit_file', 'time']] \n",
    "                    acc_jobs_rdm.append(oddata_min_rdm)\n",
    "\n",
    "                # for superdistrict\n",
    "                if 'super_district' in geo_pp_cwks.columns:\n",
    "                    oddata_min_sd = oddata_min.groupby(['super_district'])['wt_empres','EMPRES'].sum().reset_index()\n",
    "                    oddata_min_sd['jobs_from_home'] =  oddata_min_sd['wt_empres']/oddata_min_sd['EMPRES']\n",
    "                    oddata_min_sd['time_threshold'] = threshold\n",
    "                    oddata_min_sd['transit_file'] = transit_file_name\n",
    "                    oddata_min_sd['time'] = core\n",
    "                    oddata_min_sd = oddata_min_sd[['super_district', 'jobs_from_home', 'time_threshold', 'transit_file', 'time']] \n",
    "                    acc_jobs_sd.append(oddata_min_sd)\n",
    "\n",
    "                # for county\n",
    "                if 'county' in geo_pp_cwks.columns:\n",
    "                    oddata_min_cnty = oddata_min.groupby(['county'])['wt_empres','EMPRES'].sum().reset_index()\n",
    "                    oddata_min_cnty['jobs_from_home'] =  oddata_min_cnty['wt_empres']/oddata_min_cnty['EMPRES']\n",
    "                    oddata_min_cnty['time_threshold'] = threshold\n",
    "                    oddata_min_cnty['transit_file'] = transit_file_name\n",
    "                    oddata_min_cnty['time'] = core\n",
    "                    oddata_min_cnty = oddata_min_cnty[['county', 'jobs_from_home', 'time_threshold', 'transit_file', 'time']] \n",
    "                    acc_jobs_cnty.append(oddata_min_cnty)\n",
    "            \n",
    "    df_region = pd.concat(df)\n",
    "    df_pp = pd.concat(acc_jobs_pp)\n",
    "    df_rdm = pd.concat(acc_jobs_rdm)\n",
    "    df_sd = pd.concat(acc_jobs_sd)\n",
    "    df_cnty = pd.concat(acc_jobs_cnty)\n",
    "    \n",
    "    return df_region, df_pp, df_rdm, df_sd, df_cnty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a58c34",
   "metadata": {},
   "source": [
    "def get_non_work_destionations(transit_skim_files, mat_core, geo_pp_cwks, non_work_dest_tazs):\n",
    "\n",
    "    acc_jobs = []\n",
    "    nwd_jobs_nwd = []\n",
    "    df = []\n",
    "    nwd_jobs_pp = []\n",
    "    nwd_jobs_rdm = []\n",
    "    nwd_jobs_sd = []\n",
    "    nwd_jobs_cnty = []\n",
    "    \n",
    "    \n",
    "    #for 25 transit skims files - this includes time periods\n",
    "    for transit_file in transit_skim_files:\n",
    "        \n",
    "        transit_file_name = os.path.split(transit_file)[-1]\n",
    "        \n",
    "        for core in mat_core:\n",
    "            # od data from skims\n",
    "            timedaData = convertMat2Df(transit_file, core)\n",
    "            timedaData = timedaData.loc[timedaData[core]>0]\n",
    "            timedaData[core] = timedaData[core]/100\n",
    "\n",
    "            #total non work destinations based on destination\n",
    "            od_data = pd.merge(timedaData, non_work_dest_tazs, left_on='dest', right_on='taz', how = 'inner')\n",
    "            od_data.isna().sum()\n",
    "\n",
    "            #for time thresholds - currently set in config files\n",
    "            for threshold in time_thresholds:\n",
    "                print(f'processing {transit_file} for time threshold {threshold}')\n",
    "                # create data for all destinations\n",
    "                oddata_min = od_data.loc[od_data[core] <= threshold]\n",
    "                oddata_min = oddata_min.groupby(['orig'])['non_work_dest'].sum().reset_index()\n",
    "\n",
    "                # adding employed residents\n",
    "                oddata_min = pd.merge(oddata_min, tazDataPop, left_on= ['orig'], right_on =['ZONE'], how ='left')\n",
    "\n",
    "                # regional \n",
    "                oddata_min['wt_pop'] = oddata_min['non_work_dest'] * oddata_min['TOTPOP']\n",
    "                nwd_acc_min = oddata_min['wt_pop'].sum()/oddata_min['non_work_dest'].sum()\n",
    "                #acc_jobs.append(job_acc_min)\n",
    "                \n",
    "                df_temp_region = pd.DataFrame({'transit_file': os.path.split(transit_file)[-1], 'time_threshold': threshold, 'nwd_from_home': nwd_acc_min, 'time': core}, index=[0])\n",
    "                df.append(df_temp_region)\n",
    "\n",
    "                # adding priority population and geographies\n",
    "                oddata_min = pd.merge(oddata_min, geo_pp_cwks, left_on= ['orig'], right_on =['taz'], how ='left')\n",
    "                \n",
    "                # for prioirty population\n",
    "                if 'pp_share' in geo_pp_cwks.columns:\n",
    "                    oddata_min['priority_population'] = oddata_min['pp_share'].apply(lambda x: 1 if x > 0 else 0)\n",
    "                    oddata_min['wt_pop_pp'] = oddata_min['non_work_dest'] * oddata_min['TOTPOP'] * oddata_min['pp_share']/100\n",
    "                    oddata_min['pop_pp'] = oddata_min['TOTPOP'] * oddata_min['pp_share']/100\n",
    "                    oddata_min_pp = oddata_min[oddata_min['priority_population']==1]\n",
    "                    oddata_min_nonpp = oddata_min[oddata_min['priority_population']==0]\n",
    "                    oddata_min_pp = oddata_min_pp.groupby(['priority_population'])['wt_pop_pp','pop_pp'].sum().reset_index()\n",
    "                    oddata_min_pp['nwd_from_home'] = oddata_min_pp['wt_pop_pp']/oddata_min_pp['pop_pp']\n",
    "                    oddata_min_pp = oddata_min_pp[['priority_population', 'nwd_from_home']]\n",
    "\n",
    "                    oddata_min_nonpp = oddata_min_nonpp.groupby(['priority_population'])['wt_pop','TOTPOP'].sum().reset_index()\n",
    "                    oddata_min_nonpp['nwd_from_home'] = oddata_min_nonpp['wt_pop']/oddata_min_nonpp['TOTPOP']\n",
    "                    oddata_min_nonpp = oddata_min_nonpp[['priority_population', 'nwd_from_home']]\n",
    "\n",
    "                    oddata_min_comb = pd.concat([oddata_min_pp, oddata_min_nonpp], ignore_index=False)\n",
    "                    oddata_min_comb['time_threshold'] = threshold\n",
    "                    oddata_min_comb['transit_file'] = transit_file_name\n",
    "                    oddata_min_comb['time'] = core\n",
    "                    nwd_jobs_pp.append(oddata_min_comb)\n",
    "\n",
    "                # for RDM zones\n",
    "                if 'rdm_zones' in geo_pp_cwks.columns:\n",
    "                    oddata_min_rdm = oddata_min.groupby(['rdm_zones'])['wt_pop','TOTPOP'].sum().reset_index()\n",
    "                    oddata_min_rdm['nwd_from_home'] =  oddata_min_rdm['wt_pop']/oddata_min_rdm['TOTPOP']\n",
    "                    oddata_min_rdm['time_threshold'] = threshold\n",
    "                    oddata_min_rdm['transit_file'] = transit_file_name\n",
    "                    oddata_min_rdm['time'] = core\n",
    "                    oddata_min_rdm = oddata_min_rdm[['rdm_zones', 'nwd_from_home', 'time_threshold', 'transit_file', 'time']] \n",
    "                    nwd_jobs_rdm.append(oddata_min_rdm)\n",
    "\n",
    "                # for superdistrict\n",
    "                if 'super_district' in geo_pp_cwks.columns:\n",
    "                    oddata_min_sd = oddata_min.groupby(['super_district'])['wt_pop','TOTPOP'].sum().reset_index()\n",
    "                    oddata_min_sd['nwd_from_home'] =  oddata_min_sd['wt_pop']/oddata_min_sd['TOTPOP']\n",
    "                    oddata_min_sd['time_threshold'] = threshold\n",
    "                    oddata_min_sd['transit_file'] = transit_file_name\n",
    "                    oddata_min_sd['time'] = core\n",
    "                    oddata_min_sd = oddata_min_sd[['super_district', 'nwd_from_home', 'time_threshold', 'transit_file', 'time']] \n",
    "                    nwd_jobs_sd.append(oddata_min_sd)\n",
    "\n",
    "                # for county\n",
    "                if 'county' in geo_pp_cwks.columns:\n",
    "                    oddata_min_cnty = oddata_min.groupby(['county'])['wt_pop','TOTPOP'].sum().reset_index()\n",
    "                    oddata_min_cnty['nwd_from_home'] =  oddata_min_cnty['wt_pop']/oddata_min_cnty['TOTPOP']\n",
    "                    oddata_min_cnty['time_threshold'] = threshold\n",
    "                    oddata_min_cnty['transit_file'] = transit_file_name\n",
    "                    oddata_min_cnty['time'] = core\n",
    "                    oddata_min_cnty = oddata_min_cnty[['county', 'nwd_from_home', 'time_threshold', 'transit_file', 'time']] \n",
    "                    nwd_jobs_cnty.append(oddata_min_cnty)\n",
    "            \n",
    "    df_region = pd.concat(df)\n",
    "    df_pp = pd.concat(nwd_jobs_pp)\n",
    "    df_rdm = pd.concat(nwd_jobs_rdm)\n",
    "    df_sd = pd.concat(nwd_jobs_sd)\n",
    "    df_cnty = pd.concat(nwd_jobs_cnty)\n",
    "    \n",
    "    return df_region, df_pp, df_rdm, df_sd, df_cnty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
